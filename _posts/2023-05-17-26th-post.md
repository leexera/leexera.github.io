---
layout: post
title: "Detection Transformer (DETR)"
description: "Part 2. 컴퓨터비전 특화 이론과 실습", "Ch05. 객체탐지(Object Detection)와 분할(Segmentation)", "Object Detection - Object Detection by CNN"
date: 2023-05-17
tags: study
comments: true
---
# DeTR: Detection Transformer

 * Vision Transformer for Object Detection
    - CNN이 아닌 Transformer구조를 사용하여 object detection을 해보자
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/62256399-36dd-47db-bd03-39c196ec57e2)
 motivation은 CNN 구조를 활용하여 object detection을 fast R-CNN이라던지 faster R-CNN, YOLO, FPN, EfficientDet 등과 같은 model들이 있었는데 transformer 구조를 사용해서 object detection을 해보자는 것이 핵심 idea이다.
 DETR는 CNN으로 feature encoding을 한다.
 encoding된 feature를 transformer encoder-decoder 구조로 한 번 feed foward, self-attention을 하여 update를 하고 prediction한 bonding box를 추출한다.
 bounding box에서 no object가 가장 높은 것은 object가 없다고 prediction하고 box의 objectness가 충분히 있는 것은 object의 bounding box를 주어 최종적으로 biparire matching loss를 통해 object가 없는 부분들은 다시 no object로 loss를 주고 object가 있는 부분에 대해서는 다른 loss를 주어 box size도 어느정도 같아지고 class prediction도 잘 하도록 loss를 주는 것이 전반적인 구조이다.

 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/bbdfb278-7024-41d6-8dea-87eeba0df056)
 * Backbone & Encoder
    - Using individual vectors of a convolutional map as input tokens
    - Supplementing the tokens with fixed positional encoding
    - Stacking multiple multi-head self-attention layers to build the transformer encoder
       * Enlarging the receptive fields effectively and dfficiently

 backbone은 기존의 image feature를 얻기 위해서 CNN을 통과시킨다.
 물론 ViT처럼 patch를 unfolding시켜 쭉 펼칠 수도 있지만 그렇게 되면 feature로서의 역할이 잘 되지는 않기 때문에 backbone을 CNN을 차용한 것이다.
 input token으로 convolutional feature map을 사용하고 각 위치에 대한 individual vector를 사용한다.
 fixed positional encoding을 사용하여 prediction정보를 각 input feature에 대해서 넣어준다.
 숫자 뿐만이 아닌 sine곡선과 같은 positional encoding을 줄 수도 있고 다양한 형태의 고정된 positional encoding을 준다.
 relative한 positional encoding을 줄 수도 있지만 고정된 절댓값을 넣어줬다는 것이 point이다.
 multi-head self-attention layer들은 token들에 대해서 쭉 stacking 하고 multi-head self-attention을 중첩시켜 최종적인 output feature를 만든다.
 중첩되는 경우에 receptive field도 효과적으로 증가한다.

 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/1322941d-c522-4a46-bb42-8502cdacb0d1)
 * Decoder
    - Decoding a predefined number of objects in parallel through object queries
       * An object queries are transformed into an ouput embedding, which is used as input to predict a bounding box and its object class in the prediction heads
       * #of object queries (100 in th paper) is in general much larger than the actual # of objects
    - Globally reasoning about all objects together through the attention mechanism

 predefined number of object에서 object queries에 대해 판단한다.
 각 patch가 있을 때 각 patch에 대한 object query, 예를 들어 class인지, no object인지에 대해 어떤 object feature가 얼마나 유사성이 있는지를 판단하는 것이다.
 object queries가 output embedding에 들어가 bounding box와 object class를 prediction하게 된다.
 이 때 사용한 object query는 100개 정도 사용되었기 때문에 100개보다 큰 경우가 되어야 일반화할 수 있다.
 모든 object에 대해서 global하게 reasoning을 하여 attention mechanism을 통해 transformer decoder를 통해 object query가 맞는지 아닌지를 추론한다.
 최종적으로 Feed Foward Network(FFN)를 통과시켜 class, box가 class인지 object가 없는 것으로 판단하는 것인지 정하게 된다.

 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/6a652491-3d4a-49f9-b924-702c78dd49a3)
 * A feed-forward network computing the final prediction
    - A 3-layer MLP with ReLU activation functions
    - Estimating a bounding box and its class label per object query
 
 prediction head부분은 feed-forward network로 구성되고 3개의 layer MLP와 ReLU activation function을 사용하여 해당하는 patch descriptor, patch feature가 class, box를 나타내는지 object가 없다고 판단하는지를 estimation한다.
 최종적으로 bounding box와 class label이 object query마다 나오는 것이다.
 object query의 bounding box coordinate과 class label이 할당되어서 나오게 된다.
 object query가 이상하게 얽힌 부분은 no object라고 판단하는 구조를 갖는다.

 * Loss functions for learning DETR
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/a681f672-8186-48ed-a796-2cfa0126e08e)
 hungarian algorithm과 비슷하게 mathing loss를 사용한다.
 N개의 prediction이 있을 때 object가 포함되어있는 bounding box와 class prediction, Y(hat)이 있을 때 ground truth label은 Y가 있다.
 no object Φ로 padding을 시켜 N개의 label에 대해서 Y set을 형성한다.
 element y는 bounding box의 coordinate와 class label이다.
 prediction Y(hat)은 prediction score이다.
 Hungarian algorithm을 사용하여 가장 lowest matching cost가되는 permutation을 search한다.
 Hungarian algorithm을 similarity metrics에서 trace, diagonal part를 maximize하고자 하는, 남기고자 하는 matching algorithm이다.
 실제로 object가 있는 영역과 prediction한 영역이 가장 같아지도록 계속 optimize를 하는 것이다.
 match loss가 prediction과 ground truth에 대해서 cost를 계산하게 되고 이것을 펼치게 되면 bounding box score와 class prediction score가 각각에 걸치게 된다.
 class prediction은 object가 아닌 경우에 대해서 minimize를 하고 해당하는 index와 GT의 object index와 bounding box도 같아지도록 loss를 주게 된다.
 최종적인 loss는 estimated permutation에 대해서 적용된다.
 추측한 bounding box class label과 ground truth bounding box class label을 sorting한 것에 대해 bounding box loss와 class loss를 준다.

 * Results
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/53f5b3f2-5f3c-4a48-81fd-c4844f873264)
 FPN을 결합한 기존의 Faster R-CNN보다 GFLOPS나 FPS(Frames Per Second)가 연산량은 더 적게 가져가면서 더 높은 성능을 보이며 FPS는 조금 낮은데 그 이유는 transformer 구조가 연산량 model parameter 수에는 안들어가지만 연산이 오래 걸리는 연산들이 있어 오래걸리는 것이다.
 number of parameter도 비슷하게 유지하면서 성능이 어느정도 향상되는 것을 볼 수 있다.
 성능 자체가 faster R-CNN-R101-FPN+에 비해서 small object같은 경우 떨어졌기 때문에 많이 올랐다고 보기는 어렵지만 self-attention구조를 detection에 적용한 분석을 철저히 한다.
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/d91d155f-06cb-40f7-ac43-68dcea29c4a7)
 query point에 대해서 image 전체에 대한 self attention을 구하였을 때 소의 목덜미에 있는 점을 구해보면 소 자체의 object를 잡고 있고 머리만 truncation되어있는 소의 경우에도 그 부분 위주로 self attention이 잡혀 해석을 하게 된다.
 convolution filter같은 경우는 inductive bias를 고정된 size로만 주게 되는데 self attention과 같은 구조는 self similarity, 어떤 하나의 query point에 대해서 image 전체에 similar하게 주어야하는 부분에 대한 영역을 그래프로 만들다보니 주요한 영역들에 대해 query point의 attention이 잘 잡히는 것을 볼 수 있다.
 따라서 조금 더 inductive bias가 적은 self attention model을 잘 활용한 것이다.

 * Visualizations
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/2ed5c5da-f0a2-4421-880b-cee3c423a1ca)
 Fig.6: Visualizing decoder attention for every predicted object
 elephant라는 class query token이 들어갔을 때 코끼리의 코나 꼬리, 등 부분을 보아 elephant라고 판단하게 된다.
 attention을 통해 decoder가 class token이 object를 코끼리라고 판단하는 것이다.
 반면에 convolutional network는 띄어내서 판단할 수 없는데 그 이유는 convolution filter가 sliding window fashion으로 이루어지기 때문에 떨어져있는 영역들에 대해서 동시에 판단하는 것이 구조적으로 불가능한데 self attention 구조를 활용한 transformer는 멀리 떨어져 있는 구조 여러 개를 조합해서 object detection을 판단할 수 있다는 장점이 있다.
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/524e5953-e335-4cf2-a5e8-da23b19bd08c)
 Fig.9: Qualitative results for panoptic segmentation generated by DETR-R101
 panotic segmentation은 DETR-R101을 사용하여 object의 counting까지 포함하는 instance segmentation의 상위 버전 task도 수행한다.
 panotic segmentation은 background도 segmentation 결과를 측정한다.

# Deformable DETR

 * Deformable Convolution에서 idea를 얻어 transformer 구조에 적용
    - Convolution filter의 kernel위치를 학습시켜 광범위하고도, deformable한 receptive field를 같게 하였음
    - 이는 기준 pixel의 offset을 FC layer로 학습시켰음
 * 마찬가지로 conv filter의 offset을 deformable DETR에서는 encoder 내의 attention 입력인 key의 offset으로 대체하여 사용
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/5cfd17db-e303-424c-a2c5-7de37ae6c368)
 input feature map이 있으면 convolution을 통과시켜 각각의 offset field, convolution이 얼마나 이동해야하는지 구하고 convolution filter가 이동하여 위치를 읽어들이게 된다.

 변형이 많은 object에 대해서 판단할 때 convolution을 사용하는 것이 아닌 움직임이 있는 pattern을 recognize해야한다는 motivation으로 출발한다.
 3 by 3 deformabel convolution은 grid에 대해서 적용되는 것이 아닌 offset이 주어진 위치를 읽어들여 output feature map을 만들게 된다.
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/509be7b7-df00-4d0c-9e78-450f65ac5eeb)
 standard convolution은 정해진 grid에 대한 영역만 가져오게 되는데 defromable convolution은 하나의 위치에 대해서 흩뿌려진 9개의 영역을 가져올 수 있고 확장되면 더 흩뿌려져서 object의 필요 없는 영역의 pattern만 있는 것이 아니라 object 그 자체를 조금 더 inductive bias를 없앤채로 지역성을 고정하지 않고 지역적인 offset을 학습하여 convolution feedforwarding을 하게 된다.

 * Technical contributions
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/73d5b791-42f1-4658-a3ac-177cef10c349)
 image의 feature map은 image의 feature pyramid로 쌓이고 pyramid에 대해서 multi-scale feature map이 transformer encoder로 feedforwarding이 되는데 positional encoding이 막 걸리는 것이 아닌 offset에 대해서 key가 학습이 되어 서로 deformation되는 위치에서도 deformable self attention이 통과할 수 있게 된다.
 이러한 layer를 중첩시켜 최종적으로 하나의 위치에 대한 local feature를 구하게 된다.
 multi-scale deformable cross attention을 사용하여 object queries에 대해 decoder를 통과하게 된다.
 object queries가 몇 개 있고, 그것과 similarity를 구하여 feature가 각각 어떤 bounding box 위치와 class probability를 가지는지 예측하게 된다.
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/98f74b4c-39c3-49ed-8110-30620c62b719)
 deformable attention module의 도식화 그림이다.
 어떤 query feature Q가 있을 때 linear한 layer를 통과하여 transformation이 된다.
 어떤 reference point가 있을 때 해당 feature map을 떼어내서 온다.
 그 위치의 offset을 sampling하여 어떻게 이동할 수 있는지 여러 개로 sampling한다.
 각 sampling한 값들의 key값을 가져와 최종적으로 attention을 구한 aggregation을 할 때 attention weight와 key를 곱하여 aggregate를 하게 된다.
 aggregate된 sampled value를 구하고 각각 head값에 대해서 linear를 통과시켜 bounding box와 class probability를 구하는 idea이다.

 * Results: 기존 DETR보다 높은 성능
 * Conclusion: deformation을 고려한 attention구조는 성능 향상에 도움이 됨
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/01614e81-4ecb-46fc-b16f-d9f3f88d767a)
 똑같이 50epoch을 돌았을 때 deformable DETR는 조금 더 빨리 수렴하는 것을 볼 수 있다.
 FLOP이나 training GPU hours, inference FPS는 어느정도 맞추거나 더 좋게 만든다는 것을 알 수 있다.
 iterative bounding box refinement가 성능을 높이고 two-stage로 training했을 때도 성능이 높아졌다.
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/0dd27a64-89cb-4b97-957e-9b5e5ff99bca)
 two-stage training 결과로 deformable DETR가 먼저 한 번 돌고 멈춘다음 two-stage로 training한 후에 double로 update가 확 올라가는 결과를 볼 수 있다.
 반면 DETR-DC5는 two-stage를 하더라도 epoch이 오래걸려서 학습이 되게 되는데 deformable attention자체에서 object의 움직임이나 pattern의 offset을 학습해버리기 때문에 조금 더 빨리 수렴할 수 있는 것이다.
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/a4045550-bfe6-467f-babb-e1b67571e549)
 deformable DETR와 state-of-the-art method들의 COCO 2017 test-dev-set에서 object detection evaluation 결과이다.
 기존에 유명하던 FCOS, TSD, EfficientDet-D7 model보다 test-time augmentation(TTA)까지 포함했을 때 가장 높은 성능을 낸다.
 Deformable DETR가 ResNeXt-101과 DCN, test-time augmentation까지 포함했을 때 EfficientDet-D7보다 높은 성능을 낸다.
 backbone을 ResNet-50까지 줄여도 2019 model보다 성능이 좋은 것을 확인할 수 있다.
 horizontal filp이나 multi-scale을 포함한 test-time augmentation은 inference time은 오래걸리긴하지만 성능 향상의 효과는 확실히 존재한다. 



