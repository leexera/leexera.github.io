---
layout: post
title: "Faster R-CNN, Mask R-CNN"
description: "Part 2. 컴퓨터비전 특화 이론과 실습", "Ch05. 객체탐지(Object Detection)와 분할(Segmentation)", "Object Detection - Object Detection by CNN"
date: 2023-05-11
tags: study
comments: true
---
# Faster R-CNN

 * Fast R-CNN + Region Proposal Network
    - Proposal computation into network
    - Marginal cost of proposals: 10ms
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/d6e647d1-29d0-4bef-98c9-6fddc723285b)

 proposal을 계산하는 과정이 network안으로 들어와 learning time 크게 감소시켰다고 말할 수 있는 network이다.
 proposal을 계산하는데 marginal한 cost는 10ms밖에 걸리지 않는다.
 0.01s로 기존의 selective search나 edgebox와 같이 외부에서 object proposal을 만들던 algorithm을 대체 했다.
 전체적인 구조는 image가 들어오게 되면 기존엔 proposal을 먼저 뽑았는데 그것이 아닌 CNN을 통과시킨 feature map을 만든 후 region proposal network를 통과시킨다.
 한 영역들에 대해서 RoI(Region of Interest proposal)들을 많이 뽑아낸다.
 RoI 영역에서 어떤 feature가 나올 때 classification socre를 뽑아내고 bounding box regression을 통해 propose된 proposal들을 적절한 위치로 옮겨주는 trasformation을 학습시켜주는 regression loss를 똑같이 사용한다.
 region proposal network에는 feature map이 그대로 들어가서 proposal들이 제안이 되고 그 feature map을 그대로 사용해서 RoI pooling과 classifier를 수행하게 된다.
 하나의 위치에 대해 anchor box 여러 개를 뽑아 region proposal을 제안하게 된다.

 * Details of the region proposal network
    - 9 anchors per location (3 aspect ratios x 3 scales)
    - Groundtruth label per anchor
    ![image](https://github.com/leexera/leexera.github.io/assets/122149118/38e5f875-0d27-4059-b86e-e26430b7b339)
    where IoU is intersection over union:
    ![image](https://github.com/leexera/leexera.github.io/assets/122149118/4a1f1e1b-906c-47c0-bd7e-6798799dbdc1)
    - Trained with <i><u>a binary classification loss</i></u> for anchor selection and <i><u>a regression loss for box refinement</i></u>
    ![image](https://github.com/leexera/leexera.github.io/assets/122149118/69a7c572-fe46-4bca-951f-c53ab7add102)
 한 anchor point에 대해서 9개의 anchor가 뽑히게 된다.
 anchor box는 3개의 aspect ratio와 3개의 scale을 가지고 있다.
 첫 번째 aspect ratio는 정사각형과 같은 것이고 두 번째 aspect ratio는 가로가 긴 직사각형이고 세 번째 aspect ratio는 세로가 긴 직사각형을 뽑게 된다.
 각 aspect ratio에 대해서 크기가 다른 scale을 사용해서 총 9개의 anchor box를 하나의 location, 하나의 중점에 대해서 사용하게 된다.
 region proposal들이 anchor라고 생각되는 위치에 대해서 출력이 된다.
 anchor당 groundtruth label의 할당은 groundtruth와 겹치는 수준이 0.3보다 작을 때 -1, 0.7보다 클 때는 1, 그외에는 0이라는 groundtruth lable을 주게 된다.
 이 때 사용하는 IoU는 anchor box와 GT box의 합집합분의 교집합 score이다.
 여러 개 bounding box가 나올 것이고 한 위치에 대해서 object와 bounding box의 IoU가 낮다면 remove될 것이고 IoU score가 높은 것들만이 남아서 학습에 사용이 될 것이다.
 anchor selection을 위해서 binary classification loss를 이용해서 학습을 하고 bounding box regression에 사용되는 box refinement에는 regression loss를 사용한다.
 유념해야할 부분은 bounding box는 x, y의 center와 w, h을 가지고 있다는 것이 bounding box의 data structure라고 할 수 있다.
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/477f05f6-f47e-4a5b-86b0-b08743137d59)
 4k개의 coordinate 또한 x, y, w, h라는 coordinate score가 k개의 bounding box만큼 있다는 것이다.
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/26863b60-1934-44e7-99a1-e959a741f87c)
 class score도 class인지 아닌지 찾아내는 2개의 score를 가진다.
 * Result
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/f55759ae-5574-4aef-aa1b-b9e2a161060c)
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/c377e96d-1a15-40c9-934c-79188bfb231c)
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/1236b2fd-012b-41cd-8d8d-7b5475140163)
 fast R-CNN 기존 결과와 이 논문에서 다시 reimplementation하여 COCO validation set에서도 evaluation을 하였다.
 다시 구현한 fast R-CNN은 성능이 올라간 것을 확인할 수 있다.
 COCO train으로만 학습을 하고 selective search로 proposal을 2000개 구했을 때 위와 같은 성능을 확인할 수 있다.
 Faster R-CNN은 Region Proposal Network(RPN)를 사용하여 Selective Search(SS)를 대체하고 proposal 개수를 300개만 뽑았음에도 불구하고 mAP가 훨씬 더 높아지는 결과를 확인할 수 있다.
 train set뿐만 아니라 train validation set도 포함시켜 ms COCO로 training시켰을 때 성능 향상이 더 있었다.
 validation set을 학습으로 사용했기 때문에 validation set 결과는 기재하지 않고 test-dev set에서만 평가를 한 것이다.
 위 사진들은 Faster R-CNN의 정성적 결과로 위 4개의 사진은 pascal VOC의 결과이고 아래 4개의 사진은 ms COCO의 결과이다.
 작은 scale의 boat도 잘 찾아내고, 큰 scale의 boat도 잘 찾아내는 것을 확인할 수 있다.
 RPN과 bounding box regressor의 localization 강점을 보여주는 결과라고 할 수 있다.
 bird같은 경우도 background와 색깔이 비슷함에도 불구하고 objectness를 찾아내어 region proposal network와 bounding box regressor가 잘 학습되었음을 확인할 수 있다.

 * R-CNN family achieves the state-of-the-art performance in object detection
 * Stronger backbone network, better detection performance
 <b>Pascal VOC 2007 Object Detection mAP(%)</b>
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/09c67ec3-91dc-4260-ae73-cf3a41566f78)
 RCNN의 family가 object detection의 sota(state-of-the-art) 성능을 달성했다.
 VGG backbone만 사용한 것이지만 추후에 ResNet backbone을 사용해서 faster R-CNN이 조금 더 높은 성능을 달성하였다.
 VGG만 사용했을 때는 66의 성능만을 냈었는데 ResNet을 사용했을 때 86의 mAP 성능을 PASCAL VOC 2007 dataset에서 달성했다.
 backbone이 더 strong해질 수록 detection performance가 더 증가한다는 시사점을 준다.
 이러한 결과에 motivate되어 backbone을 더 강하게 하려는 representation learning과 transformer를 사용한 구조와 같이 backbone을 더 잘 사용하여 transferring을 하거나 더 좋은 feature extraction을 하고자 하는 움직임들이 계속 지속되고 있다.
 Vit와 같은 self supervised learning 다양한 기법들이 충분히 잘 transfer될 수 있다는 것처럼 거대 model의 관점에서 deep learning이 진행되고 있다.

# Faster R-CNN with ResNet
![image](https://github.com/leexera/leexera.github.io/assets/122149118/7e28905d-3d18-4150-9fae-0f91e97b7ea4)
ResNet을 사용한 Faster R-CNN의 정성적 결과이다.
굉장히 정밀하면서도 꽤 그럴듯하게 찾아내는 것을 볼 수 있다.

![image](https://github.com/leexera/leexera.github.io/assets/122149118/0ebbeed5-3ba1-482f-983e-34e3f59d1468)
드라마의 한 장면에서도 각 object를 정확한 localize 성능과 classification confidence score로 찾아낸다.

# Mask R-CNN
 * The Mask R-CNN framework for instance segmentation
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/ad030d5d-a0b1-4196-b8a1-bfbe0c86c5da)
 mask R-CNN은 instance segmentation을 위한 framework이다.
 ![image](https://user-images.githubusercontent.com/122149118/235717577-15d69019-27cc-4364-80a9-9938fa40212b.png)
 object detection은 bounding box를 찾고, instance segmentation은 object에서 segmentation mask를 찾아내는 task이다.
 sementic segmentation과 object detection을 섞은 것이 instance segmentation이라고 할 수 있다.
 mask R-CNN은 기존 faster R-CNN에서 segmentation mask를 정확하게 찾아내기 위해 RoIAlign을 제안했다는 특성을 가진다.
 input image에서 feature map을 뽑았을 때 Region Proposal Network로부터 region proposal들이 많이 생성되고 이 때 "Fast" R-CNN에서는 RoI pooling으로 전체 영역을 움직여 정수의 위치로 pooling했었고 RoIAlign은 상하좌우 coordinate를 잡고 interpolation을 하여 그 사이에 있는 feature를 조금 더 정밀하게 RoI들을 capture해내겠다는 것이 이 논문의 contribution이다.
 뒷 부분은 동일하게 bounding box regression과 classification을 수행하고 최종적인 box map에서 각 instance들의 mask들을 찾아내는 결과를 보여준다.

 * Instance segmentation by mask R-CNN
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/d47f77ef-898c-4ea8-9655-f4342b5ce7d6)
 ms COCO test set에서 결과를 보면 굉장히 정밀하고 정확하게 결과를 찾아낸다.
 bounding box로도 찾아주고 class label로도 찾아준다.
 ResNet backbone의 효과에 더하여 RoI align의 효과라고 볼 수 있다.
 게다가 mask R-CNN은 5fps라는 0.2의 image inference 성능도 추가적으로 가지고 있다.

 * RoI Align & Head Architecture
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/7f312895-b25d-42c2-9825-ea7f14be6eba)
 RoI pooling은 가까이에 있는 정수 점들을 grid 점들로 옮겨 pooling을 하는 법이고 grid값들을 주변에 있는 pixel들을 interpolation시켜서 box의 이동 없이 그대로 pooling을 해내고 feature map에서 extraction을 해내는 것이 RoI Align이다.
 실수 값의 bounding box값을 가져올 수 있다는 것이 핵심이다.
 위의 예시는 dashed grid, feature map에서 RoI가 실수로 잡혔을 때 2 by 2 bin size, feature map size로 bounding box를 뽑아내려고 할 때이다.
 실제로는 7 by 7을 뽑아내므로 더 커진다.
 coordinate에 대해 quantization없이 그대로 feature map을 추출할 수 있는 방법을 제안한다.
 위 figure는 그 당시 제시되었던 special transformer network같은 구현 방식을 활용한 것이다.
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/5ce874cb-3724-4093-a8ff-4e207c74d793)
 Head Architecture는 두 가지의 방식을 사용한다.
 RoI를 뽑아낸 후 ResNet 5번째까지 통과시켜 class와 bounding box만을 찾아내는 것이 아니라 mask도 prediction하는 두 가지 variation을 나타낸 것이다.
 기존의 faster R-CNN에서도 뒤에서 갈라지는 경우가 있고 앞에서 갈라져서 나오는 경우가 있었는데, 두 가지 architecture를 모두 사용했다고 한다.
 첫 번째 architecture는 14 by 14 resolution으로 최종적인 mask map을 찾아내서 bilinear interpolation으로 mask map을 구하는 것이다.
 두 번째는 14 by 14를 한 번 더 interpolation을 시켜 upsampling을 하여 28 by 28 size의 feature map으로 통과시킨 것이다.
 또 하나 다른 점은 faster R-CNN에 ResNet backbone을 사용한다면 뒤에는 faster R-CNN에 feature pyramid network라는 추후에 나온 architecture를 추가적으로 활용했다는 것이다.

 * Results of instance segmentation with different backbone
![image](https://github.com/leexera/leexera.github.io/assets/122149118/f9f8910a-0d89-41a8-858e-5851950a8fc0)
 different backbone을 사용하는 instance segmentation 결과로, mask R-CNN에서 ResNet 101의 C4에서 extraction을 사용했을 때 결과보다 ResNet 101의 Feature Pyramid Network(FPN)을 적용했을 때 scale robustness가 향상된다.
 조금 더 진보된 backbone인 ResNeXt 101의 Feature Pyramid Network까지 활용했을 때 가장 높은 성능을 ms COCO에서 냈다고 한다.
 위 결과는 instance segmentation결과로 instance segmentation은 AP를 마찬가지로 기존의 Recall과 precision의 결과를 같이해서 측정하게 된다.
 * Results of object detection
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/1ab6d131-4920-488d-8ef9-48dae6cbd054)
 Faster R-CNN에서 pooling을 RoIAlign으로 바꾸었을 때 기존의 기법들보다 성능이 향상된다.
 제안된 RoIAlign이 object detection에서도 더 정교하게 bonding box feature를 추출할 수 있어 성능향상이 있다.
 진보된 model인 mask R-CNN의 전체적인 구조를 head까지 사용했을 때 성능 향상과 backbone을 ResNet으로 바꾸었을 때 성능 향상을 보여주는 것이다.
 bounding box score의 threshold를 증가시킬 때 결과와 object의 크기에 따른 결과이다.
 Large object일 때는 오히려 Faster R-CNN에 TDM을 사용한 경우가 조금 더 성능이 좋긴 하지만 small object에서 특히 큰 성능 향상이 있고 middle object에서도 기존 Faster R-CNN보다 성능이 향상된다.
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/e3ec15a0-620f-43de-8514-e7bed4bba30d)
 첫 번째는 mask R-CNN의 backbone architecture를 변환했을 때 성능 향상이다.
 ResNet 101이나 ResNet 50의 결과보다 ResNet 50 FPN이나 ResNeXt 101 FPN으로 증가할 때 AP 성능이 일관성있게 향상되는 것을 볼 수 있다.
 즉, backbone이 좋아질수록 결과도 좋아질수도 있다는 것을 나타낸다는 시사점을 준다.
 multinomial mask와 independent mask를 사용했을 때, softmax classification과 sigmoid classification을 사용했을 때 성능 결과이다.
 여기서는 softmax보다 sigmoid에서 성능 향상이 있었다.
 (c)는 기존의 RoI pooling을 사용한 것보다 RoIAlign이 일관성있게 높은 결과를 내는 것을 알 수 있다.
 (d)는 RoI pooling과 RoIAlign을 비교한 것으로 ablation study를 통해 RoIAlign이 RoIpooling에 비해 object detection에서 mask level과 bounding box level에 성능 향상을 꾸준히 보여주는 것을 확인할 수 있다.
 stride는 16 feature를 사용했다고 한다.
 (e)는 mask branch에 대한 결과로 resolution size를 바꿔서 표현을 하거나 MLP를 다르게 쌓은 경우는 Fully Convolution Network로 FCN을 통과시킨 것이 fc(fully convolution layer)를 통과시킨 것보다 지역적인 정보를 유지하면서 propagation을 하기 때문에 더 높은 성능을 내었다고 한다.
 결과적으로 RoIAlign이나 backbone 향상, network 구조의 변환이 어떻게 변하는지 알 수 있다.
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/f8f0ccff-9c22-48b7-be5e-c413a5baab93)
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/28b6343d-9852-41a2-8263-fc644e215a7b)
 figure 8같은 경우는 failure case로 사람이 너무 겹쳐있는 경우에는 실패하기도 한다. 
