---
layout: post
title: "SL Foundation"
description: "ì´í™”ì—¬ìëŒ€í•™êµ ê°•ì œì› êµìˆ˜ë‹˜(Electronic & Electrical Engineering)"
date: 2023-02-21
tags: study
comments: true
---

image, video 3ì°¨ì› ë°ì´í„°ì™€ ê°™ì´ ë¯¸ë””ì–´ë¥¼ êµ¬ì„±í•˜ëŠ” ì—¬ëŸ¬ ê°€ì§€ ë°ì´í„°ì— ëŒ€í•œ ì¸ê³µì§€ëŠ¥ algorithm ì—°êµ¬

ìµœê·¼ì— ìš°ë¦¬ê°€ ì¦ê¸°ëŠ” ë¯¸ë””ì–´ëŠ” ì£¼ë¡œ ì‚¬ëŒì´ ë§Œë“¤ê³  ì†Œëª¨ë¥¼ í•˜ì˜€ì§€ë§Œ ìš”ì¦˜ì€ ê¸°ê³„ê°€ ëŒ€ì‹ í•˜ê³  ìˆìŒ. 
ë”°ë¼ì„œ ê¸°ì¡´ê³¼ ê°™ì´ ì‚¬ëŒì˜ ì†Œí†µ ì´ì™¸ì—ë„ ì‚¬ëŒê³¼ ê¸°ê³„, ê¸°ê³„ì™€ ê¸°ê³„ ì‚¬ì´ì˜ ì†Œí†µì„ ë§¤ê°œë¡œ í•˜ëŠ” ë¯¸ë””ì–´ì— ëŒ€í•œ ì—°êµ¬

Supervised Learningì˜ ê¸°ë³¸ ê°œë…ê³¼ ì›ë¦¬

Supervised Learningì€ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ ì •ë‹µê³¼ í•™ìŠµ ë¬¸ì œë¥¼ ì•Œë ¤ì£¼ì–´ ìƒˆë¡œìš´ ë¬¸ì œì— ëŒ€í•´ ìƒˆë¡­ê²Œ ë‹µì„ í•  ìˆ˜ ìˆëŠ” í•™ìŠµ ë°©ì‹

---

# ì§€ë„ í•™ìŠµ

## Learning from data

![image](https://user-images.githubusercontent.com/122149118/220206186-a7a5fe04-cacc-4322-bf11-3e8b73a0a3bc.png)

 * What is this animal?
    You would answer the question in few seconds

 * Will you need a zoological(mathematical) definition to distinguish it?
    Yes or **No**
 **We have learned a lot from data (pattern) despite being ignorant of a rigorous definition of a lion**

 ì´ ë™ë¬¼ì´ ë¬´ì—‡ì¸ì§€ ë¬¼ì–´ë³´ê²Œ ë˜ë©´ ì¦‰ê° ì‚¬ìë¼ê³  ë‹µì„ í•˜ê²Œ ë  ê²ƒì´ë‹¤.
 ê·¸ëŸ°ë° ì´ ë™ë¬¼ì´ ì‚¬ìì¸ì§€ ì–´ë–»ê²Œ ì•Œê²Œ ë˜ì—ˆëŠ”ì§€ ë¬»ëŠ”ë‹¤ë©´ ì•„ë§ˆ ëŒ€ë¶€ë¶„ ë™ë¬¼í•™ì ìœ¼ë¡œ ì—„ë°€í•œ ì •ì˜ë¡œë¶€í„° ì•Œê²Œë˜ì—ˆë‹¤ê¸°ë³´ë‹¤ëŠ” ê²½í—˜ì ìœ¼ë¡œ ì•Œê²Œ ë˜ì—ˆë‹¤ê³  ë§ì„ í•  ìˆ˜ê°€ ìˆì„ ê²ƒì´ë‹¤.
 ì„ ìƒë‹˜ì´ë‚˜ ë¶€ëª¨ë‹˜ ë˜ëŠ” ì±…ì—ì„œ ì´ ê·¸ë¦¼ì€ ì‚¬ìë¼ê³  ê³„ì†í•´ì„œ êµìœ¡ì„ ë°›ê²Œ ë˜ë©´ ì´í›„ì—ë„ ë„¤ ë°œì„ ê°€ì§€ê³  ì´ë¹¨ì´ ë‚ ì¹´ë¡­ê³  ê·¸ë¦¬ê³  ëª© ì£¼ìœ„ì— ê°ˆê¸°ë¥¼ ê°€ì§„ ê²ƒê³¼ ê°™ì€ íŒ¨í„´ì„ ê°€ì§„ ë™ë¬¼ì€ ì‚¬ìì„ì„ ì´í•´ í•˜ê²Œ ë  ê²ƒì´ë‹¤.
 ê·¸ë¦¬ê³  ì´í›„ë¡œë„ ë¹„ìŠ·í•˜ê²Œ ì§€ì†ë˜ëŠ” ì…ë ¥ì„ ë°›ëŠ”ë‹¤ê³  í•œë‹¤ë©´ ê³„ì†í•´ì„œ ê·¸ ì§€ì‹ì„ ê³µê³ í•˜ê²Œ ê°€ì§€ëŠ” í•™ìŠµ ê³¼ì •ì„ ê±°ì¹˜ê²Œ ë  ê²ƒì´ë‹¤.
 <mark>Machine Learningë„ ì´ì™€ ê°™ì€ ì‚¬ëŒì˜ ê²½í—˜ì ì¸ í•™ìŠµ ê³¼ì •ê³¼ ìœ ì‚¬í•˜ê²Œ dataë¡œ ë¶€í„° ë‚´ì¬ëœ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ê³¼ì •ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.</mark>

## Machine learning problems

 1. Is it a spam mail or not?
 2. Image recognition
 ![image](https://user-images.githubusercontent.com/122149118/220376517-aaf61bb5-86a0-4415-ac34-7c1db82ee1a6.png)
 3. House prices

 ì£¼ìœ„ì—ëŠ” ë‹¤ì–‘í•œ Machine Learning Taskë“¤ì´ ìˆë‹¤.
 ì˜ˆë¥¼ ë“¤ì–´ ë°›ì€ ë©”ì¼ì´ ìŠ¤íŒ¸ ë©”ì¼ì¸ì§€ ì•„ë‹Œì§€ë¥¼ íŒë‹¨í•˜ëŠ” ë¬¸ì œë¼ë“ ê°€ ì•„ë‹ˆë©´ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì´ìš©í•´ì„œ ê°œì¸ì§€ ê³ ì–‘ì´ì¸ì§€ ì•„ë‹ˆë©´ ì‚¬ìì¸ì§€ ë“±ê³¼ ê°™ì´ ìˆ˜ë°± ë˜ëŠ” ìˆ˜ì²œ ê°œì˜ ì¹´í…Œê³ ë¦¬ ì¤‘ì— í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œ, ì´ëŸ¬í•œ ë¬¸ì œë¥¼ Image recognition ë¬¸ì œë¼ê³  í•œë‹¤.
 Image Recognition ë¬¸ì œëŠ” ImageNet Large Scale Visual Challenge ë“±ì—ì„œì™€ ê°™ì´ ìµœê·¼ì— Deep Learning ì—°êµ¬ê°€ ê¸°ì¡´ì˜ ì—°êµ¬ ë°©ë²• ëŒ€ë¹„ ì»´í“¨í„° ë¹„ì „ ë“±ì—ì„œ ëŒ€ë‹¨íˆ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ì œê³µí•œë‹¤ëŠ” ìƒì§•ì ì¸ ê²°ê³¼ë¥¼ ë³´ì´ê¸°ë„ í–ˆë‹¤.
 ë˜ ì§‘ê°’ì˜ ì¶”ì´ë¥¼ ì—¬ëŸ¬ í™˜ê²½ì ì¸ ìš”ì¸ìœ¼ë¡œë¶€í„° ì˜ˆì¸¡ì„ í•˜ê±°ë‚˜ í†µê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ëŠ” ë“±ì˜ ë¬¸ì œ ë“±ì´ ìˆë‹¤.
 ì´ëŸ¬í•œ ë¬¸ì œë“¤ì€ ê³ ìœ í•œ Machine Learning ë¬¸ì œ ì¤‘ í•˜ë‚˜ì´ë‹¤.
 ğŸ“ŒMachine Learning ë¬¸ì œ
 > Binary Classification

 > Multiclass Classification

 > Regression

 Supervised Learningì˜ í•œ ì¢…ë¥˜ì´ë‹¤.
 Machine Learningì€ ì—¬ëŸ¬ ê°€ì§€ í•™ìŠµ ë¬¸ì œë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤.

 ![image](https://user-images.githubusercontent.com/122149118/220401188-6e3f2ba7-cbd7-4408-91b3-220a130aef79.png)

 ê·¸ ì¤‘ì— ëŒ€í‘œì ìœ¼ë¡œ Supervised Learningê³¼ Unsupervised Learningìœ¼ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤.
 ì´ ì¤‘ì—ëŠ” ë¬¼ë¡  Reinforcement Learningì´ë¼ê³  í•˜ëŠ” Machine Learningì˜ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ í•™ìŠµ ë°©ì‹ë„ ìˆì§€ë§Œ <mark>ì—¬ê¸°ì—ì„œ Unsupervised Learningê³¼ ë¹„êµë¥¼ í•˜ì—¬, Supervised Learningì€ dataì˜ labelì´ ìˆë‹¤ê³  í•˜ëŠ” ê²ƒì´ ê°€ì¥ í° ì°¨ì´ê°€ ëœë‹¤.
 Supervised Learningì—ëŠ” Regressionê³¼ Classification ë¬¸ì œê°€ ìˆìœ¼ë©° ê°ê° ì¶œë ¥ì´ ì—°ì† ë³€ìˆ˜ì¸ì§€ ì´ì‚° ë³€ìˆ˜ì¸ì§€ì— ë”°ë¼ êµ¬ë¶„ì„ í•˜ê²Œ ëœë‹¤.</mark>

## Supervised learning

 Supervised Learningì˜ Data set êµ¬ì„±ì— ëŒ€í•´ ì•Œì•„ë³¸ë‹¤.

 * Given a set of labeled examples (x<sup>1</sup>,y<sup>1</sup>),...,(x<sup>n</sup>, y<sup>n</sup>), learn a mapping function g : <i>X â†’ Y</i>, such that given an unseen sample x', associated output y' is predicted.

 ![image](https://user-images.githubusercontent.com/122149118/220402567-abc0fd16-322d-4cf8-84ff-4f113498b0a3.png)

 Supervised Learningì—ì„œ ì‚¬ìš©í•˜ëŠ” data sampleì€ ì…ë ¥ Xì™€ ì¶œë ¥ yì˜ ìŒìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.
 <mark>ì´ ë•Œ ì¶œë ¥ yë¥¼ label ë˜ëŠ” ì •ë‹µì´ë¼ê³  ì´ì•¼ê¸° í•œë‹¤.</mark>
 ì˜ˆë¥¼ ë“¤ì–´ ì˜¤ë¥¸ìª½ ì•„ë˜ ì˜ˆì‹œë¥¼ ë³´ë©´ ê° ì´ë¯¸ì§€ë“¤ì´ ì…ë ¥ xë¡œ ì‚¬ìš© ë˜ê³  ìˆê³ , ì´ ê·¸ë¦¼ì—ì„œì˜ cat, dog, mun ë“±ê³¼ ê°™ì€ label ë“¤ì´ ì¶œë ¥ yë¡œ ì‚¬ìš© ë˜ê³  ìˆë‹¤.
 ì´ëŸ¬í•œ data setì˜ êµ¬ì„±ì„ ì´ìš©í•˜ì—¬ ë‹¬ì„±í•˜ê³ ì í•˜ëŠ” ëª©ì ì€ ë°”ë¡œ <i>X â†’ Y</i>ë¡œ ê°€ëŠ” í•¨ìˆ˜ hë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤.
 ì¦‰, ìƒˆë¡œìš´ ì…ë ¥(ì˜ìƒ)ì´ ë“¤ì–´ì™”ì„ ë•Œ, í•´ë‹¹ ì˜ìƒì˜ category labelì„ ë§ì¶”ê²Œ ë˜ëŠ” ê·¸ëŸ¬í•œ í•¨ìˆ˜ hë¥¼ í•™ìŠµì„ í•˜ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.

## Learning pipline

 Supervised Learningì˜ ê³¼ì •ì„ ë” ìì„¸í•˜ê²Œ ë„ì‹í™”í•´ì„œ ë³¸ë‹¤.

 ![image](https://user-images.githubusercontent.com/122149118/220404787-27cba1bc-57de-4ed2-9d41-c80cae1a5a26.png)

 í¬ê²Œ ë‘ ê°€ì§€ ë‹¨ê³„ë¡œ êµ¬ë¶„ì´ ë˜ì–´ìˆë‹¤.
 ì²« ë²ˆì§¸ëŠ” modelì˜ training ê³¼ì •ì´ë‹¤.
 ì²˜ìŒì—ëŠ” modelì˜ ì¶œë ¥ì´ ì •í™•í•˜ì§€ ì•Šì„ ê²ƒì´ë‹¤.
 Machine learning modelì´ ë‹¤ìŒê³¼ ê°™ì€ ì¶œë ¥ì„ ì œê³µí•˜ê²Œ ë˜ë©´ í˜„ì¬ ê°€ì¥ ì´ˆê¸° ë‹¨ê³„ì—ì„œì˜ machine learning modelì€ ì •ë‹µì„ ì •í™•í•˜ê²Œ ë§ì¶”ì§€ ëª»í•  ê²ƒì´ë‹¤.
 í•˜ì§€ë§Œ ê·¸ ì •ë‹µì„ ê³„ì†í•´ì„œ machine learning modelì´ í•™ìŠµì„ í•˜ëŠ” ê³¼ì •ì´ í‘œí˜„ë˜ì–´ ìˆë‹¤.
 machine learning modelì´ ì œê³µí•˜ëŠ” ë‹µì•ˆì„ ë¬´ì—‡ìœ¼ë¡œë¶€í„° êµì •ì„ í•  ìˆ˜ ìˆëŠ”ê°€?
 ê°€ì§€ê³  ìˆëŠ” data setì˜ label ì¦‰ ê°€ì§€ê³  ìˆëŠ” ì •ë‹µìœ¼ë¡œë¶€í„° Machine Learning modelì´ ì´ ì¶œë ¥ì„ ì •í™•í•˜ê²Œ ë§ì¶”ê³  ìˆëŠ” ê²ƒì¸ì§€ ê·¸ë ‡ì§€ ì•Šì€ì§€ì— ëŒ€í•´ì„œ ì•Œë ¤ì£¼ê²Œë” í•˜ëŠ” ê²ƒì´ë‹¤.
 ê·¸ë¦¬ê³  Machine Learning modelì€ ì´ëŸ¬í•œ ì •ë‹µì„ ë³´ë‹¤ ë” ì •í™•í•˜ê²Œ ë§ì¶”ê²Œë” modelì˜ parameter ê°’ì„ ë³€ê²½í•´ ë‚˜ê°ˆ ìˆ˜ê°€ ìˆì„ ê²ƒì´ë‹¤.
 ì´ì™€ ê°™ì´ Supervised Learningì—ì„œëŠ” model outputê³¼ ì •ë‹µê³¼ì˜ ì°¨ì´ì¸ errorë¥¼ í†µí•´ì„œ ê·¸ errorë¥¼ ì¤„ì—¬ê°€ë©´ì„œ í•™ìŠµì´ ì§„í–‰ ë˜ê²Œ ëœë‹¤.
 ë°”ë¡œ í•™ìŠµ sampleì˜ labelì´ ì£¼ì–´ì§ìœ¼ë¡œ ê°€ëŠ¥í•œ ì¼ì´ë©° ì´ê²ƒì´ ë°”ë¡œ Supervised Learningì˜ ê¸°ë³¸ì ì¸ ì•„ì´ë””ì–´ê°€ ëœë‹¤.
 ì´ì–´ì„œ ë‘ ë²ˆì§¸ ë‹¨ê³„ì¸ ì¦‰, test ë‹¨ê³„ì—ì„œëŠ” modelì´ ì‹¤ì œ í™˜ê²½ì— ì ìš©ë˜ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ê²Œ ëœë‹¤.
 ì´ë•ŒëŠ” training ë‹¨ê³„ì—ì„œ ë³´ì§€ ëª»í–ˆë˜ ìƒˆë¡œìš´ ì…ë ¥ ì˜ìƒì„ ì´ìš©í•˜ê²Œ ëœë‹¤.
 ì´ ë•Œ modelì˜ ì •í™•ë„ê°€ ì§„ì •í•œ ì„±ëŠ¥ì´ ë  ê²ƒì´ë‹¤.

## Example: family car

 Supervised Learning ë¬¸ì œì˜ formulationì„ ìœ„í•´ family carë¥¼ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œë¥¼ ì˜ˆì‹œë¡œ ìƒê°í•´ë³¸ë‹¤.

 * Class <i>C</i> of a "family car"
    - Prediction is this car a family car?
 * Output:
    Positive (+) and negative (-) examples, or multi-class examples
 * Input representations:
    x<sub>1</sub>: price, x<sub>2</sub>: engine power
 ![image](https://user-images.githubusercontent.com/122149118/220407518-82b075cb-c812-4ea9-97a1-d3ed4f47a253.png)
 ì£¼ì–´ì§„ ë¬¸ì œëŠ” ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ì°¨ëŸ‰ì´ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì˜¤ê²Œ ë  ë•Œ ì´ ì°¨ëŸ‰ì´ family carì¸ì§€ ê·¸ë ‡ì§€ ì•Šì€ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œì´ë‹¤.
 ì´ ë•Œ ì°¨ëŸ‰ dataì˜ ì…ë ¥ í‘œí˜„ìœ¼ë¡œ ì°¨ëŸ‰ì˜ ê°€ê²©ê³¼ ì—”ì§„ íŒŒì›Œë¥¼ ê³ ë ¤í•œë‹¤.
 ì´ëŸ¬í•œ ì…ë ¥ í‘œí˜„ì„ ì…ë ¥ featureë¼ê³ ë„ ì´ì•¼ê¸° í•œë‹¤.
 ì…ë ¥ featureëŠ” í’€ê³ ì í•˜ëŠ” ë¬¸ì œì— ê´€í•˜ì—¬ ì–´ëŠ ì •ë„ì˜ domain knowledgeë¥¼ ì•Œê³  ìˆì–´ì•¼ ì‚¬ìš©í•  ìˆ˜ ìˆê¸°ë„ í•œë‹¤.
 ì¦‰, ì°¨ì— ê´€í•´ ì§€ì‹ì´ ì „ë¬´í•˜ë‹¤ë©´ ì´ëŸ¬í•œ classification ë¬¸ì œì—ì„œ íš¨ê³¼ì ì¸ input featureë¥¼ ë„ì…í•˜ê¸° ì–´ë ¤ìš¸ ê²ƒì´ë‹¤.
 ì…ë ¥ featureì˜ designì€ Machine Learning Engineerê°€ í•  ìˆ˜ë„ ìˆì§€ë§Œ ê´€ë ¨ ë¶„ì•¼ì˜ ì „ë¬¸ì„±ì„ ê°€ì§„ ì „ë¬¸ê°€ì™€ í•¨ê»˜ í•´ê²°í•  ìˆ˜ë„ ìˆë‹¤.
 ë¬¼ë¡  ìµœê·¼ì˜ Deep Learning modelê³¼ ê°™ì€ ê²½ìš°ëŠ” ì´ëŸ¬í•œ feature ì—­ì‹œ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— domain knowlegeë¡œë¶€í„° ë¹„êµì  ììœ ë¡œìš´ í¸ì´ë‹¤.

## Problem formulation

 ì°¨ëŸ‰ sampleì€ ì •ì˜í•œ ì…ë ¥ featureì— ì˜í•´ ë‹¤ìŒê³¼ ê°™ì€ ì¢Œí‘œ ê³µê°„ì— í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤.

 ![image](https://user-images.githubusercontent.com/122149118/220408900-58d9151f-b82b-427d-ac5b-9404d3e8dbb7.png)

 xì¶•ìœ¼ë¡œëŠ” price ì°¨ëŸ‰ì˜ ê°€ê²©ì„ í‘œì‹œí•˜ê²Œ ë˜ê³  yì¶•ìœ¼ë¡œëŠ” ì°¨ëŸ‰ì˜ engine powerë¥¼ í‘œì‹œí•˜ê²Œ ëœë‹¤ë©´ ê°ê°ì˜ ì°¨ëŸ‰ì˜ sampleë“¤ì„ ìš°ë¦¬ê°€ xy coordinate ìƒì—ì„œ í‘œí˜„ì„ í•  ìˆ˜ ìˆê²Œëœë‹¤.
 ê·¸ë¦¼ì—ì„œ ë³´ëŠ” ë°”ì™€ ê°™ì´ (+) í‘œí˜„ì´ ë˜ì–´ ìˆëŠ” ê²ƒì€ family carë¼ê³  categorizationì„ í•œ positive samplì´ ë˜ê²Œ ë˜ëŠ” ê²ƒì´ê³  (-)ë¼ê³  í‘œí˜„ì„ í•˜ê²Œ ëœ ê²ƒì€ familu carê°€ ì•„ë‹Œ negative sampleì„ ì˜ë¯¸ í•˜ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.
 ì•ì„œ ì–¸ê¸‰í•œ ë¬¸ì œë¥¼ ì¡°ê¸ˆ ë” ì •í˜•í™”í•˜ê³  ì¼ë°˜í™”í•œë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.
 ì•ì˜ ë¬¸ì œì—ì„œëŠ” ì˜ˆì‹œë¡œ 2ì°¨ì› ê³µê°„ì— ë¬¸ì œë¥¼ ì •ì˜í–ˆì§€ë§Œ ë³´ë‹¤ ì¼ë°˜ì ìœ¼ë¡œ dì°¨ì›ì˜ ê³µê°„ì—ì„œ ì…ë ¥ feature vectorê°€ ìˆë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.
 ì¶œë ¥ì€ binary classificationì˜ ë¬¸ì œì¸ ê²½ìš° yes ë˜ëŠ” no ë¬¸ì œì´ë‹¤.

 * X = R<sup>d</sup> is an input space
    - R<sup>d</sup>: a <i>d</i>-dimensional Euclidean space
    - input vector x âˆˆ X: x = (x<sub>1</sub>, x<sub>2</sub>,..., x<sub>d</sub>)
 * Y is an output space
    - Binary (yes/no) decision
 * Now, we want to approximate a target function <i>f</i>
    - <i>f</i>: <i>X â†’ Y</i> (unknown ideal function)
    - Data (x<sup>1</sup>, y<sup>1</sup>),..., (x<sup>N</sup>, y<sup>N</sup>); dataset where y<sup>N</sup> = <i>f(x<sup>N</sup>)</i>
    - Correct label is ready for a training set
    - **<u>Hypothesis</u>** <i>g: X â†’ Y</i> (ML model to approxivate <i>f</i>): <i>g âˆˆ **H**</i>
 Target functionì€ ì…ë ¥ xë¥¼ ì¶œë ¥ yë¡œ mappingì„ í•˜ê²Œë˜ëŠ” ì •ë‹µí•¨ìˆ˜ê°€ ëœë‹¤.
 í•˜ì§€ë§Œ ì´ í•¨ìˆ˜ëŠ” ë™ì‹œì— ë‹¬ì„±í•˜ê¸° ì–´ë ¤ìš´ ì¦‰, ì´ìƒì ì¸ í•¨ìˆ˜ê°€ ëœë‹¤.
 ê·¸ ì´ìœ ëŠ” ëª¨ë“  ì…ë ¥ sampleì— ëŒ€í•´ family carì¸ì§€ ê·¸ë ‡ì§€ ì•Šì€ì§€ë¥¼ ë¶„ë¥˜ë¥¼í•˜ê³  ê·¸ ì„±ëŠ¥ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ì„œëŠ” ì „ ì„¸ê³„ ëª¨ë“  sampleë“¤ì„ ë‹¤ ê´€ì°°í•œ í›„ì—ì•¼ ê°€ëŠ¥í•˜ê²Œ ë  ê²ƒì´ë‹¤.
 ê·¸ë¦¬ê³  modelì€ ê·¸ëŸ¬í•œ ì¼€ì´ìŠ¤ì— ëŒ€í•´ì„œ ëª¨ë‘ ì„±ê³µì ì¸ ê²°ê³¼ë¥¼ ë‹¬ì„±í•´ì•¼í•œë‹¤.
 í•˜ì§€ë§Œ í˜„ì‹¤ì ìœ¼ë¡œ ê·¸ê²ƒì€ ë¶ˆê°€ëŠ¥í•œ ì¼ì´ë‹¤.
 ë‹¤ë§Œ í•  ìˆ˜ ìˆëŠ” ê²ƒì€ ê·¸ ë¬¸ì œë¥¼ ëŒ€í‘œí•  ìˆ˜ ìˆì§€ë§Œ ì œí•œì ì¸ ìˆ«ì ì˜ˆë¥¼ ë“¤ì–´ ë³´í†µ ìˆ˜ë°±ì—ì„œ ìˆ˜ì²œì˜ sampleë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ” datasetë§Œì„ í™œìš©í•˜ì—¬ ì´ëŸ¬í•œ target function fì˜ ê·¼ì ‘í•œ í•¨ìˆ˜ë¥¼ ë§Œë“œëŠ” ê²ƒì´ í•  ìˆ˜ ìˆëŠ” ì¼ì´ë‹¤.
 ì´ëŸ¬í•œ í•¨ìˆ˜ë¥¼ hypothesis Hë¼ê³  ì •ì˜ í•œë‹¤.
 Machine Learning Modelì€ ë°”ë¡œ ì´ëŸ¬í•œ í•¨ìˆ˜ì˜ ì—­í• ì„ í•˜ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.

## Learning model

 ![image](https://user-images.githubusercontent.com/122149118/220413737-6e9f7d4a-7101-4da4-84d1-3f3c057fce06.png)

 ğŸ” hypothesis í•¨ìˆ˜ì˜ í•™ìŠµ ê³¼ì •ì„ ìˆœì„œëŒ€ë¡œ ë³´ì´ê³  ìˆëŠ” ê²ƒì´ë‹¤. ë‹¨ê³„ë³„ë¡œ ì–´ë– í•œ ìš”ì†Œë¥¼ ê³ ë ¤í•´ì•¼ í•˜ëŠ”ê°€?
 â¡ ìš°ì„  ëª©ì ì€ ì•ì—ì„œ ì´ì•¼ê¸°í•œ ë°”ì™€ ê°™ì´ target function fë¥¼ ëª¨ì‚¬í•˜ëŠ” ê²ƒì´ê³ , ì´ëŸ¬í•œ ë°©ë²•ì ì¸ ìˆ˜ë‹¨ìœ¼ë¡œ real worldì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” sampleë“¤ì˜ realizationìœ¼ë¡œì„œì˜ data setì„ í™œìš©í•œë‹¤.
 ì´ì–´ì„œ modelì„ í•™ìŠµí•˜ëŠ” ê³¼ì •ì—ì„œëŠ” í¬ê²Œ features selection, model selection, optimization ê³¼ì •ì„ ê±°ì¹˜ê²Œ ëœë‹¤.
 ì•ì„  ê³¼ì •ë“¤ì€ ë³´í†µ machine learning engineerê°€ ê°œì…ì„ í•  ìˆ˜ë„ ìˆì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ì£¼ì–´ì§€ëŠ” ë¬¸ì œë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.
 ê·¸ ë’¤ì— ë‚˜ì˜¤ê²Œ ë˜ëŠ” ë¬¸ì œê°€ ì•ìœ¼ë¡œ machine learning engineerê°€ í’€ì–´ì•¼í•  ë³´ë‹¤ ë” ë³¸ê²©ì ì¸ ë¬¸ì œê°€ ëœë‹¤.
 ë¨¼ì € Model selectionì€ í’€ê³ ì í•˜ëŠ” ë¬¸ì œì— ê°€ì¥ ì í•©í•œ modelì„ ì„ íƒí•˜ëŠ” ê³¼ì •ì´ë‹¤.
 ì˜ˆë¥¼ ë“¤ì–´ target function fê°€ í‘¸ë¥¸ìƒ‰ ë°•ìŠ¤ì—ì„œ ë³´ì´ëŠ” ë°”ì™€ ê°™ì€ ë¶„í¬ë¡œ ìˆì„ ê²ƒìœ¼ë¡œ ìƒê°ëœë‹¤ë©´ ê·¸ì™€ ê°™ì€ decision boundary ì¦‰ ì‚¬ê°í˜•ì„ ê°€ì§€ëŠ” modelì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ë°”ëŒì§í•  ê²ƒì´ë‹¤.
 ê·¸ ì¤‘ì—ëŠ” ì„ í˜• modelë„ ìˆê³ , ì•„ë‹ˆë©´ neural networkì™€ ê°™ì€ ë¹„ì„ í˜• modelë„ ìˆë‹¤.
 <mark>Optimizationì€ model parameterë¥¼ ìµœì í™”í•˜ì—¬ modelì´ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ì œê³µí•˜ë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤.</mark>
 ì´ë ‡ê²Œ í•™ìŠµí•œ modelì€ ìƒˆë¡œìš´ sampleì— ëŒ€í•´ì„œë„ ì •í™•í•˜ë©´ì„œë„ ì¼ë°˜ì ì¸ ì„±ëŠ¥ì„ ì œê³µí•´ì•¼ë  ê²ƒì´ë‹¤.
 Machine Learningì€ ê·¸ ìì²´ë¡œ Dataì˜ ê²°í•ìœ¼ë¡œ ì¸í•œ ë¶ˆí™•ì‹¤ì„±ì„ í¬í•¨í•˜ê³  ìˆë‹¤.
 í•™ìŠµ ê³¼ì •ì—ì„œ ëª¨ë“  data sampleë“¤ì„ ê´€ì°°í•  ìˆ˜ê°€ ì—†ê¸° ë•Œë¬¸ì´ë‹¤.

## Model generalization

 * Learning is an ill-posed problem; data is limited to find a unique solution
 * **Generalization(Goal): a model needs to perform well on unseen data**
    - Generalization error E<sub><i>gen</i><sub>; the goal isto minimize this error, but it is impractical to compute in the real world
 > Learning from data â†’ Learning from error (supervision)
 * Use training/Validation/test set errors for the proxy

 Machine Learning modelì˜ í•™ìŠµ ê³¼ì •ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒ ì¤‘ì— í•˜ë‚˜ëŠ” ë°”ë¡œ Generalization ì¼ë°˜í™”ì´ë‹¤.
 ì¦‰, modelì´ í•™ìŠµ ê³¼ì •ì—ì„œ ìš°ë¦¬ê°€ ê´€ì°°ì„ í•˜ì§€ ëª»í•œ sampleì— ëŒ€í•´ì„œë„ ë°”ë¡œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ì œê³µí•  ìˆ˜ê°€ ìˆì–´ì•¼ ëœë‹¤.
 ì´ëŸ¬í•œ modelì˜ ì¼ë°˜í™”ëœ ì„±ëŠ¥ì„ ì¸¡ì •ì„ í•˜ê¸° ìœ„í•œ measurementë¡œì¨ ë°”ë¡œ generalization error Eë¥¼ ì •ì˜í•´ë³¸ë‹¤.
 ëª©ì ì€ generalization errorEë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ë‹¤.
 ì´ error functionì„ ìš°ë¦¬ê°€ ìµœì†Œí™”í•  ê¸¸ì€ ì—†ë‹¤.
 ì´ìƒì ì¸ í•¨ìˆ˜ê°€ ë˜ê¸° ë•Œë¬¸ì´ë‹¤.
 ë”°ë¼ì„œ Supervised Learningì—ì„œëŠ” training error, validation error, test errorë¥¼ í†µí•´ generalization errorë¥¼ ìµœì†Œí™”í•˜ë„ë¡ í•˜ëŠ” ë…¸ë ¥ì„ í•˜ê²Œ ëœë‹¤.

## Errors

 * Pointwise erroris measured on an each input sample: <i>e(h(x), y)</i>
 * Examples:
    âœ”squared error <i>e(h(x<sup>i</sup>, y<sup>i</sup>)) = (h(x<sup>i</sup>) - y<sup>i</sup>)<sup>2</sup></i>
    âœ”binary error <i>e(h(x<sup>i</sup>, y<sup>i</sup>)) = 1[h(x<sup>i</sup>) â‰  y<sup>i</sup>]</i>
 * From a pointwise error to overall errors
    âœ”<i>E[(h(x<sup>i</sup>) - y<sup>i</sup>)<sup>2</sup>]</i>

    If an input sample is chosen from training, validation, and testing datasets, the errors are called a training error (<i>E<sub>train</sub></i>), a validation error (<i>E<sub>val</sub></i>), and a testing error (<i>E<sub>test</sub></i>)
 ErrorëŠ” ê° sample ë³„ë¡œ pointwiseë¡œ ê³„ì‚°í•œë‹¤.
 ìˆ˜ì‹ì—ì„œ h(x)ëŠ” modelì˜ ì¶œë ¥ì´ê³ , yëŠ” ì •ë‹µì´ë‹¤.
 ëŒ€í‘œì ì¸ error í•¨ìˆ˜ë¡œ squared errorê°€ ìˆë‹¤.
 model ì¶œë ¥ê³¼ ì •ë‹µê³¼ì˜ ì°¨ì´ë¥¼ ì œê³±í•˜ì—¬ ê³„ì‚°í•œë‹¤.
 ë˜í•œ binary errorê°€ ìˆë‹¤.
 ë‚´ë¶€ì˜ logicì„ íŒë³„í•˜ì—¬ ë§ìœ¼ë©´ 0 í‹€ë¦¬ë©´ 1ì¸ í•¨ìˆ˜ì´ë‹¤.
 ì¦‰, ì´ í•¨ìˆ˜ëŠ” model ì¶œë ¥ê³¼ ì •ë‹µì´ í‹€ë¦¬ë©´ 1ì„ ì¶œë ¥í•˜ëŠ” error í•¨ìˆ˜ê°€ ëœë‹¤.
 ìµœì¢…ì ìœ¼ë¡œëŠ” <mark>data sampleì—ì„œ ë°œìƒí•˜ëŠ” ëª¨ë“  sampleë“¤ì˜ pointwise errorë¥¼ í•©ì³ì„œ overall errorë¥¼ ê³„ì‚°í•œë‹¤.
 ì´ëŸ¬í•œ overall errorë¥¼ ì†ì‹¤í•¨ìˆ˜, loss function í˜¹ì€ cost functionì´ë¼ê³  ë¶ˆë¦°ë‹¤.</mark>
 í•™ìŠµì„ í•˜ê²Œ ë˜ë©° errorë¥¼ ì¸¡ì •í•˜ê²Œë  sampleë“¤ì´ ë§Œì•½ training set, validation set, ê·¸ë¦¬ê³  testë¥¼ ìœ„í•œ sample setì—ì„œ ë½‘ì•„ ì¶”ì¶œì„ í•œë‹¤ê³  í•˜ì˜€ì„ ë•Œ ê° sampleì—ì„œ ë°œìƒí•˜ëŠ” errorë¥¼ E<sub>train</sub>, E<sub>val</sub>, E<sub>test</sub>ë¼ê³  ì´ì•¼ê¸° í•œë‹¤.

 * Training error <i>E<sub>train</sub></i> measured on a training set, which may or may not represent <i>E<sub>gen</sub></i>; used for fitting a model
 * Testing error <i>E<sub>test</sub></i> (not used in training), which can be used for a proxy of <i>E<sub>gen</sub></i>
 **Goal**: E<sub>test</sub> â‰ˆ E<sub>gen</sub> â‰ˆ 0
 **How to achieve the goal in practice?

 E<sub>train</sub>ì€ modelì„ ì£¼ì–´ì§„ data setì— ë§ì¶”ì–´ í•™ìŠµí•˜ëŠ”ë° ì‚¬ìš©í•˜ëŠ” errorì´ë‹¤.
 ì¦‰, ì£¼ì–´ì§„ sampleì—ì„œ model parameterë¥¼ ìµœì í™” í•˜ë„ë¡ ì‚¬ìš©í•œë‹¤.
 ê·¸ë ‡ê¸° ë•Œë¬¸ì— ì´ errorëŠ” E<sub>general</sub>ì„ approximationí•˜ëŠ”ë° ì í•©í•˜ì§€ ì•Šë‹¤.
 ëŒ€ì‹ ì— training sampleê³¼ overlapì´ ë˜ì§€ ì•Šë„ë¡, ì „ì²´ data setì—ì„œ ì¼ë¶€ sampleì„ ë”°ë¡œ ë¹¼ì„œ test sampleì„ ì •ì˜í•œë‹¤.
 ì´ sampleì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” errorë¥¼ E<sub>test</sub>ë¡œ ì •ì˜í•œë‹¤.
 ì´ errorëŠ” modelì´ real worldì—ì„œ ì ìš©ë  ë•Œ ë‚˜íƒ€ë‚˜ëŠ” generalization errorë¥¼ í‘œí˜„í•˜ëŠ” ê²ƒì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.
 ë‹¤ìŒ ëª©ì ì€ E<sub>test</sub>ê°€ 0ìœ¼ë¡œ ê·¼ì‚¬í•˜ë„ë¡ í•˜ì—¬ E<sub>general</sub> ì—­ì‹œ 0ìœ¼ë¡œ ê·¼ì‚¬í•  ìˆ˜ ìˆë‹¤ëŠ” ê¸°ëŒ€ë¥¼ ê°–ê²Œ í•˜ëŠ” ê²ƒì´ë‹¤.
 ì´ëŸ¬í•œ ê³¼ì •ì„ ì–´ë–»ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ”ê°€?
 2ê°€ì§€ì˜ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ ìˆ˜í–‰í•˜ê²Œëœë‹¤.
 * Split into two objectives:
 <i>1. E<sub>test</sub> â‰ˆ E<sub>train</sub></i>
 <i>2. E<sub>train</sub> â‰ˆ 0</i>
    Objective 1: make <i>E<sub>test</sub> â‰ˆ E<sub>train</sub></i>
       - Failure : <u>overfitting</u> â†’ high variance
       - Cure : <u>regularization, more</u> data
    Objective 2: make <i>E<sub>train</sub> â‰ˆ 0</i>
       - Failure : <u>underfitting</u> â†’ high bias
       - Cure : optimization, <u>more</u> complex model
 ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” E<sub>test</sub>ê°€ E<sub>train</sub>ê³¼ ê°€ê¹Œì›Œ ì§€ë„ë¡ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤.
 ë‘ ë²ˆì§¸ ë‹¨ê³„ëŠ” E<sub>train</sub>ì´ 0ìœ¼ë¡œ ê°€ê¹Œì›Œì§€ë„ë¡ í•™ìŠµì„ í•˜ê²Œ ëœë‹¤.
 ê°ê°ì˜ í•™ìŠµ ë°©ì‹ì€, ì„œë¡œ ìœ ì‚¬í•´ ë³´ì´ì§€ë§Œ ì‚¬ì‹¤ êµ‰ì¥íˆ ë‹¤ë¥´ë‹¤.
 ë¨¼ì € ì²« ë²ˆì§¸ ëª©ì ì¸ E<sub>test</sub>ê°€ E<sub>train</sub>ê³¼ ê°€ê¹Œì›Œ ì§€ë„ë¡í•˜ëŠ” ë°©ì‹ì€ ìš°ë¦¬ê°€ í•™ìŠµí•œ modelì´ ì¼ë°˜ì ì¸ ì„±ëŠ¥ì„ ì œê³µí•˜ë„ë¡ í•˜ëŠ” ì„±ëŠ¥ì„ ê°–ë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤.
 ì¼ë°˜ì ì¸ ì„±ëŠ¥ì´ë¼ê³  í•˜ëŠ” ê²ƒì€ variance, ë¶„ì‚°ì´ ì‘ë‹¤.
 í•˜ì§€ë§Œ ì´ëŸ¬í•œ í˜•íƒœê°€ ì‹¤íŒ¨ë¥¼ í•œë‹¤ë©´ ì´ê²ƒì„ overfitting, ê³¼ì í•© ë¬¸ì œë¼ê³  í•œë‹¤.
 ì¦‰ ìš°ë¦¬ê°€ í•™ìŠµí•œ trainingì˜ í™˜ê²½ì´ ì¼ë°˜ì ì¸ testí™˜ê²½ì—ì„œ ì ìš©ì„ í–ˆì„ ë•Œ ê·¸ modelì˜ ì„±ëŠ¥ì´ ë‹¬ë¼ì§€ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.
 ë‘ ë²ˆì§¸ë¡œëŠ” E<sub>train</sub>ì´ 0ìœ¼ë¡œ ê°€ê¹Œì›Œì§€ë„ë¡ í•™ìŠµì„ í•˜ëŠ” ê²ƒì´ë‹¤.
 E<sub>train</sub>ì´ 0ìœ¼ë¡œ ê°€ê¹Œì›Œì§„ë‹¤ëŠ” ì˜ë¯¸ëŠ” modelì˜ í•™ìŠµì´ ì˜ ë˜ì–´ì§€ê³  ìˆë‹¤, ì¦‰ modelì˜ ì •í™•ë„ê°€ ì˜¬ë¼ê°€ê³  ìˆë‹¤ë¼ëŠ” ê²ƒì´ë‹¤.
 ì´ê²ƒì€ í¸ì°¨, biasê°€ ë‚®ì•„ì§„ë‹¤ë¼ê³  ì´ì•¼ê¸°í•  ìˆ˜ ìˆë‹¤.
 ì´ëŸ¬í•œ ê²½ìš°ê°€ ì‹¤íŒ¨ë¥¼ í•œë‹¤ë©´ underfitting, high biasì˜ ë¬¸ì œë¥¼ ê°€ì§€ê³  ìˆë‹¤ê³  í•œë‹¤.
 ì´ë•ŒëŠ” ë³´ë‹¤ ë³µì¡í•œ modelì„ ì‚¬ìš©í•˜ê±°ë‚˜ ìµœì í™”ë¥¼ ì˜ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ í•´ë²•ì´ ëœë‹¤.
 ê°ê°ì˜ ë¬¸ì œëŠ” í•´ê²° í•˜ëŠ” ë°©í–¥ ìì²´ê°€ ì•½ê°„ì˜ ì„œë¡œ ë‹¤ë¥¸ ë°©í–¥ì„±ì„ ê°€ì§€ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
 ğŸ“Œ model ì •í™•ë„ â¬†

 > bias â¬‡

 > model ì¼ë°˜ì„± â¬†

 > variance â¬‡
 
 modelì˜ ì •í™•ë„ë¥¼ ì˜¬ë¦¬ëŠ” ì¼ì€ biasë¥¼ ë‚®ì¶”ëŠ” ë°©ë²•ê³¼ modelì˜ ì¼ë°˜ì„±ì„ ë†’ì´ê³  varianceë¥¼ ë‚®ì¶”ëŠ” ì¢…ì¢… ì„œë¡œ ìƒë°˜ëœ í•´ê²°ì±…ì´ í•„ìš”í•œ ê²½ìš°ê°€ ìˆë‹¤.
 íŠ¹íˆ modelì˜ ë³µì¡ë„ë¥¼ ê²°ì •í•  ë•Œ íŠ¹ë³„íˆ ë” ê·¸ë ‡ë‹¤.

## Bias and Variance
 
 * Bias - error because the model can not represent the concept
 * Variance - error because a model overreacts to small changes(noise) in the training data

 Total loss = Bias + Variance (+noise)
 > bias
 
 ğŸ”ƒ Trade off ê´€ê³„
 
 > variance

 ì´ëŸ¬í•œ ê°œë…ì„ biasì™€ varianceì˜ trade off ê´€ê³„ê°€ ìˆë‹¤.

## Underfitting

 **Try a simple model**

 ![image](https://user-images.githubusercontent.com/122149118/220595124-5cfffc9a-df42-4199-b96c-cacbfda78dea.png)

 Underfitting problem beacause of using too simpler model than actual data distribution (high bias)

 underfittingì— ì˜í•œ high biasë¬¸ì œë¥¼ ë³´ì´ê³  ìˆë‹¤.
 ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒê³¼ ê°™ì€ waveë¥¼ ìƒê°í–ˆì„ ë•Œ ì˜ˆë¥¼ë“¤ì–´ positive sampleê³¼ negative sampleì´ ê·¸ë¦¼ì—ì„œì™€ ê°™ì´ ì¡´ì¬í•˜ê³  ìˆë‹¤.
 boundary decisionì´ ì´ìƒì ìœ¼ë¡œ ì¡´ì¬í•œë‹¤ê³  í•  ë•Œ wave ìœ„ ìª½ì— ì¡´ì¬í•˜ëŠ” sampleë“¤ì€ positive sampleë“¤ì´ë©° signer wave ì•„ë˜ì— ìœ„ì¹˜í•˜ëŠ” sampleë“¤ì€ negative sampleë“¤ì´ë‹¤. ì´ëŸ¬í•œ boundaryë¥¼ ìš°ë¦¬ì˜ modelì„ ì´ìš©í•˜ì—¬ í•™ìŠµì„ í•œë‹¤ê³  ìƒê°í–ˆì„ ë•Œ ê°€ì¥ ë¨¼ì € ì‹œë„í•´ë³¼ ìˆ˜ ìˆëŠ” ê²ƒì€ ê·¸ë¦¼ì—ì„œ ë³´ëŠ” ë°”ì™€ ê°™ì´ ì„ í˜• modelì´ ë  ê²ƒì´ë‹¤. 
 ì„ ì€ signer wave approximationí•´ì£¼ëŠ” decision boundaryê°€ ë  ê²ƒì´ë‹¤. 
 ì´ ëŒ€ê°ì„ ì— ì˜í•´ì„œ ìœ„ìª½ì— ìˆëŠ” sampleë“¤ì€ positive sampleë“¤ì´ ë  ê²ƒì´ê³  ì„  ì•„ë˜ìª½ì— ìˆëŠ” sampleë“¤ì€ negative sampleìœ¼ë¡œ íŒì •í•˜ê²Œ ëœë‹¤.
 ì´ ì‚¬ì´ì— ì¡´ì¬í•˜ëŠ” sampleë“¤ ì˜ˆì»¨ë° ì™¼ìª½ ì¢Œì¸¡ í•˜ë‹¨ ë™ê·¸ë¼ë¯¸ì¹œ ë¶€ë¶„ì— ìˆëŠ” sampleë“¤ì€ íš¨ê³¼ì ìœ¼ë¡œ ë¶„ë¥˜í•˜ì§€ ëª»í•˜ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
 ê·¸ ì´ìœ ëŠ” ì‹¤ì œë¡œ ì¡´ì¬í•˜ê²Œë˜ëŠ” decision boundaryë¥¼ ìš°ë¦¬ì˜ modelì´ ì¦‰, linearí•œ modelì´ íš¨ê³¼ì ìœ¼ë¡œ approximationí•  ìˆ˜ê°€ ì—†ê¸° ë•Œë¬¸ì´ë‹¤.
 ì´ë ‡ê²Œ ë°œìƒí•˜ëŠ” ì˜¤ì°¨ë¥¼ high biasë¼ê³  í•œë‹¤.

## Overfitting

 **Try a complex model**

 ![image](https://user-images.githubusercontent.com/122149118/220675921-c5bdc588-b09d-452c-80b1-7f6cee8bc508.png)

 Overfitting problem because of using more complex nodel than actual data distribution (high variance)

 ë°˜ëŒ€ë¡œ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì´ë ‡ê²Œ ì§€ë‚˜ì¹˜ê²Œ ë³µì¡í•œ modelì„ ë„ì…í•˜ê²Œ ë˜ë©´ ë°œìƒí•˜ëŠ” ë¬¸ì œì— ëŒ€í•´ ì•Œì•„ë³¸ë‹¤.
 ì˜ˆë¥¼ ë“¤ì–´ sine waveë¡œ êµ¬ì„±ë˜ì–´ìˆëŠ” decision boundaryë¥¼ ëª¨ì‚¬í•˜ê¸° ìœ„í•´ì„œ ê·¸ë¦¼ê³¼ ê°™ì´ ë³µì¡í•œ modelì„ ë„ì…í•˜ì—¬ ë¶„ë¥˜í•˜ê³  ìˆë‹¤ê³  ìƒê°í•´ë³¸ë‹¤.
 ê·¸ëŸ¬ë©´ ìµœì†Œí•œ ì´ phaseì— ë‚˜ì™€ìˆëŠ” negative sampleë“¤, ê·¸ë¦¬ê³  positive sampleë“¤ì€ modelì„ ê°€ì§€ê³  ì •í™•í•˜ê²Œ ë‹¤ ë¶„ë¥˜í•˜ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
 í˜„ì‹¤ ì„¸ê³„ì—ì„œ ì´ê²ƒì´ ê³¼ì—° ì •í™•í•˜ë‹¤ê³  í•  ìˆ˜ ìˆëŠ”ê°€?
 í˜„ì‹¤ ì„¸ê³„ì—ì„œëŠ” ìš°ë¦¬ê°€ ì•Œê³ ìˆê¸°ë¡œëŠ” ì™¼ìª½ì— positive, ì˜¤ë¥¸ìª½ì— negative sampleë“¤ì´ ì¡´ì¬í•˜ê²Œ ë˜ëŠ”ë° ë”°ë¼ì„œ ì´ëŸ¬í•œ modelì„ ë„ì…í•˜ê²Œ ë˜ë©´ modelì—ì„œ í‘¸ë¥¸ìƒ‰ìœ¼ë¡œ í‘œí˜„ì´ ë˜ì–´ìˆëŠ” ë¶€ë¶„ë“¤ì€ ì „ë¶€ negative sampleë“¤ì´ ì¡´ì¬í•˜ê³  ìˆëŠ” ì˜ì—­ì´ ëœë‹¤.
 í•˜ì§€ë§Œ modelì„ í†µí•´ì„œëŠ” ì „ë¶€ positive sampleë¡œ êµ¬ë¶„í•˜ê³  ìˆëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
 ì´ëŸ¬í•œ ë¬¸ì œëŠ” ì´ modelì´ ì§€ë‚˜ì¹˜ê²Œ ë³µì¡í•˜ê²Œ tuning, í•™ìŠµì´ ë˜ì–´ ì˜¤ë¶„ë¥˜ê°€ ì¼ì–´ë‚˜ê³  ìˆë‹¤.
 ì´ ë¬¸ì œë¥¼ overfittingì´ë¼ê³  í•œë‹¤.

## Bias-variance trade-off

 ì´ì™€ ê°™ì´ modelì˜ ë³µì¡ë„ì— ì˜í•´ ë‘ ê°€ì§€ì˜ ëª©ì ì€ ì„œë¡œ trade-offì˜ ê´€ê³„ë¥¼ ê°€ì§€ê³  ìˆë‹¤.

 * Split into two objectives:
    <i>1. E<sub>test</sub> â‰ˆ E<sub>train</sub></i>
    <i> 2. E<sub>train</sub> â‰ˆ 0</i>
 * Objective 1: make <i>E<sub>test</sub> â‰ˆ E<sub>train</sub></i>
       - Failure : <u>overfitting</u> â†’ high variance and low bias
       - if a model is too complex
 * Objective 2: make <i>E<sub>train</sub> â‰ˆ 0</i>
       - Failure : <u>underfitting</u> â†’ high bias and low variance
       - if a model is too simple
 
 ![image](https://user-images.githubusercontent.com/122149118/220680602-79c4a97f-4c77-47b5-8381-80d433bfaea0.png)

 modelì´ ë³µì¡í•´ì§€ë©´ overfittingì´ ë°œìƒí•˜ê¸° ì‰¬ìš°ë©° biasëŠ” ì¤„ì–´ë“¤ì§€ë§Œ varianceê°€ ì¦ê°€í•˜ê²Œ ë˜ì–´ modelì´ ìƒˆë¡œìš´ sampleì— ì¼ë°˜ì ì¸ ì„±ëŠ¥ì„ ì œê³µí•˜ê¸°ê°€ ì–´ë µë‹¤.
 ë°˜ëŒ€ë¡œ modelì´ ì§€ë‚˜ì¹˜ê²Œ ë‹¨ìˆœí•˜ë©´ underfittingì´ ë°œìƒí•˜ì—¬ biasê°€ ë†’ì•„ì ¸ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ì œê³µí•˜ê¸° ì–´ë µë‹¤.
 ì´ëŸ¬í•œ ë‘ê°œì˜ factorë¥¼ ê· í˜•ìˆê²Œ ë§ì¶° generalization errorë¥¼ ìµœì†Œí™”í•˜ë„ë¡ í•˜ëŠ” ê²ƒì´ í•™ìŠµê³¼ì •ì—ì„œ ì¤‘ìš”í•˜ë‹¤.
 
 * The two objectives have trade-off between approximation and generalization w.r.t model complexity

 ![image](https://user-images.githubusercontent.com/122149118/220680940-9724d344-3251-414b-a553-3e77e422b601.png)

 ì˜¤ë¥¸ìª½ìœ¼ë¡œ model complexityê°€ ëŠ˜ì–´ë‚˜ê²Œ ë˜ë©´ overfittingì´ ë°œìƒí•˜ê¸° ì‰¬ìš°ë©° ì´ ë•ŒëŠ” training errorì™€ test errorì˜ ê°„ê·¹ì´ ë²Œì–´ì§€ê³  ìˆëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
 ë°”ë¡œ varianceê°€ ì¦ê°€í•˜ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.
 ë°˜ëŒ€ë¡œ model complexityê°€ ê°ì†Œí•˜ê²Œ ë˜ë©´ ì´ê²ƒì„ underfitting zoneì´ë¼ê³  ì´ì•¼ê¸° í•˜ë©° ì´ ë•ŒëŠ” biasê°€ ë†’ì•„ì§€ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
 ì¦‰ ì´ ë‘ ê°œì˜ trade-offë¥¼ ì¡°ì ˆí•˜ì—¬ generalization errorë¥¼ ê°€ì¥ ìµœì†Œí•œìœ¼ë¡œ ë§Œë“œëŠ” ê²ƒì´ ìˆ™ì œì´ë‹¤.

 Deep Learningê³¼ ê°™ì€ ì˜¤ëŠ˜ë‚ ì˜ Machine Learning modelì€ ì´ë¯¸ì§€ë‚˜ ë¹„ë””ì˜¤ ë“±ê³¼ ê°™ì€ ê³ ì°¨ì›ì˜ dataë¥¼ ë‹¤ë£¨ê¸° ë•Œë¬¸ì— ê·¸ ë³µì¡ë„ê°€ êµ‰ì¥íˆ ì¦ê°€í•˜ê³  ìˆë‹¤.
 ğŸ“Œë³µì¡ë„ ì¦ê°€ ì†ë„ > Data set sample ìˆ˜
 â¡ Overfitting ë¬¸ì œ â¬†
 í•˜ì§€ë§Œ ë³µì¡ë„ê°€ ì¦ê°€í•˜ëŠ” ì†ë„ì— ë¹„í•´ì„œ data sampleì˜ ìˆ«ìëŠ” ìš°ë¦¬ê°€ ì‰½ê²Œ ëŠ˜ë¦¬ì§€ ëª»í•˜ëŠ” ë¬¸ì œê°€ ìˆì–´ ìš”ì¦˜ì˜ ë¬¸ì œë“¤ì€ overfittingì˜ ë¬¸ì œê°€ í˜„ì €íˆ ëŠ˜ì–´ë‚˜ê²Œëœë‹¤.
 ì´ëŸ¬í•œ ë¬¸ì œë¥¼ **Curse of dimension ì°¨ì›ì˜ ì €ì£¼ ë¬¸ì œ**ë¼ê³  ì´ì•¼ê¸° í•œë‹¤.
 ë§Œì•½ ì…ë ¥ data ë˜ëŠ” ì…ë ¥ featureì˜ ì°¨ì›ì´ ì¦ê°€í•œë‹¤ë©´ ì§€ìˆ˜ì ìœ¼ë¡œ sampleì˜ ìˆ«ìê°€ ëŠ˜ì–´ë‚˜ì•¼í•˜ì§€ë§Œ ê·¸ë ‡ê²Œ í•˜ê¸° ì–´ë µë‹¤.

## Avoid Overfitting

 * **Problem** 
 In today's ML problems, a complex model tends to be used to handle high-dimensional data (and relativelu insufficient number of data) ; prone to an overfitting problem
 * **Cures of dimension** Will you increase the dimensrion of the data to improve the performance as wee as maintain the density of the examples per bin> If so, you need to increase the data exponentially.

 ![image](https://user-images.githubusercontent.com/122149118/220686798-4420b9cb-02da-4e4d-92ea-525f387adde9.png)

 ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê°€ì¥ ì§ì ‘ì ì¸ ìˆ˜ë‹¨ìœ¼ë¡œëŠ” <mark>Dataë¥¼ ëŠ˜ë¦¬ëŠ” ê²ƒì´ë‹¤.</mark>
 ê°„ë‹¨í•œ ì¼ì²˜ëŸ¼ ë³´ì´ì§€ë§Œ ì¶©ë¶„í•œ ìˆ«ìì˜ dataë“¤ì„ í™•ë³´í•˜ê³  ë§Œë“œëŠ” ê²ƒì€ ì‰½ì§€ ì•Šë‹¤. 
 ê±°ê¸°ì— ë“œëŠ” ë¹„ìš©ê³¼ ë…¸ë ¥ì´ êµ‰ì¥íˆ ë§ì´ ì†Œìš”ë˜ê¸° ë•Œë¬¸ì´ë‹¤.
 ì´ë¥¼ ìœ„í•´ ë‹¤ì–‘í•œ data augmentation ë°©ë²•ì´ ìˆë‹¤.
 dataë¥¼ ë³€í˜•í•  ìˆ˜ë„ ìˆê³  ìµœê·¼ì—” computerë¡œ sampleì„ ìƒì„±í•˜ê±°ë‚˜ í•©ì„±í•˜ì—¬ ë³„ë„ì˜ ìƒì„± modelì„ ê°œë°œí•¨ìœ¼ë¡œì„œ sample ìˆ«ìë¥¼ ëŠ˜ë¦¬ê¸°ë„ í•œë‹¤.
 í˜„ì‹¤ì ìœ¼ë¡œëŠ” data sampleì˜ ìˆ«ìë¥¼ ëŠ˜ë¦¬ê¸° ì–´ë ¤ìš´ ê²½ìš°ì— ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ëª‡ ê°€ì§€ ê¸°ìˆ ì  ìˆ˜ë‹¨ì´ ìˆë‹¤.

 * **Remedy**
    âœ” **Data augmentation**
    âœ” Regularization to penalize complex models (variance reduction) ; make a model not too sensitive to noise or outliers (e.g. drop-out, LASSO)
    âœ” Ensemble : average over a number of models
 
  ![image](https://user-images.githubusercontent.com/122149118/220699366-82705f90-eeda-4e04-941d-ec4f36693e36.png)
 
 ì²« ë²ˆì§¸ ê¸°ìˆ ì ì¸ ìˆ˜ë‹¨ìœ¼ë¡œ Regularization ë°©ì‹ì´ ìˆë‹¤.
 ë‘ ë²ˆì§¸ë¡œëŠ” ì•™ìƒë¸” ëª¨ë¸ì— ëŒ€í•œ ë°©ë²•ì´ ìˆë‹¤.

## Cross-validation (CV)

 **k-fold cross-validation**

 ![image](https://user-images.githubusercontent.com/122149118/220704013-b85d63f0-1d1d-403b-86b1-7fe0997e72ff.png)

 * Training data set - used to train a model to fit data
 * Validation data set - used to provide unbiased evaluation of the model's fitness
 * Test data set - never been used in the training

 â†’ **cross-validaton** allows a better model to avoid overfitting (but more complexity)
 ê·¸ë¦¼ ìƒë‹¨ì—ì„œì™€ ê°™ì´ ì›ë˜ data setê°€ êµ¬ì„±ì´ ë˜ì–´ìˆì„ ë•Œ training setì™€ test setë¡œ êµ¬ë¶„í•˜ì—¬ í•™ìŠµì„ í•˜ê³  í‰ê°€ë¥¼ í•œë‹¤.
 ì´ ë•Œ training setë¥¼ ì´ëŸ°ì‹ìœ¼ë¡œ êµ¬ë¶„í•˜ê²Œ ëœë‹¤ë©´ dataë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµì„ í•˜ëŠ”ë° í™œìš©í•˜ì§€ ëª»í•  ìˆ˜ ìˆë‹¤.
 ê·¸ ë•Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë°”ë¡œ cross-validationì´ë¼ê³  í•œë‹¤.
 cross-validationì—ì„œëŠ” training setë¥¼ í¬ê²Œ kê°œì˜ ê·¸ë£¹ìœ¼ë¡œ êµ¬ë¶„í•˜ê²Œ ëœë‹¤. 
 ê·¸ë¦¬ê³  ì´ k - 1ê°œì˜ ê·¸ë£¹ì„ trainingì— í™œìš©í•˜ê²Œ ë˜ë©° ë‚˜ë¨¸ì§€ 1ê°œì˜ ê·¸ë£¹ì„ validationìœ¼ë¡œ ì´ìš©í•œë‹¤.
 trainingì€ model parameterë¥¼ fitting í•˜ëŠ”ë° ì‚¬ìš©í•˜ê²Œ ë˜ê³  validationì€ ì´ modelì˜ ìµœì í™”ë¥¼ ìœ„í•´ ì‚¬ìš©í•˜ê²Œ ëœë‹¤.
 ì§€ê¸ˆ í˜„ì¬ modelì—ì„œëŠ” kê°€ 5ì¸ modelì¸ 5 fold cross-validationì„ ë³´ì´ê³  ìˆë‹¤.
 ì´ëŸ¬í•œ ê³¼ì •ì„ 5ë²ˆ ì—°ì† ìˆ˜í–‰í•˜ì—¬ data sampleë“¤ì„ ìì—°ìŠ¤ëŸ½ê²Œ augmentationì„í•˜ì—¬ í•™ìŠµí•˜ëŠ”ë° í™œìš©í•˜ê²Œ ë˜ë©´ ì¡°ê¸ˆ ë” modelì„ ì¼ë°˜í™”ì‹œí‚¤ëŠ”ë° ë„ì›€ì„ ì¤„ ìˆ˜ ìˆë‹¤.
 ë¬¼ë¡  modelì˜ ìµœì¢… ì„±ëŠ¥ì€ ë°”ë¡œ test datasetë¥¼ ì´ìš©í•˜ì—¬ ì¸¡ì •í•œë‹¤.

## Quiz

 **1.** What are two examples of classification?

 **A.** Determine when a heater is on or off based on weather data 
 **B.** Translate the numbers or letters form a handwritten message to ASCII text
 **C.** Develop a mathmatical relationship between heater level (0-100%) and temperature(20-70â„ƒ)
 AëŠ” ë‚ ì”¨ì— ë”°ë¼ íˆí„°ë¥¼ í‚¤ê±°ë‚˜ ë„ëŠ” ë¬¸ì œ, BëŠ” í•„ê¸°ì²´ ì–¸ì–´ë¥¼ digital ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” ë¬¸ì œ, CëŠ” ì˜¨ë„ì— ë”°ë¼ ë³€í™˜ì„ í•˜ëŠ” ë¬¸ì œ

 **A Correct.** The classifier distinguishes between on or off with temperature and temperature derivatives as the features.

 ![image](https://user-images.githubusercontent.com/122149118/220705157-6c75b47d-37cf-4ca9-b6f1-aa46a94d1b86.png)

 ì£¼ì–´ì§„ ì…ë ¥ featureê°€ heaterë¥¼ í‚¤ê±°ë‚˜ ë„ëŠ” ë¬¸ì œ, ì¦‰ binary classification ë¬¸ì œë¼ê³  í•  ìˆ˜ ìˆë‹¤.
 **B Correct.** The classifier analyzes the pixels of each letter to determine the alpha-numeric value.

 ![image](https://user-images.githubusercontent.com/122149118/220705443-3caf0ff4-a258-4ff2-9179-db9ce65b0d9f.png)

 ê·¸ë¦¼ì—ì„œì™€ ê°™ì€ í•„ê¸°ì²´ê°€ ì£¼ì–´ì§„ë‹¤ë©´ 0ë¶€í„° 9ì— í•´ë‹¹í•˜ëŠ” class id, 0ì—ì„œ 9ê¹Œì§€ ì¤‘ì— í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ëŠ” multi-class classificationë¬¸ì œê°€ ëœë‹¤.

 CëŠ” ì—°ì† ë³€ìˆ˜ë¥¼ ì¶œë ¥í•˜ëŠ” taskì´ë¯€ë¡œ classificationì´ ì•„ë‹ˆë¼ regressionë¬¸ì œê°€ ëœë‹¤.

 **2.** What answers are correvt for supervised learning?

 **A.** Requires labeled data that reveals the measured or true outcome
 **B.** Training and test samples can be overlapped

 **A Correct.** Supervised learning uses labeled data to compute an error with a model output.
 supervised learningì—ì„œëŠ” label dataë¥¼ ì´ìš©í•´ ì´ label dataë¡œ í•˜ì—¬ê¸ˆ ì´ modelì˜ errorë¥¼ ì¸¡ì •í•˜ì—¬ í•™ìŠµì— ë°˜ì˜í•˜ë„ë¡ í•˜ëŠ” machanismì„ ê°€ì§€ê³  ìˆë‹¤.

 **B False.** Training and test samples must not be overlapped
 trainingê³¼ test sampleì´ overlapì´ ë˜ì–´ì„œëŠ” ê²°ì½” ì•ˆëœë‹¤.

## Summary

 * **Introduction to supervised learning**
    - Regression and calssification
    - Learning pipline of a supervised learning
       - Learning from data (error)
    - Overfittind Vs underfitting (Bias-variance trad-off)
    - Model generalization
       - Avoid overfitting and cross validation

 Supervised Learningì—ì„œëŠ” regressionê³¼ classificationë¬¸ì œê°€ ìˆë‹¤.
 data setì— ì£¼ì–´ì§„ ì •ë‹µ ì¦‰ labelê³¼ ë¹„êµí•˜ì—¬ errorë¥¼ ì¤„ì´ë©° í•™ìŠµí•œë‹¤.
 í•™ìŠµ ê³¼ì •ì—ì„œ overfittingê³¼ underfittingì— ëŒ€í•œ ë¬¸ì œì™€ ê·¸ ê´€ê³„ë¥¼ ì‚´í´ë³´ì•˜ë‹¤.
 Machine Learningì—ì„œëŠ” modelì˜ ì •í™•ë„ë¥¼ ì˜¬ë¦¬ëŠ” ì¼ ì™¸ì—ë„ ì²˜ìŒ ë³´ëŠ” sampleë“¤ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì„±ëŠ¥ì„ ì œê³µí•˜ëŠ” ì¼ ì—­ì‹œ ëŒ€ë‹¨íˆ ì¤‘ìš”í•˜ë‹¤.