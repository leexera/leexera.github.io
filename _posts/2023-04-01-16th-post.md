---
layout: post
title: "SSD Hands-on Practice"
description: "SSD_Pytorch"
date: 2023-04-01
tags: study, code
comments: true
---

# SSD Pytorch

###### 사용환경 : Google Colab

---

## Installing Dependencies
```python
!pip install torch==1.7.1
!pip install torchvision==0.8.2
!pip install os
!pip install opencv-python
!pip install numpy
```

## Mounting Drive
```python
cd /content/drive/SSD/SSD-pytorch
```
실행 위치 변경


## Importing Libraries
```python
import numpy as np
import cv2
from PIL import Image
import os
import itertools
from math import sqrt
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import shutil
import torch
import torch.nn as nn
import torchvision
from src.transform import SSDTransformer
from src.utils import generate_dboxes, Encoder, colors, coco_classes
from torchvision.datasets import CocoDetection
from torch.utils.data.dataloader import default_collate
from torchvision.models.resnet import resnet50
from torchvision.models.mobilenet import mobilenet_v2, InvertedResidual
from pycocotools.cocoeval import COCOeval
from tqdm.autonotebook import tqdm
from torch.optim.lr_scheduler import MultiStepLR
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from src.model import SSD, SSDLite, ResNet, MobileNetV2
from src.loss import Loss
import torch.nn.functional as F
from torchvision.ops.boxes import box_iou, box_convert
```

## Defining Variables
```python
cls_threshold=0.3
nms_threshold=0.5
epochs=65
batch_size=32
multistep=[43,54]
lr=2.6e-3
momentum =0.9
local_rank=0
num_workers=4
weight_decay= 0.0005
model="ssd"
parent_dir="/content/drive/MyDrive/SSD/SSD-pytorch"
pretrained_model_path=parent_dir +"/checkpoints"
ckpt = pretrained_model_path+"/SSD.pth"
dataset_path= "/content/drive/MyDrive/data"+"/coco"
save_folder=parent_dir+"/trained_models"
log_path=parent_dir+"/tensorboard"
dir_path = dataset_path+"/val2017"
#result_dir=parent_dir+"/Results"
```
hyper parameter 정의

## Helper Functions
```py
def train(model, train_loader, epoch, writer, criterion, optimizer, scheduler):
    model.train()
    num_iter_per_epoch = len(train_loader)
    progress_bar = tqdm(train_loader)
    scheduler.step()
    for i, (img, _, _, gloc, glabel) in enumerate(progress_bar):
        if torch.cuda.is_available():
            img = img.cuda()
            gloc = gloc.cuda()
            glabel = glabel.cuda()

        ploc, plabel = model(img)
        ploc, plabel = ploc.float(), plabel.float()
        gloc = gloc.transpose(1, 2).contiguous()
        loss = criterion(ploc, plabel, gloc, glabel)

        progress_bar.set_description("Epoch: {}. Loss: {:.5f}".format(epoch + 1, loss.item()))

        writer.add_scalar("Train/Loss", loss.item(), epoch * num_iter_per_epoch + i)

        # if is_amp:
        #     with amp.scale_loss(loss, optimizer) as scale_loss:
        #         scale_loss.backward()
        # else:
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()


def evaluate(model, test_loader, epoch, writer, encoder, nms_threshold):
    model.eval()
    detections = []
    category_ids = test_loader.dataset.coco.getCatIds()
    for nbatch, (img, img_id, img_size, _, _) in enumerate(test_loader):
        print("Parsing batch: {}/{}".format(nbatch, len(test_loader)), end="\r")
        if torch.cuda.is_available():
            img = img.cuda()
        with torch.no_grad():
            # Get predictions
            ploc, plabel = model(img)
            ploc, plabel = ploc.float(), plabel.float()

            for idx in range(ploc.shape[0]):
                ploc_i = ploc[idx, :, :].unsqueeze(0)
                plabel_i = plabel[idx, :, :].unsqueeze(0)
                try:
                    result = encoder.decode_batch(ploc_i, plabel_i, nms_threshold, 200)[0]
                except:
                    print("No object detected in idx: {}".format(idx))
                    continue

                height, width = img_size[idx]
                loc, label, prob = [r.cpu().numpy() for r in result]
                for loc_, label_, prob_ in zip(loc, label, prob):
                    detections.append([img_id[idx], loc_[0] * width, loc_[1] * height, (loc_[2] - loc_[0]) * width,
                                       (loc_[3] - loc_[1]) * height, prob_,
                                       category_ids[label_ - 1]])

    detections = np.array(detections, dtype=np.float32)

    coco_eval = COCOeval(test_loader.dataset.coco, test_loader.dataset.coco.loadRes(detections), iouType="bbox")
    coco_eval.evaluate()
    coco_eval.accumulate()
    coco_eval.summarize()

    writer.add_scalar("Test/mAP", coco_eval.stats[0], epoch)
```
training 함수로써, 먼저 model을 train하게 만듦 `model.train()` 
한 epoch의 iteration은 DataLoader가 가지고 있는 data의 개수를 정의 `num_iter_per_epoch` 
program의 progress bar를 보여줄 수 있는 tqdm package를 사용하여 표현
<del>scheduler.step()을 하는이유?</del>
`progress_bar`가 돌아가면서 image와 label이 나오게 됨
image와 location, label을 모두 cuda에 올림 `img.cuda()`
model에 image를 넣음
prediction된 label과 prediction된 location이 나옴
location은 bounding box를 의미하고 label은 그 class를 의미
이 것들을 floating point로 바꾸어줌 `ploc.plabel = ploc.float().plabel.float()`
<del>floating point로 바꾸어주는 이유?</del>
location을 transpose를 첫 번째 dimension과 두 번째 dimenstion의 위치를 바꾸어준 후에 `gloc = gloc.transpose(1,2).contiguous()` loss를 계산
`gloc = gloc.transpose(1,2).contiguous()`는 label과 ground truth 간에 데이터 형태를 맞추기 위함
loss는 prediction한 값 2개와 ground truth 값 2 개를 받아 계산
현재 epoch에 하나를 더한 epoch의 loss가 어떻게 되는지 표시하는 bar로 setting
한 iteration의 loss가 계산되었으니 `loss.backward`, `optimizer.step()`, `oprimizer.zero.grad()`진행
evaluate는 model을 먼저 evaluate로 setting `model.eval()`
category id를 test loader에서 getCatIds함수로 얻음 `category_ids = test_loader.dataset.coco.getCatIds()`
test loader를 돌려가며 image와 image id, size를 받아옴
image를 cuda로 만들어줌 `img = img.cuda()`
evaluate를 하는 것이 gradient를 쓰지 않는 것이므로 `torch.no_grad()` gradient를 안쓰게 한 후에 image를 inference해줌
prediction한 location과 label이 training 때와 똑같이 나올 것
prediction된 location들을 돌아가면서 개수만큼 model이 영상을 받아 ploc와 plabel을 출력
ploc와 plabel을 batch size에 따라서 location의 shape 첫 번째 위치에 batch가 indexing됨
batch index에 따라 나누어 구분해줌
`encoder.decoder_batch`는 NMS process를 수행하기 위함
decoder_batch는 box와 score number, nms threshold를 받아 bbox와 probability를 고려하여 decoder single을 호출하여 최종 output을 만듦
decoder single도 score를 이용해 nms threshold를 가지고 box들을 합치는 과정을 수행
encoder 함수는 최종적으로 가지고 있게 될 number of box를 SSD 출력 box의 개수인 8732개로 만들어 주고 decoder는 8732개의 box를 n개의 box로 만드는 과정
SSD는 input image size가 300이나 512로 정해져있음
실제 image size의 location은 따로 추정할 필요가 있음
batch size에 따라서 image size를 불러온 후 hight와 width로 받은 다음 추정된 location에 index들을 곱해주어 원하는 최종 detection 결과를 만들어냄
location 값은 영상 전체를 0에서 1사이의 값으로보고 prediction한 것
prediction한 후 width의 pixel값을 곱해주어 x축 좌표를 찾고 0에서 1사이의 값을 갖는 location 1에 height를 곱해주어 y축 pixel값을 찾음
torch에서 제공해주는 COCOeval함수를 사용하여 비교할 수 있음

## Defining SSD Model with backbone
```py
class Base(nn.Module):
    def __init__(self):
        super().__init__()

    def init_weights(self):
        layers = [*self.additional_blocks, *self.loc, *self.conf]
        for layer in layers:
            for param in layer.parameters():
                if param.dim() > 1:
                    nn.init.xavier_uniform_(param)

    def bbox_view(self, src, loc, conf):
        ret = []
        for s, l, c in zip(src, loc, conf):
            ret.append((l(s).view(s.size(0), 4, -1), c(s).view(s.size(0), self.num_classes, -1)))

        locs, confs = list(zip(*ret))
        locs, confs = torch.cat(locs, 2).contiguous(), torch.cat(confs, 2).contiguous()
        return locs, confs


class ResNet(nn.Module):
    def __init__(self):
        super().__init__()
        backbone = resnet50(pretrained=True)
        self.out_channels = [1024, 512, 512, 256, 256, 256]
        self.feature_extractor = nn.Sequential(*list(backbone.children())[:7])

        conv4_block1 = self.feature_extractor[-1][0]
        conv4_block1.conv1.stride = (1, 1)
        conv4_block1.conv2.stride = (1, 1)
        conv4_block1.downsample[0].stride = (1, 1)

    def forward(self, x):
        x = self.feature_extractor(x)
        return x


class SSD(Base):
    def __init__(self, backbone=ResNet(), num_classes=81):
        super().__init__()

        self.feature_extractor = backbone
        self.num_classes = num_classes
        self._build_additional_features(self.feature_extractor.out_channels)
        self.num_defaults = [4, 6, 6, 6, 4, 4]
        self.loc = []
        self.conf = []

        for nd, oc in zip(self.num_defaults, self.feature_extractor.out_channels):
            self.loc.append(nn.Conv2d(oc, nd * 4, kernel_size=3, padding=1))
            self.conf.append(nn.Conv2d(oc, nd * self.num_classes, kernel_size=3, padding=1))

        self.loc = nn.ModuleList(self.loc)
        self.conf = nn.ModuleList(self.conf)
        self.init_weights()

    def _build_additional_features(self, input_size):
        self.additional_blocks = []
        for i, (input_size, output_size, channels) in enumerate(
                zip(input_size[:-1], input_size[1:], [256, 256, 128, 128, 128])):
            if i < 3:
                layer = nn.Sequential(
                    nn.Conv2d(input_size, channels, kernel_size=1, bias=False),
                    nn.BatchNorm2d(channels),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(channels, output_size, kernel_size=3, padding=1, stride=2, bias=False),
                    nn.BatchNorm2d(output_size),
                    nn.ReLU(inplace=True),
                )
            else:
                layer = nn.Sequential(
                    nn.Conv2d(input_size, channels, kernel_size=1, bias=False),
                    nn.BatchNorm2d(channels),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(channels, output_size, kernel_size=3, bias=False),
                    nn.BatchNorm2d(output_size),
                    nn.ReLU(inplace=True),
                )

            self.additional_blocks.append(layer)

        self.additional_blocks = nn.ModuleList(self.additional_blocks)


    def forward(self, x):
        x = self.feature_extractor(x)
        detection_feed = [x]
        for l in self.additional_blocks:
            x = l(x)
            detection_feed.append(x)
        locs, confs = self.bbox_view(detection_feed, self.loc, self.conf)
        return locs, confs


feature_maps = {}


class MobileNetV2(nn.Module):
    def __init__(self):
        super().__init__()
        self.feature_extractor = mobilenet_v2(pretrained=True).features
        self.feature_extractor[14].conv[0][2].register_forward_hook(self.get_activation())

    def get_activation(self):
        def hook(self, input, output):
            feature_maps[0] = output.detach()

        return hook

    def forward(self, x):
        x = self.feature_extractor(x)
        return feature_maps[0], x


def SeperableConv2d(in_channels, out_channels, kernel_size=3):
    padding = (kernel_size - 1) // 2
    return nn.Sequential(
        nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size,
                  groups=in_channels, padding=padding),
        nn.BatchNorm2d(in_channels),
        nn.ReLU6(),
        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1),
    )


def StackedSeperableConv2d(ls_channels, multiplier):
    out_channels = 6 * multiplier
    layers = [SeperableConv2d(in_channels=in_channels, out_channels=out_channels) for in_channels in ls_channels]
    layers.append(nn.Conv2d(in_channels=ls_channels[-1], out_channels=out_channels, kernel_size=1))
    return nn.ModuleList(layers)
```
model architecture로 SSD는 



it is intended only for human readers.[^1]

[^1]: <http://en.wikipedia.org/wiki/Syntax_highlighting>


To modify styling and highlight colors edit `/_sass/_highlighter.scss`.







### Standard Code Block

    {% raw %}<nav class="pagination" role="navigation">
        {% if page.previous %}
            <a href="{{ site.url }}{{ page.previous.url }}" class="btn" title="{{ page.previous.title }}">Previous article</a>
        {% endif %}
        {% if page.next %}
            <a href="{{ site.url }}{{ page.next.url }}" class="btn" title="{{ page.next.title }}">Next article</a>
        {% endif %}
    </nav><!-- /.pagination -->{% endraw %}

### GitHub Gist Embed

An example of a Gist embed below.

<script src="https://gist.github.com/mmistakes/43a355923921d22cd993.js"></script>
