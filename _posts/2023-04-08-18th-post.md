---
layout: post
title: "YOLO Hands-on Practice "
description: "YOLO Pytorch"
date: 2023-04-08
tags: study, code
comments: true
---
# YOLO (CVPR 2016)

![image](https://user-images.githubusercontent.com/122149118/230702239-6caaf9d9-b5a4-43b6-9fda-a0bcec9aaa18.png)

 YOLO는 먼저 미리 정해진 S x S grid로 나눈 후에 bounding box와 confidence를 추론하고 그 다음에 class probability map을 추론하였다.
 bounding box는 격자 무늬 하나 당 두 개의 box를 추론하고 그에 해당하는 confidence score, confidence score란 box에 object가 있는지 없는지를 나타내는 score이다.
 해당 grid가 어떤 class를 나타내는지 class probability map을 위와 같이 표현한다.
 bounding box는 bounding box대로 merge시키다보면 결과물과 같이 3가지 box가 남게되고 해당 box의 class는 class probability map을 사용하여 box에 포함되는 grid들의 class probability를 고려하여 최종 해당 box의 class를 결정한다.

![image](https://user-images.githubusercontent.com/122149118/230705807-36ca663e-f5e3-4514-9390-0caad89063aa.png)

 network의 prediction은 실제로 7이 S의 값을 나타내고 input video도 정해진 resolution이 존재하였고 이것을 7 by 7로 예상한다음 class probability가 20개, box 하나 당 5개의 channel, 하나는 confidence score이고 나머지 4개는 box의 좌표인 x<sub>1</sub>, y<sub>1</sub>, x<sub>2</sub>, y<sub>2</sub>를 나타낸다.
 box를 2개씩 사용하므로 총 10개의 channel이 필요하고 dataset에 맞는 20개의 class들을 합쳐 총 channel 30개를 최종 output으로 prediction한다.


# YOLO Pytorch

###### 사용환경 : Google Colab
---

## evaluate.ipynb

```py


if __name__ == '__main__':
    # Path to the yolo weight to test.
    model_path = 'weights/yolo/model_best.pth'
    # GPU device on which yolo is loaded.
    gpu_id = 0
    # Path to label file.
    label_path = 'data/voc2007test.txt'
    # Path to image dir.
    image_dir = 'data/VOC_allimgs/'


    voc_class_names = list(VOC_CLASS_BGR.keys())
    targets = defaultdict(list)
    preds = defaultdict(list)


    print('Preparing ground-truth data...')

    # Load annotations from label file.
    annotations = []
    with open(label_path, 'r') as f:
        lines = f.readlines()
    for line in lines:
        anno = line.strip().split()
        annotations.append(anno)

    # Prepare ground-truth data.
    image_fnames = []
    for anno in annotations:
        filename = anno[0]
        image_fnames.append(filename)

        num_boxes = (len(anno) - 1) // 5
        for b in range(num_boxes):
            x1 = int(anno[5*b + 1])
            y1 = int(anno[5*b + 2])
            x2 = int(anno[5*b + 3])
            y2 = int(anno[5*b + 4])

            class_label = int(anno[5*b + 5])
            class_name = voc_class_names[class_label]

            targets[(filename, class_name)].append([x1, y1, x2, y2])


    print('Predicting...')

    # Load YOLO model.
    yolo = YOLODetector(model_path, gpu_id=gpu_id, conf_thresh=-1.0, prob_thresh=-1.0, nms_thresh=0.45)

    # Detect objects with the model.
    for filename in tqdm(image_fnames):
        image_path = os.path.join(image_dir, filename)
        image = cv2.imread(image_path)

        boxes, class_names, probs = yolo.detect(image)
        for box, class_name, prob in zip(boxes, class_names, probs):
            x1y1, x2y2 = box
            x1, y1 = int(x1y1[0]), int(x1y1[1])
            x2, y2 = int(x2y2[0]), int(x2y2[1])
            preds[class_name].append([filename, prob, x1, y1, x2, y2])


    print('Evaluate the detection result...')

    evaluate(preds, targets, class_names=voc_class_names)
}
```
 model_path와 여러가지 data_path들을 설정
 `VOC_CLASS_RGV`는 POSCAL VOC Dataset에 맞는 class name들을 정의
 target과 prediction을 dictionary기반으로 된 list를 기본으로 하는 dictionary를 선언
 annotation을 load
 annotation에 맞는 groundtruth data도 불러옴
 bounding box 위치와 `class_label`과 class label에 해당하는 `class_name`을 받아줌
 `YOLODetector`함수를 이용하여 yolo model을 load
 image를 불러와 `yolo.detect(image)`에서 boxes, class_names, probs를 얻음

## detect.py

```py
import torch
from torch.autograd import Variable
import torchvision.transforms as transforms

import os
import cv2
import numpy as np

from darknet import DarkNet
from yolo_v1 import YOLOv1
```
 yolo에서 baseline으로 사용하고 있는 backbone architecture로 darknet 사용

```py
class YOLODetector:
    def __init__(self,
        model_path, class_name_list=None, mean_rgb=[122.67891434, 116.66876762, 104.00698793],
        conf_thresh=0.1, prob_thresh=0.1, nms_thresh=0.5,
        gpu_id=0):

        os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
        use_gpu = torch.cuda.is_available()
        assert use_gpu, 'Current implementation does not support CPU mode. Enable CUDA.'

        # Load YOLO model.
        print("Loading YOLO model...")
        darknet = DarkNet(conv_only=True, bn=True, init_weight=True)
        darknet.features = torch.nn.DataParallel(darknet.features)
        self.yolo = YOLOv1(darknet.features)
        self.yolo.conv_layers = torch.nn.DataParallel(self.yolo.conv_layers)
        self.yolo.load_state_dict(torch.load(model_path))
        self.yolo.cuda()
        print("Done loading!")

        self.yolo.eval()

        self.S = self.yolo.feature_size
        self.B = self.yolo.num_bboxes
        self.C = self.yolo.num_classes

        self.class_name_list = class_name_list if (class_name_list is not None) else list(VOC_CLASS_BGR.keys())
        assert len(self.class_name_list) == self.C

        self.mean = np.array(mean_rgb, dtype=np.float32)
        assert self.mean.shape == (3,)

        self.conf_thresh = conf_thresh
        self.prob_thresh = prob_thresh
        self.nms_thresh = nms_thresh

        self.to_tensor = transforms.ToTensor()

        # Warm up.
        dummy_input = Variable(torch.zeros((1, 3, 448, 448)))
        dummy_input = dummy_input.cuda()
        for i in range(10):
            self.yolo(dummy_input)
```
 GPU 설정
 backbone architecture로 쓰는 darknet을 정의
 `YOLOv1(darknet.features)` YOLOv1에 darknet의 feature들을 집어넣어줌
 `self.yolo.conv_layers` YOLO의 convolution을 정의 

## yolo_v1.py
```py
class YOLOv1(nn.Module):
    def __init__(self, features, num_bboxes=2, num_classes=20, bn=True):
        super(YOLOv1, self).__init__()

        self.feature_size = 7
        self.num_bboxes = num_bboxes
        self.num_classes = num_classes

        self.features = features
        self.conv_layers = self._make_conv_layers(bn)
        self.fc_layers = self._make_fc_layers()

    def _make_conv_layers(self, bn):
        if bn:
            net = nn.Sequential(
                nn.Conv2d(1024, 1024, 3, padding=1),
                nn.LeakyReLU(0.1, inplace=True),
                nn.Conv2d(1024, 1024, 3, stride=2, padding=1),
                nn.LeakyReLU(0.1),

                nn.Conv2d(1024, 1024, 3, padding=1),
                nn.LeakyReLU(0.1, inplace=True),
                nn.Conv2d(1024, 1024, 3, padding=1),
                nn.LeakyReLU(0.1, inplace=True)
            )

        else:
            net = nn.Sequential(
                nn.Conv2d(1024, 1024, 3, padding=1),
                nn.BatchNorm2d(1024),
                nn.LeakyReLU(0.1, inplace=True),
                nn.Conv2d(1024, 1024, 3, stride=2, padding=1),
                nn.BatchNorm2d(1024),
                nn.LeakyReLU(0.1),

                nn.Conv2d(1024, 1024, 3, padding=1),
                nn.BatchNorm2d(1024),
                nn.LeakyReLU(0.1, inplace=True),
                nn.Conv2d(1024, 1024, 3, padding=1),
                nn.BatchNorm2d(1024),
                nn.LeakyReLU(0.1, inplace=True)
            )

        return net
```
`feature_size = 7` 최종 output tensor가 7 by 7 by 30이었으므로 7로 지정
`num_boxes`는 한 grid에서 추론하는 box의 개수로 default 2로 설정
`num_classes` 20개로 정의되어있지만 dataset에 따라 달라지는 값
darknet feature를 받아 `self.features = features`로 넣어줌

### Standard Code Block

    {% raw %}<nav class="pagination" role="navigation">
        {% if page.previous %}
            <a href="{{ site.url }}{{ page.previous.url }}" class="btn" title="{{ page.previous.title }}">Previous article</a>
        {% endif %}
        {% if page.next %}
            <a href="{{ site.url }}{{ page.next.url }}" class="btn" title="{{ page.next.title }}">Next article</a>
        {% endif %}
    </nav><!-- /.pagination -->{% endraw %}


it is intended only for human readers.[^1]

[^1]: <http://en.wikipedia.org/wiki/Syntax_highlighting>

### Highlighted Code Blocks

To modify styling and highlight colors edit `/_sass/_highlighter.scss`.


### GitHub Gist Embed

An example of a Gist embed below.

<script src="https://gist.github.com/mmistakes/43a355923921d22cd993.js"></script>
