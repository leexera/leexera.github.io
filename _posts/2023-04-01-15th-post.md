---
layout: post
title: "SSD : Single Shot MultiBox Detector"
description: "1 Method and Results"
date: 2023-04-01
tags: study
comments: true
--- 

# Faster R-CNN (NIPS 2015)

 SSD는 Faster R-CNN의 imporve version이라고 생각할 수 있다.

 > * A single, unified network
 > * Region proposal network with achor boxes
 > * End-to-end training

 Faster R-CNN의 장점은 a sigle shot unified network이고 region proposal network를 anchor box를 사용하여 제한했다.
 전체 network가 end-to-end로 training이 가능했다.

 > * Limitations
 >   - Limited flexibility of anchor boxes
 >   - Three subnetworks: feature extraction, region proposal, classification/bbox regression

![image](https://user-images.githubusercontent.com/122149118/229261942-e66ab357-b93d-48db-bcc4-81063c5c02fb.png)
![image](https://user-images.githubusercontent.com/122149118/229261925-c0006840-8411-4d10-b281-0d679bb1a72f.png)

 limitation으로는 anchor box의 flexibility가 제한적이라는 것이다.
 2번째로는, 3개의 sub network로 구성이 되어있는데, 이 network들이 충분히 유기적으로 작동하는가에 대해 의문이 남는다.

 # SSD (ECCV 2016)

 > * Feature pyramid with default anchors
 >   - the same size of anchors represents different sizes of objects in feature pyramid

 SSD는 feature pyramid를 사용하면 default anchor들의 크기가 달라진다는 점에 초점을 맞추었다.
![image](https://user-images.githubusercontent.com/122149118/229262096-8a6e5949-a4e0-4078-9983-759ca65b35b2.png)
 빨간색 box로 싸여진 강아지와 파란색 box로 싸여진 고양이가 있다. 두 box의 size는 차이가 존재한다.
 이 feature map의 default anchor들은 faster R-CNN처럼 각 point마다 미리 정해진 개수의, 미리 정해진 크기로 default anchor들이 정해져있다.
 8 by 8 featuremap에서는 파란색 box의 고양이가 detect가 되고 똑같은 anchor box를 사용하여 4 by 4 feature map에서는 강아지를 detect할 수 있다.
 anchor box의 size는 feature map에 상관없이 유지되지만 feature map이 2배 만큼 줄어드니 그 anchor box가 표시하는 size가 2배가 된 것이다.
 이 concept을 가지고 SSD는 object detection network를 구성하였다.

 > * Feature pyramid with default anchors
 >   - the same size of anchors represents different sizes of objects in feature pyramid
 >   - CNN has used multi-scale feature maps

![image](https://user-images.githubusercontent.com/122149118/229262389-0e205cdf-c920-4220-b126-af1206e8e854.png)
 image classification network에서부터 multi-scale feature map을 사용해왔다.
 위와 같은 VGG model에서 video가 input으로 들어갔을 때 224 by 224 by 3 여기서 3은 color channel(R, G, B)이고 224 by 224는 video의 resolution이다.
 input이 들어갔을 때 feature map이 layer에 걸러지면서 size가 반으로 줄게되어 최종적으로 resolution이 1 by 1으로 되는 과정을 학습을 진행했다.
 여기서 착안하여 SSD는 다음과 같은 model을 제안했다.

  > * Multi-scale feature maps for detection

  ![image](https://user-images.githubusercontent.com/122149118/229262570-11d95825-bee9-4e9b-bf11-7d34575551a1.png)

  처음 input으로 300 by 300 video를 받고 300 by 300 video를 VGG-16을 통과시켜 38 by 38 resolution을 갖는 feature map을 생성한다.
  Conv6에서는 38 by 38 resolution이 19 by 19로 줄어들고 Conv8에서는 10 by 10으로 줄어들고 Conv9에서는 5 by 5로 줄고 Conv10에서는 3 by 3으로 줄어들게되고 마지막으로 1 by 1으로 줄게되는 architecture이다.
  중간 feature map들을 convolution으로 연결하여 prediction을 바로하게되는데, 이 convolution의 output값이 defualt box, bounding box의 location과 class들의 score값이다.
  classifier convolution은 3 by 3 kernel로 구현되어 있으며 class의 개수만큼 default anchor box의 number를 추가하고 그 box의 offset을 계산한 값을 사용한다.
  즉, 한 anchor box 당 class에 대한 확률값들과 anchor box와 실제 prediction하고자 하는 box의 kernel값들의 offset을 추가하여 구현한 것이다.
  feature resolution이 큰 쪽에서는 작은 object들을 detection할 수 있는 위치에서는 offset box를 4개만 사용하게되고 그 이후부터는 6개씩 사용하였다.
  최종적인 anchor box의 수가 feature map 38 by 38인 곳에서는 38 by 38 by 4개의 anchor box가 생성되고 그 다음 resolution에서는 19 by 19 by 6개 anchor box들이 생성되며 그 다음에서는 10 by 10 by 6개의 anchor box가 생성되게 된다.
  총 prediction되는 anchor box 수를 합하면 8732개가 된다.
  기존의 Faster R-CNN에서 proposal을 300개정도 사용한 것에 비하면 많은 양의 anchor box를 사용했다는 것을 알 수 있다.
  이 전체를 Non-maximum Suppression을 통해 최종 bounding box를 찾으면 상당히 높은 MAP를 얻을 수 있다.

  > * Training
  >   - Loss functions

 training loss function은 기존의 것과 비슷한 방법을 사용했다.
 ![image](https://user-images.githubusercontent.com/122149118/229263376-e9312efe-96ed-4d8b-aeb0-93ad7547bfb9.png)

 먼저, classification과 location에 대한 loss의 합으로 전체 loss function의 design을 하였다.

 ![image](https://user-images.githubusercontent.com/122149118/229263382-ffe28899-3e57-4b66-9a61-1a82ed4ccc36.png)

 location에 대한 loss는 center와 anchor box의 좌표 x와 y에 대한 값과 넓이와 높이에 관한 값들을 regression해주었다.
 regression의 smooth<sub>L1</sub>이라는 것을 사용했는데, -1에서 1 사이의 값에서는 L<sub>2</sub> norm을 사용하고 그 이후 부분에서는 L<sub>1</sub> norm을 사용하여 L<sub>1</sub> norm이 0부분에서 꺾이는 점을 L<sub>2</sub>로 smooth하게 표현하여 smooth<sub>L1</sub>이다.

 ![image](https://user-images.githubusercontent.com/122149118/229263389-e348256f-0e4d-469e-8ba7-af9444d73f5b.png)
 
 classification loss도 일반적인 classification loss를 사용했다고 볼 수 있는데 object가 있는 positive한 example에서는 softmax function을 사용했다고 볼 수 있으며 negative한 example에 대해서는 log c(hat)<sub>i</sub><sup>0</sup>값을 사용했다.
 classification loss도 softmax를 사용했다고 볼 수 있는데 positive example과 negative example에 대해 각각 표현해주었다.
 negative example에 대해 따로 표기해준 것만 차이가 있을 뿐 전반적으로 classification loss를 적용했다고 보아도 무방하다.

 >   - Scale of default boxes
 >      * 0.2 ~ 0.9

 ![image](https://user-images.githubusercontent.com/122149118/229264482-7632d570-4553-4c1f-aea6-d848605723f2.png)

 >   - Aspect ratios for default boxes
 >      * 1, 2, 3, ½, ⅓

 >   - Hard negative minig
 >      * Using negative boxes with high confidence scores as negative training examples
 
 SSD는 simple한 network architecture에 비해 굉장히 data agmentation을 많이 하였다.
 default box에 대한 scaling도 많이 하였다.
 m은 number of multi-feature map이다.
 ![image](https://user-images.githubusercontent.com/122149118/229262570-11d95825-bee9-4e9b-bf11-7d34575551a1.png)
 사용하는 multi-scale feature map이 총 6개가 존재하기 때문에 m = 6이다.
 m = 6을 대입하고 s<sub>min</sub> = 0.2, s<sub>max</sub> = 0.9를 대입하여 여러가지 다양한 s<sub>k</sub>를 만들었다.
 default box의 aspect ratio는 1, 2, 3, ½, ⅓로 가로, 세로가 된다.
 ![image](https://user-images.githubusercontent.com/122149118/229264758-a918a0c8-54fa-4b84-ac5d-0611e00f9ecf.png)

 ground truth의 label을 agmentation은 사람이 칠하는 box가 물체 당 하나 있다.
 이 box 하나는 network를 학습할 때 proposal되는 box의 수에 비해 현저히 적다.
 사람도 해당 물체에 ground truth box를 칠할 때 항상 똑같은 곳만 칠하지 못하고 조금씩 pixel 오차가 발생한다.
 이러한 점을 보완하기 위해 사람이 쳐놓은 box와 iou가 충분히 많이 overlap되는 box를 ground truth로 보는 방법이다.
 hard negative mining은 이와 비슷한 방법을 negative sample에 적용하는 것이라고 볼 수 있다.
 ground truth와 겹치는 부분이 적은 example들은 모두 negative한 example로 사용을 해왔는데, negative sample이 너무 많다는 문제가 있다.
 confidence score가 굉장히 높은 negative sample들에 대해서만 negative example로 선정하는 방법을 선택했다.
 positive sample과 negative sample의 비가 1:3 정도로 유지되는 학습을 진행했다.
 detection model은 positive sample에 비해 negative sample이 현저히 많아 training unbalance가 발생하고 이를 해결하기 위해 negative sample 중에서는 굉장히 negative한, confidence score가 높은 sample들만 training example로 사용하면서 전체 학습의 balance를 맞춰주었다.

 >   - Data augmentation
 >      * Use the entire original input image
 >      * Sample a patch so that the minimum jaccard overlap with the objects is 0.1, 0.3, 0.5, 0.7, or 0.9
 >      * Randomly sample a patch

 실제 data augmentation도 많이 하였다.
 먼저, 전체 original input video를 사용하고 유사 bounding box와 같은 patch를 뜯어내어 특정 영역을 잘라내어 example로 사용하는데 그 때 원하는 object와 겹치는 정도를 0.1, 0.3, 0.5, 0.7, 0.9로 변경해가면서 뜯어내었다.
 jaccard overlap은 iou개념과 흡사하다.
 0.1을 예시로 들어보면 patch의 object가 살짝 걸쳐있는것과 같다.
 0.9는 background가 거의 없고 object만 있는 것과 같다.
 이러한 sample들을 randomly하게 사용하고 sample로 뽑은 patch들도 augmentation을 적용했다.
 예를 들면 input size를 0.1에서 1까지 축소하는 scaling이나 aspect ratio를 원본 size의 영상을 가로로도 줄이고 세로로도 줄여 0.2에서 2까지 변화를 주며 바꾸고 patch들을 horizental, virtical flip을 하여 섞고 영상의 색깔을 바꾼다거나 여러가지 computer vision에서 제공하는 영상을 섞는 방법 등을 적용하여 학습 example의 수를 늘렸다.

 > * Experiments

 ![image](https://user-images.githubusercontent.com/122149118/229265525-92699ac3-ed1e-4692-91fb-73b8ff24de9f.png)

 data는 VOC2007, VOC2007과 VOC2012를 합한 것, VOC2007, VOC2012, COCO dataset를 합한 것을 사용했다.
 VOC2007 data를 사용하고 Fast R-CNN이 66.9mAP, Faster R-CNN일 때 3%정도 올라 69.9mAP가 되었고 SSD300은 68.0mAP가 나왔고 SSD512는 71.6mAP가 나왔다.
 SSD300과 SSD512는 input video의 size에 차이가 있는 것이다.
 SSD512는 input video의 size가 512 by 512인 것이다.
 SSD300같은 경우 PASCAL VOC2007 data만 가지고 보았을 때 Faster R-CNN보다 살짝 성능이 떨어지지만 다른 data들을 추가하였을 때는 SSD300이 Faster R-CNN보다 성능이 더 높다는 것을 알 수 있다.

 > * Experiments

 ![image](https://user-images.githubusercontent.com/122149118/229266964-769d0a90-2962-46d8-9ba8-750fa423ff55.png)

 SSD model에서 prediction을 feature의 중간중간에서 뽑아온 것이다.
 feature가 mulitiple scale로 점점 줄어드는 모양으로 Conv4_3이 가장 큰 38 by 38 resolution을 갖는 feature map에서 뽑은 것이고 최종적으로 Conv11에서 뽑은 것은 resolution이 1 by 1이다.
 final model은 모든 feature 6개의 feature map에서 모두 prediction한 것이 성능이 가장 좋을 것이다.
 Conv7에서만 feature를 뽑아보거나, Conv4와 Conv7에서만 뽑아보거나 3개, 4개, 5개에서 뽑아보는 ablation을 진행했다.
 boundary box를 ground truth로 사용하여 학습했는지에 대해 나타나 있다.
 SSD algorithm이 원본 영상을 crop하여 사용하다 보니 잘리는 물체들이 많고 ground truth와 겹치는 부분이 0.1인 것도 존재하는데 boundary box들을 포함하여 학습할 것인지 아니면 object가 아니라고 판단할 것인지에 대한 것이다.
 boundary box를 포함하여 학습시켰을 때가 보통 더 성능이 좋게 나오는데 Conv를 높은 feature map에서 convolution을 계산했을 때, prediction했을 때 즉, 작은 영상들, object에 주로 초점이 맞추어져 있는 model을 학습했을 때는 boundary box를 사용하여 학습시키는 것이 성능이 더 떨어지는 것을 확인할 수 있다.
 그 이유는 작은 영상들에 초점이 맞춰져 있어 boundary가 잘려있는 영상들은 큰 영상들이 많기 때문에 큰 영상들을 학습하기 위해 강요하다보면 오히려 성능이 떨어진다. 

 ![image](https://user-images.githubusercontent.com/122149118/229267627-fea6f22b-1c19-440a-98f6-edff89fad2c7.png)

 SSD300과 SSD512의 성능을 다각도로 분석한 것이다.
 좌측 2개의 그래프는 object의 size에따라 prediction결과를 비교한 것이다. 
 object size를 XS, S, M, L, XL로 구분하였고 object 위에는 class들이 나타나있다.
 SSD300에 비해 SSD512가 Small object에 대해 더 좋은 성능을 보이는 것을 알 수 있다. 
 resolution size가 300에서 512로 커졌으므로 small object를 더 잘 detect할 수 있다는 것을 알 수 있다. 
 우측은 aspect ratio의 관한 그래프로 XT, T, M, W, XW로 구분하였다.
 SSD512가 전반적으로 성능이 높아진 것을 볼 수 있지만 뚜렷하게 차이가 나는 결과는 보이지 않는다.
 SSD300과 SSD512를 비교했을 때 small object에 대해서 SSD512가 detect를 더 잘한다는 것을 알 수 있다. 

 ![image](https://user-images.githubusercontent.com/122149118/229267885-1732dd38-9e67-4a16-b6d4-ac8d63fd33b8.png)
 결과적으로 작은 object들도 잘 구분하는 것을 알 수 있다.
 SSD는 Fster R-CNN과 달리 simple한 방법으로 상당히 좋은 성능을 낸다.