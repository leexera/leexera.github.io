---
layout: post
title: "Advanced Classification"
description: "이화여자대학교 강제원 교수님(Electronic & Electrical Engineering)"
date: 2023-02-28
tags: study
comments: true
---
# 지도 학습
## Linear classification

 ![image](https://user-images.githubusercontent.com/122149118/221790149-39b56aef-0b7b-47f8-aea2-c10080e60caa.png)

 Linear classification 문제에서 배운 바와 같이 sample의 분류는 hyper plane을 기준으로 score 값을 계산하여 classification을 수행한다.
 이 때 hyper plane을 구성하는 model parameter가 w이면 hyper plane의 normal한 방향으로 hyper parameter w vector를 구성하게 된다.
 score 값을 계산하게 되면 w<sup>T</sup>x가 0보다 큰 경우가 positive sample이 된다.
 0보다 작은 경우는 negative sample이 된다.
 이 hyper plane에 위치하는 h(x)는 0이 되게 된다.
 이러한 hyper plane을 sample 사이 중간에 긋는데 많은 선택의 문제가 생기게 된다.

## Multiple solutions

 ![image](https://user-images.githubusercontent.com/122149118/223774259-def8e7ab-187b-43c0-90c5-cc3c9e12e157.png)
 Same on empirical loss; Different on test/expected loss

 현재 그림에서 positive sample과 negative sample을 구분하는데 다양한 종류의 hyper plane이 있음을 보여주고 있다.
 예를 들어 w<sub>1</sub>과 같은 hyper plane을 그을 수도 있을 것이고 w<sub>2</sub>와 같은 hyper plane을 그을 수도 있을 것이고 마지막으로 w<sub>3</sub>와 같은 hyper plane을 그을 수도 있을 것이다.
 이와 같이 서로 다른 hyper plane을 그었을 때, 최소한 여기 있는 data sample들에 대해서는 아마도 같은 loss, 비슷한 loss를 제공하게 될 것이다.
 여기에 있는 sample들은 모두 훌륭하게 분류했기 때문이다.
 하지만 이 model을 실생활에다 적용을 하게 되는 경우에는 data sample에는 보이지 않는 굉장히 다양한 종류의 sample들이 나타날 수가 있게 될 것이고 이 때 설정한 다른 종류의 hyper plane들은 서로 다른 성능을 제공할 수 있다.

## What about <i>w<sub>1</sub></i>?

 ![image](https://user-images.githubusercontent.com/122149118/223775887-dd5d366d-ea8d-415c-aa61-e72d6430dd67.png)

 먼저 w<sub>1</sub>, hyper parameter를 이용한 hyper plane을 다음과 같이 그어 model을 만들었다고 생각해본다.
 이 때 여기에 보이는 sample들은 잘 분류할 수 있지만 아마 이 hyper plane 근처에서 즉, positive sample들 근처에서 또다른 positive sample이 위와 같은 형태로 나타날 가능성이 대단히 크다고 볼 수 있다.
 이러한 가정은 굉장히 합리적인 가정이다.
 그 이유는 위와 같이 위치한 새로운 positive sample은 주위에 있는 positive sample과 굉장히 서로 간에 거리가 가깝기 때문이다.
 반대로 negative sample들 사이의 거리는 멀다라고 할 수 있다.
 따라서 이와 같은 새로운 test sample이 들어오게 되었을 때 설정한 w<sub>1</sub>의 hyper plane은 오류를 낳을 확률이 높다고 할 수 있다.

# What about <i>w<sub>3</sub></i>?

 ![image](https://user-images.githubusercontent.com/122149118/223777490-2c57e01e-dd6e-405b-aa7c-9516fb6b1bef.png)

 또한 다른 예시와 유사하게 만약 w<sub>3</sub>에 해당하는 hyper plane을 다음과 같이 설정했다고 할 때 비슷한 이유로 negative sample에 해당하는 sample이 새로운 test sample에서 negative sample들과 서로 간에 가까운 거리에서 발생할 확률이 높다고 볼 수 있다.
 이는 마찬가지로 positive sample과는 거리가 멀기 때문에 negative sample쪽에 훨씬 더 나타나게 될 확률이 높은 것이다. 
 이와 같이 w<sub>3</sub>를 반대로 negative sample쪽에 hyper plane을 설정하게 된다면 이 역시 negative sample쪽에서 오류를 낳을 확률이 상당히 높다고 할 수 있을 것이다.

## Most confident: <i>w<sub>2</sub></i>

 ![image](https://user-images.githubusercontent.com/122149118/223778725-7d6f1b74-65e1-4faa-8e37-8988cafc2fdd.png)

 따라서 선택할 수 있는 가장 좋은 위치라고 할 수 있는 것은 바로 positive sample과 negative sample 중간 어딘가에 hyper plane을 긋는 것이 가장 최적이라고 할 수 있다.
 positive sample과 negative sample 사이에 어떠한 방식으로 hyper plane을 그어야할 것인가가 바로 model을 수립하기 위한 숙제라고 할 수 있다.
 SVM(Support Vector Machine)에서는 이러한 숙제를 어떻게 해결하는지 알아본다.
 SVM에서는 margin이라고 하는 것을 정의하게 된다.
 Margin이 hyper plane을 결정하는 SVM의 핵심적인 아이디어가 된다.

# Intuition: margin

 ![image](https://user-images.githubusercontent.com/122149118/223779854-20202f4b-2046-4478-b0d3-55bb869c57a8.png)

 그림에서와 같이 가장 hyper plane에 가까운 positive smaple이 있을 수 있고 반대로 negarive sample 역시 hyper plane의 가장 가까운 sample로 존재할 수 있다.
 그러면 hyper plane은 현재 가장 가까운 positive sample을 지나가는 이 점선, 그리고 negative sample을 지나가는 이 점선으로 구성된 hyper plane의 둘 사이에 가장 서로 간의 위치가 동일한 hyper plane이 바로 이 둘 positive sample과 negative sample 사이에 최대 margin을 확보할 수 있는 최적화 방식이 되게 될 것이다.
 이렇게 구성이 된 w<sub>2</sub>의 hyper parameter는 이 두 개의 점선 사이에 중간에서 설정이 되어 있는 것을 알 수 있다.

## Support Vector Machine

 * Choose the linear separator (hyperplane) with the largest **margin** on either side
    - Maximum margin hyperplane with **support vectors**
    - Robust to outliers

 **Support vector** : an instance with the minimum margin, which will be the most sensible data points to affect the performance

 ![image](https://user-images.githubusercontent.com/122149118/223781420-c500198e-cae6-4d41-972b-cf2d515da15f.png)
 <i>h(x) = w<sup>T</sup>x + b</i>

 Support Vector Machine에서는 이러한 개념을 설명하기 위해서 support vector를 정의한다.
 support vector라고 하는 것은 positive sample과 hyper plane 사이에 있는 거리 중에서 가장 가까운 sample을 의미하게 되고, 반대로 negative sample 중에서도 가장 가까운 margin을 갖는 vector라고 했을 때 이러한 support vector들은 결국 성능을 가장 좌지우지할 수 있는 가장 민감한 data point라고 할 수 있다.
 이러한 support vector들끼리의 거리를 가장 최대화 하도록 하는 maximum margin을 설정하는 것이 최적화 전략이 된다.
 이 방식은 unseen test data가 발생했을 때 어떠한 outlier들에 대해서도 보다더 안정적인 성능을 제공하도록 하는 최적화 방식이 된다.

## Margin

 * Twice the distance from the hyperplane to the nearest instance on either side

 * <i>w</i> is orthogonal to the hyperplane

 * Lemma : <i>x</i> has distance <sup>|<i>h<sub>wb</sub>(x)</i>|</sup>/<sub>||<i>w</i>||</sub> to the hyper plane <i>h<sub>wb</sub>(x) = w<sup>T</sup>x + b = 0</i>

 ![image](https://user-images.githubusercontent.com/122149118/223784264-087ea83e-85cf-4a85-9487-a2cf34b8e1fd.png)

 🔍 Margin은 어떻게 계산할 수 있는가?

 ➡ 수식에서 보이는 바와 같이 가운데에 있는 hyper plane은 h(x) = 0인 hyper plane이다.
 hyper plane에서 가장 가까운 support vector간 거리는 한 vector의 길이의 두 배라고 할 수 있다.
 model parameter w는 이 hyper plane의 nomal한 방향으로 설정이 되어 있다고 생각할 수 있다.
 이러한 support vector x와 hyper plane h와의 사이 거리를 계산해보면 <sup>|<i>h<sub>wb</sub>(x)</i>|</sup>/<sub>||<i>w</i>||</sub>와 같이 구할 수 있다.

## Margin distance

 Proof:
 * Let <i>x = x<sub>⊥</sub> + r<sup>w</sup>/<sub>||w||</sub></i>, then |<i>r</i>| is the distance
 * Multiply both sides by <i>w<sup>T</sup></i> and add <i>b</i>
 * Left hand side: <i>w<sup>T</sup>x + b = h<sub>w,b</sub>(x)</i>
 * Right hand side: <i>w<sup>T</sup>x<sub>⊥</sub> + r<sup>w<sup>T</sup>w</sup>/<sub>||w||</sub> + b = 0 + r||w||</i>

 ![image](https://user-images.githubusercontent.com/122149118/223786301-5f6ed27c-b8c5-4d12-a1db-dcc0f32bc4b7.png)

 ## Optimization

  * Optimal weight <i>w</i> and bias <i>b</i>
     - Classifies points correctly as well as achieves the largest possible margin
     - **Hard margin SVM** - assumes linear seperability
     - Soft margin SVM - extends to non-separable cases
     - Nonlinear transform & kernel trick

 ![image](https://user-images.githubusercontent.com/122149118/223787348-6ba03e2b-d9af-4cc8-b1ad-8fe6beb17511.png)

 이렇게 설정한 margin 즉, hyper plane으로부터 떨어져 있는 support vector 간 거리를 최소화하기 위해서는 SVM에서는 다양한 종류의 oprimization 방식을 이용한다. 
 그 중의 하나로 hard margin SVM이 있다.
 hard margin SVM 같은 경우 이 sample들이 linear separable하다는 것을 가정하고 있다.
 즉, 위의 그림과 같이 hyper plane이 있고 각 support vector들을 연결하는 plane들이 있다라고 했을 때 그 둘 사이의 영역에서는 어떠한 sample도 있지 않는 것을 의미하게 된다.
 반대로 soft margin SVM과 같은 경우 어느 정도의 error를 용인하는 범위에서의 optimization 방법을 설명한다.
 SVM에서는 nonlinear transform, kernel trick을 이용한 optimization 방식 역시 사용하고 있다.
 이러한 linear transform과 같은 경우 SVM이 linear separable한 hyper plane을 사용하기 때문에 linear한 경우에만 사용할 수 있다는 단점을 극복하기 위하여 만들어졌다.
 ![image](https://user-images.githubusercontent.com/122149118/223787197-2cf15c51-51e6-451e-91fa-f2b3a524e8cb.png)
 즉, SVM을 사용하게 될 때 kernel을 이용하여 예컨데 현재 data sample들이 위와 같이 구성되어 있다고 할 때 이 sample들은 결코 하나의 일직선으로 선형 model로서 classification을 수행하기가 어렵다.
 하지만 nonlinear transform, 혹은 kernel trick을 이용하게 되면 2차원의 sample들을 보다 더 고차원의 sample들로 mapping을 하는 함수를 이용하게 된다.
 위의 그림과 같이 3차원 공간으로 sample들을 mapping할 수 있다고 하면 2차원의 hyper plane을 이용하여 positive sample과 negative sample을 수직으로 분할하여 구분할 수 있음을 알 수 있다.

## Optimization
### constraints: linearly separable; hard-margin linear SVM

 <i>h(x) = w<sup>T</sup>x + b ≥ 1 for y = 1
 h(x) = w<sup>T</sup>x + b ≤ -1 for y = -1</i>
 ⬇{: .center}
 
 support vector 값들에 의해서 hypothesis h(x)가 어떻게 결정이 되는지 알아본다.

##### Center
{: .center}

##### Right
{: .right}

## 2. Body Text

Lorem ipsum dolor sit amet, [test link](https://www.google.com) adipiscing elit. **This is strong.** Nullam dignissim convallis est. Quisque aliquam. *This is emphasized.* Donec faucibus. Nunc iaculis suscipit dui. 5<sup>3</sup> = 125. Water is H<sub>2</sub>O. Nam sit amet sem. Aliquam libero nisi, imperdiet at, tincidunt nec, gravida vehicula, nisl. <u>Underline</u>. Maecenas ornare tortor. Donec sed tellus eget `COPY filename` sapien fringilla nonummy. Mauris a ante. Suspendisse quam sem, consequat at, <del>Dinner’s at 5:00.</del> commodo vitae, feugiat in, nunc. Morbi imperdiet augue <mark>mark element</mark> quis tellus.

## 3. Images

![Large example image](http://placehold.it/800x400 "Large example image")

![Medium example image](http://placehold.it/400x200 "Medium example image")
![Small example image](http://placehold.it/200x200 "Small example image")

### 3-1. Image Alignment

![Center example image](http://placehold.it/200x200 "Center")
{: .center}

## 4. Blockquotes

> Lorem ipsum dolor sit amet, test link adipiscing elit. Nullam dignissim convallis est. Quisque aliquam.

## 5. List Types

### Unordered List

* Lorem ipsum dolor sit amet, consectetur adipiscing elit.
* Nam ultrices nunc in nisi pellentesque ultricies. Cras scelerisque ipsum in ante laoreet viverra. Pellentesque eget quam et augue molestie tincidunt ac ut ex. Sed quis velit vulputate, rutrum nisl sit amet, molestie neque. Vivamus sed augue at turpis suscipit fringilla.
* Integer pretium nisl vitae justo aliquam, at varius nisi blandit.
  1. Nunc vehicula nulla ac odio gravida vestibulum sed nec mauris.
  2. Duis at diam eget arcu dapibus consequat.
* Etiam vel elit in purus iaculis pretium.

### Ordered List

1. Quisque ullamcorper leo non ex pretium, in fermentum libero imperdiet.
2. Donec eu nulla euismod, rhoncus ipsum nec, faucibus elit.
3. Nam blandit purus gravida, accumsan sem in, lacinia orci.
  * Duis congue dui nec nisi posuere, at luctus velit semper.
  * Suspendisse in lorem id lacus elementum pretium nec vel nibh.
4. Aliquam eget ipsum laoreet, maximus risus vitae, iaculis leo.

### Definition Lists

kramdown
: A Markdown-superset converter

Maruku
: Another Markdown-superset converter

## 6. Tables

| Header1 | Header2 | Header3 |
|:--------|:-------:|--------:|
| cell1   | cell2   | cell3   |
| cell4   | cell5   | cell6   |
|----
| cell1   | cell2   | cell3   |
| cell4   | cell5   | cell6   |
|=====
| Foot1   | Foot2   | Foot3


## 7. Code Snippets

### Highlighted Code Blocks

```css
#container {
  float: left;
  margin: 0 -240px 0 0;
  width: 100%;
}
```

### Standard code block

    <div id="awesome">
      <p>This is great isn't it?</p>
    </div>
