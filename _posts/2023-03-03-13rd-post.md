---
layout: post
title: "Classical Object Detection - SVM"
description: "Part 2. 컴퓨터비전 특화 이론과 실습", "Ch05. 객체탐지(Object Detection)와 분할(Segmentation)"
date: 2023-03-03
tags: study
comments: true
---

Classical Object Detection은 어떤 feature를 뽑고 그 feature에서 특정한 위치로 localize한다.
그 localize된 bounding box에서 그것이 어떤 object인지 아닌지를 classify하는 classifier를 학습한다.
classifier를 어떻게 학습했는지에 대해 배워본다.

---
# Concpets of Support Vector Machine

 * Which hyperplane is best?
    - Support Vector Machine: The hyperplane that maximizes the margin
  ![image](https://user-images.githubusercontent.com/122149118/224546985-a4c0cf24-3ea4-46f4-aaee-cf274a4a58f8.png)


 대표적인 classifier는 Support Vector Machine이다.
 Support Vector Machine은 어떤 data의 사이를 나누어주는 hyper plane을 학습하는 것이다.
 가장 좋은 hyper plane은 data point들이 있을 때 margin, plane과 data들 사이의 거리를 뜻하는데 이 거리를 maximize한, data를 정확하게 반으로 나누어주는 hyper plane이 가장 좋다.
 hyper plane에 가장 가까운 data point와 hyper plane의 margin을 높여주는 것이 support vector이고 이것을 찾는 과정을 Support Vector Machine이라고 한다.
 결국 data point들을 가장 잘 나누어주는 hyper plane을 찾는 것이다.

## Support Vector Machine (SVM)

 * Characteristics
    - Supervised learning technique
    - Finding the hyperplane that separates two classes with <mark>the largest margin</mark>
       - The hyperplane that has the largest distance to the cearest training data points of any class
       - Generally, the larger the margin → the lower the generalization error

 * Pros
    - Very powerful classifier, especially when combined with kernel method
    - Works very well in practice, even with very small training sample size

 * Cons
    - No "direct" extension to multi-class SVM

 SVM의 기본적인 성질은 Supervised learning이다.
 학습 당시에 data point들의 label을 알고 있다는 것이다.
 2개의 class를 나누어주는 hyperplane을 찾는 것이다.
 또한 margin을 가장 largest하게 만드는 hyperplane을 찾는 과정이다.
 이 hyperplane은 각 class에 대한 nearest training data point와 largest distance를 찾는 것이며 일반적으로 margin이 가장 클 때 generalization error는 가장 줄어드는 경향성이 일반적이다.
 장점은 가장 강한 classifier이고, 특히 kernel method, 어떤 embedding space를 high demension으로 높이거나 이러한 kernel method와 결합했을 때 매우 성능이 좋은 classifier이다.
 실제로도 가장 잘 작동하며 small training sample size도 잘 작동한다.
 단점은 multi-class SVM으로의 direct extension이 있다.
 bounding box에 있는 것이 어떤 class인지 아닌지를 하나의 class에대해서만 구분하는 것이 아니라 multi-class label에서 multi-class SVM으로 가장 direct한 extension은 없고 맞는지 아닌지만 판단해준다는 것이다.

# Linear SVM, Seperable Case

 ![image](https://user-images.githubusercontent.com/122149118/224549988-f9c45985-633f-4210-8087-b1cbd2167f12.png)

 * Seperabel training examples, (x<sub>1</sub>, y<sub>1</sub>), (x<sub>2</sub>, y<sub>2</sub>),..., (x<sub>n</sub>, y<sub>n</sub>) 
 * Distance between a point **<i>x<sub>i</sub></i>** and the hyperplane **<i>w<sup>T</sup>x** + b</i>
 ![image](https://user-images.githubusercontent.com/122149118/224549780-f671607a-856b-46ad-8933-54fe5cbc07bd.png)

 Linear SVM은 data들이 먼저 모두 분리되는 case를 살펴본다.
 Seperable한 training example, (x<sub>1</sub>, y<sub>1</sub>), (x<sub>2</sub>, y<sub>2</sub>),..., (x<sub>n</sub>, y<sub>n</sub>)까지 있을 때 x<sub>1</sub>, x<sub>2</sub>는 각각 label y<sub>1</sub>, y<sub>2</sub>를 가지고 있다.
 positive인 경우 label이 0보다 큰 값을 가지고 negative인 경우 label이 0보다 작은 값을 가진다.
 이 때 hyperplane w<sup>T</sup> + b와 data point들 x<sub>i</sub>의 사이의 거리는 위의 식으로 나타낼 수 있다.
 이 식은 label이 0보다 큰지 작은지와 hyperplane에 x<sub>i</sub>값을 대입한 것에 vector, w의 norm값으로 나누어준 것이다.

 * Margin
 ![image](https://user-images.githubusercontent.com/122149118/224549858-777419a1-9261-4bde-af70-3de6fd13cc6d.png)

 모든 data point들 사이에서 어떤 hyperplane과 거리가 가장 작은 값이 margin이다.
 negative class의 경우를 보면 제 2사분면 x축에 가까이 있는 data point가 negative class의 margin을 결정하는 기준이된다.
 positive class의 경우 제 2사분면에 초록색 영역에 들어와 있는 data point가 margin을 계산하는 support vector가 될 것이다.
 이 data point 두 개 사이를 가장 잘 나누어주는 hyperplane을 찾는 것이 목적이다.

 * Max-margin solution
 ![image](https://user-images.githubusercontent.com/122149118/224549943-7d628820-3b14-4508-b67b-4466f0d1ef86.png)

 solution으로 max-margin solution을 사용한다.
 가장 낮은 거리를 margin을 가장 크게 만드는 w와 b, argmax를 찾는 것이다.
 그림에서 볼 수 있듯이 파란색 선을 이동하여 초록색과 빨간색을 나누어주는 이 경계를 찾는 것이다.

 ![image](https://user-images.githubusercontent.com/122149118/224599802-65a9f619-7dd5-41ff-9ff4-5989b0ba7f67.png)
    - Direct solution of this optimization problem would be complicated
 * Identical, but easier-to-solve problem
    - Rescaling the parameters <i>**w**</i> and <i>b</i> so that the margin becomes 1: <i>y<sub>i</sub></i>(<i>**w<sup>T</sup>x<sub>i</sub>** + b</i>) ≥ 1

  linear SVM은 max-margin solution으로 푸는데, 가장 hyper plane과 가까이에 있는, distance가 minimum한 i에 대해서 가장 maximize한 w와 b를 찾는 것이 목표이다.
 이 optimization 문제를 푸는 direct solution은 상당히 복잡하다.
 norm에 대한 제곱 수식도 포함되어있고, vector또한 있기 때문이다.
 identical 하지만 easier-to-solve로 변환하기위해 parameter w와 bias b를 rescaling을 해준다.
 margin이 1이 될 수 있도록 data point들에 대해 <i>y<sub>i</sub></i>(<i>**w<sup>T</sup>x<sub>i</sub>** + b</i>) ≥ 1와 같은 constraint를 가지도록 변환을 한다.

  ![image](https://user-images.githubusercontent.com/122149118/224600740-8a87b254-0e3d-4be7-9302-c2214771d6f0.png)
    **One constraint per example**

 기존의 max-margin solution에 대해서 변환을 하고 example마다 constraint가 1개만 있을 때 두 번째 수식으로 변환할 수 있고 support vector와 hyperplane 사이의 거리가 1로 만들었을 때 수식이 간단해진다.
 w 값에 쓸 argmax 값을 찾으면 된다.
 모든 data point들의 hyper plane 대입했을 때 값이 1보다 같거나 크다는 조건하에 argmax를 argmin으로 변환하게 되면 w가 역수가 되면서 제곱값으로 변한다.

 ![image](https://user-images.githubusercontent.com/122149118/224715533-db6afa6e-0bd1-43c4-8d6c-c391cc592fd8.png)

 이 solution을 푸는 방식은 어떤 f(x, y)가 있을 때 g(x, y)가 0보다 크거나 같은 경우에 대해 풀면, f(x, y)가 위의 예시처럼 norm이 원형 방정식이라고 했을 때 반지름을 점차 늘려 접하는 직선 g(x, y)를 찾으면 그 때 (x, y) point가 optimal한 point로 λ는 0보다 크거나 같다는 조건을 두고 편미분을 통하여 optimal한 (x, y)값, 즉 argmin f(x, y)를 찾을 수 있다.

 ![image](https://user-images.githubusercontent.com/122149118/224716709-54df1ec7-86f7-4313-a678-40a8fb305f9b.png)

 이러한 문제를 푼다고 했을 때 결국 min max optimization problem을 푸는 형태로 바꿀 수 있다.
 minmal한 w 값을 찾고 hyperplane에 support vector를 대입한 값이다.
 기존에 있던 KKT condition이라는, Quadratic Programming을 푸는 optimization에서 나와있던 조건을 w대신 대입하여 수식을 전개하고 maxmize하는 λ를 찾도록 dual problem으로 푸는 것이다.
 따라서 위의 수식은 기존의 objective function을 Dual form으로 변경한 형태이다.
 λ<sub>i</sub>에 대해서 0보다 크거나 같다는 가정이 있으며 λ<sub>i</sub>y<sub>i</sub>를 대입하고 모두 더하게 되면 0이라는 값이 나오도록 조건을 추가적으로 주어 λ를 maximize하는 dual problem으로 풀어 w와 b를 찾는 것이다.

 * Final objective function formulated by Quadratic Programming(QP)
 ![image](https://user-images.githubusercontent.com/122149118/224719531-74ec70a1-e293-4d33-861f-427a9cb8ca16.png)

 final objective function은 Quadratic Programming (QP)의 형태로 나누어지게 된다.

 * The solution **<i>λ<sup>*</sup></i>** = [<i>λ<sup>*</sup><sub>1</sub>, λ<sup>*</sup><sub>2</sub>,..., λ<sup>*</sup><sub>n</sub></i>]<sup>T</sup> can be obtained by any QP solver.

 QP solver를 사용하여 λ vector를 찾아내어 결국 (x, y) point를 나눈 hyper plane을 찾을 수 있다.

 * Parameters for the max-margin hyperplane
    - Weight coefficients **<i>w</i>**
    ![image](https://user-images.githubusercontent.com/122149118/224721052-ba48922a-ae0e-409e-a11a-f7c02d5b5054.png)
    - Any data point for which λ<sup>*</sup><sub>i</sub> = 0 will not contribute
    - A training example **x<sub>i</sub>** is called <i>support vector</i> if λ<sup>*</sup><sub>i</sub> > 0

  ![image](https://user-images.githubusercontent.com/122149118/224722552-36549af9-44fa-4a5f-8212-63ed51da9d6a.png)

 max-margin hyperplane을 위한 parameter들로는 weigh coefficient가 있다.
 KKT condition에 맞춘 수식에 대한 w 값을 정의할 수 있으며 λ 값을 QP solver를 통해 알게 되면 x, y값도 자연스럽게 알 수 있게 되고 w값을 알 수 있게 된다.
 λ<sup>*</sup><sub>i</sub>가 0인 경우는 contribute를 못하는 것으로 support vector의 margin에 걸치지 않는 data는 도움을 주지 못하는 것이다. 
 λ<sup>*</sup><sub>i</sub>가 0보다 큰 support vector의 margin값에 걸치는 data point들이 training에 기여하는 hyperplane을 정하는데 기여하는 것이다.
 이러한 x, y data point들을 support vector라고 한다.

 * Bias parameter <i>b</i>
    - From the fact that <i>y<sub>i</sub>(**w<sup>*T</sup>x<sub>i</sub> + b)</i> = 1 for all support vectors,
    ![image](https://user-images.githubusercontent.com/122149118/224723297-f98d9085-8553-4e4d-a4ed-ebb12eb5d094.png)

 bias parameter b는 hyperplane상에서 plane을 수직이동 해주는 것으로 모든 support vector에 대해서 <i>y<sub>i</sub>(**w<sup>*T</sup>x<sub>i**</sub> + b)</i> = 1 이 조건을 만족시키는 값을 찾으면 된다.
 sample 수에 대해 이 수식을 전개해주면 y<sub>i</sub>와 λ<sup>*</sup><sub>j</sub>, x<sub>i</sub>, x<sub>j</sub>값들을 다 알고있기 때문에 대입하면 b 값을 얻게 된다.
 QP solver를 통해 hyper plane을 찾아주는 것이 핵심이다.

 * Testing
 ![image](https://user-images.githubusercontent.com/122149118/224725730-971cf7bb-d543-47dc-8947-a1944bbb8113.png)
 ![image](https://user-images.githubusercontent.com/122149118/224725794-820495d8-6f77-4530-b36a-ec40c24dccb0.png)

 test time때는 w*와 b값을 알게되는데 여기에 data point들을 넣었을 때 0보다 크거나 같다면 +1이고 초록색이라고 classify하면 되는 것이고, 어떤 data point, 회색을 넣었을 때 0보다 작다면 -1이라고 classify하면 된다.
 hyperplane사이로 classification 문제를 test할 때 적용할 수 있다.
 보지 않은 data point에 대해서도 풀 수 있다.
 data point들은 localize된 object bounding box안에 어떤 image가 feature space로 projection된 vector들이라고 보면 된다.

 * Soft margin
    - Introduce slack variables, ξ<sub>i</sub> > 0
    - Allow training example **<i>x<sub>i</sub></i>** to be within the margin or even on the wrong side of the linear separator

 ![image](https://user-images.githubusercontent.com/122149118/224745188-184d39a3-4047-4cac-ab56-af8572a3349e.png)

  항상 data들이 saparabel한 case만 있는 것은 아니다.
 saparable하지 않은 경우에는 위 그림과 같이 빨간색 점들이 초록색 쪽으로 더 가까워질 수 있고, 초록색이 빨간색쪽으로 넘어올 수도 있다.
 직선인 hyperplane으로 구분할 수 없는 경우도 있기 때문에 soft margin slack variable을 추가해준다.
 ξ값을 통하여 hyper plane의 적절한 soft margin을 통해 추가 해준다.
 training example이 margin이 linear separator가 다른 side로 넘어가도 허용을 해주는 것이다.

 * New objective function with slack variables
 ![image](https://user-images.githubusercontent.com/122149118/224745128-db948b63-4372-4d4b-b2b3-0ada566739a9.png)
 ![image](https://user-images.githubusercontent.com/122149118/224745262-d4c41eb9-eaf2-4314-a9bf-4e5a8cea0ff3.png)

 기존의 optimization 수식에서 regularizer 같이 값을 추가해주는 것이다.
 optimization 수식의 조건이 0보다 크거나 같은 방향값을 추가하도록 허용해주는 것이다.
 최종적으로 사용하는 form이 soft margin을 추가한 SVM과 hinge loss의 form이 같다.
 argmin을 해주고 constraint를 추가해준다.
 마지막 식에서 ξ는 1 - y<sub>i</sub>(w<sup>T</sup>x<sub>i</sub> + b), 1 - 어떠한 hyperplane으로 정의된다.
 잘못된 data point를 1에서 빼주고 이 값을 최대화시키는 값을 찾는다.
 만약 hyperplane과 관련이 없는 값이라면 max로 0이 도출될 것이다.
 하지만 조금이라도 hyperplane을 넘어갈 가능성이 있는 point라면 뒤에 값이 max로 잡혀 반영이 될 것이다.
 
# Non-Linear SVM
 
 * When datset is linearly separable
 ![image](https://user-images.githubusercontent.com/122149118/224763980-fe0887b9-58a0-4584-813e-268beac33a39.png)

 data point가 hyperplane을 넘을 수 있는 경우를 non-linear SVM이라고 한다.
 data가 separabel한 경우는 빨간색 point와 파란색 point를 support vector로 잡아 hyperplane을 찾으면 된다.

 * What if dataset is not linearly separable?
 ![image](https://user-images.githubusercontent.com/122149118/224764224-2df85b00-13a0-45dd-adf4-50caf6b12f44.png)
    - Mapping data to the higher-dimensional sapce using <i>kernel method</i>
    ![image](https://user-images.githubusercontent.com/122149118/224764406-aa2efa5a-2d3e-42c9-9302-0d9e349c2fc5.png)
 
 linear한 hyperplane인데 data가 separabel하지 않은 경우는 위 그림에서와 같이 1d 평면에서는 빨간색과 파란색을 하나의 linear함수로 구분할 수 없으므로 이런 경우에는 data를 high dimensional space로 mapping시켜주는 방법인 kernel method를 사용한다.
 kernel method를 사용하여 2차원 평면으로 projection하면 직선으로 구분이 가능하여 linear한 SVM을 찾을 수 있다.
 즉 kernel method를 사용하여 linear SVM을 활용할 수 있다.
 강력하게 많은 low dimensional data라고 해도 higher-dimensional space로 upsample mapping을 시키고 어떠한 linear classifier로도 분리할 수 있다.

 * Kernel method
    - The original input space can always be mapped to some higher dimensional space where separation of training data may be easier
    
    Mapping to a higher dimensional space 
    **<i>x → φ(x)</i>**
    ![image](https://user-images.githubusercontent.com/122149118/224766882-111034a0-9d37-4791-8cb8-b2918200d1a0.png)

    Linear separation in the higher dimensional space
    ![image](https://user-images.githubusercontent.com/122149118/224767013-e03b8ee9-c3bd-4ef7-94cc-400b494a5176.png) 

 kernel method는 original input space가 있을 때 이것을 higher-dimensional space로 항상 mapping할 수 있다는 방식이다.
 train data를 separation하기에 좋다.
 kernel을 학습시키는 것인데 이 kernel은 neural network가 될 수도 있고, high dimensional mapping하는 radom variable weight가 될 수도 있어 다양한 방법을 사용할 수 있다.
 그림에서와 같이 2차원 평면에서 data point들이 곡선으로 나누어질 때 support vector machine은 직선, wx + b라는 hyper plane이기 때문에 higher-dimensional space로 mapping하여 사용한다.
 보라색으로 나타낸 hyper plane을 학습하여 파란색과 빨간색을 data point들을 구분할 수 있게 된다.
 higher-dimensional space의 x<sub>i</sub>와 x가 원래 값으로 있었다면 φ라는 kernel을 추가하여 mapping을 시키는 것이고 이 표현을 k(x<sub>i</sub>, x)로, x는 함수값이고 x<sub>i</sub>, y<sub>i</sub>, λ*<sub>i</sub>는 실제 data point이다.
 이렇게 kernel을 통하여 f(x)를 표현할 수 있다.

 * Widely-used kernels
    - Linear kernel: <i>k(x<sub>i</sub>, x<sub>j</sub>) = x<sub>i</sub><sup>T</sup>x<sub>j</sub></i>
    - Polynomaial kernel: <i>k(x<sub>i</sub>, x<sub>j</sub>) = (x<sub>i</sub><sup>T</sup>x<sub>j</sub>)<sup>d</sup></i>
    - Gaussian kernel (Radial basis function - RBF) kernel: <i>k(x<sub>i</sub>, x<sub>j</sub>) =exp(-||x<sub>i</sub> - x<sub>j</sub>||<sup>2</sup>/σ<sup>2</sup>)</i>
    - Histogram intersection kernel: <i>k(x<sub>i</sub>, x<sub>j</sub>) = ∑<sup>B</sup><sub>b = 1</sub>min(x<sub>i</sub><sup>b</sup>, x<sub>j</sub><sup>b</sup>)</i>
    - And many others

    In most cases,we do not need to know the explict form of the mapping function φ, but only know k(·,·), the inner product of the mapping results

    For some <i>k</i>, the corresponding <i>φ</i> maps <i>x</i> to a point in an infinite-dimensional space

    If you are interested in the kernel method, check <i>Reproducing Kernel Hilbert Space(RKHS)</i> for more details

 non-linear SVM에서 사용하는 kernel의 종류는 가장 기본적인 linear kernel이 있다.
 Polynomial kernel은 d 제곱을 해주게 된다.
 Gaussian kernel은 Radial basis function, RBF kernel이라고도 하는데 Gaussian distribution의 form을 사용하여 kernel을 사용할 수 있다.
 Histogram inersection kernel은 histogram의 형태를 가지는 kernel을 사용할 수 있다.
 다른 많은 kernel들이 있다.
 explicit한 form을 가지는 mapping function에 대해 이야기했지만 반드시 explict한 form을 가지지 않아도 된다.
 즉 이 kernel 자체를 학습시켜도 된다는 것이다.
 어떤 kernel이 있다는 것을 알고 mapping의 inner product만 사용해도 된다.
 어떤 kernel function은 φ를 사용하는데 x를 infinite한 dimensional space로 projection을 mapping하는 function을 사용하기도 한다.
 SVM이 linear한 classifier이고 kernel method들로 higher dimensional projection을 하고 mapping을 시켜 분리할 수 있다는 것이 핵심이고, non-linear한 classifier를 사용하지 않아도 SVM으로 분리할 수 있다.
 어떤 경우에도 linear classifier, wx + b로 decision boundary를 정할 수 있다.
 결국에는 neural network에 최종적으로 어떤 logic으로 판단하는 multi level classifier의 기초적인 이론적 토대가 된다.

 * How to optimize the SVM objective function with kernel
    - Recall the dual form of the SVM objective function
    ![image](https://user-images.githubusercontent.com/122149118/225044022-5d111f65-e31e-4969-b728-ce05a5f05e06.png)
    - The same optimization techmiques can be used
    - We only know the kernel <i>k</i>, and don't have to know the mapping <i>φ</i> explicitly

 kernel을 사용한 SVM objective function을 optimize하는 방법은 dual form의 문제를 푸는 것인데, 위와 같이 max λ를 찾고 kernel을 통과하게 되면 x<sup>T</sup><sub>i</sub>x<sub>j</sub>에 φ라는 mapping 함수가 통과되어 φ(x<sub>i</sub>)<sup>T</sup>φ(x<sub>j</sub>)가 된다.
 φ라는 mapping 함수는 kernel이라는 형태로 k(x<sub>i</sub>x<sub>j</sub>) 표현할 수 있다.
 kernel 함수로 다양한 함수가 있다는 것과 학습할 수 있다는 것을 배웠다.
 linear SVM과 동일한 optimization technique을 사용할 수 있다.
 kernel k를 알고 있을 때, mapping φ explictly는 몰라도 된다.

 * Linear Vs. Non-Linear
 
 A. Linear separation
 ![image](https://user-images.githubusercontent.com/122149118/225048881-f82a1e49-f0ec-422e-b870-6806566c9c44.png)

 support vector가 margin값에 걸리는 네모와 동그라미를 support vector로 잡게 되고 hyper plane은 wx + b로 linear하게 class 1과 class 2를 분류하는 soft vector machine을 갖는다.

 B. Non-Linear separation
 ![image](https://user-images.githubusercontent.com/122149118/225049048-80060e3a-338a-46e9-8bf8-d08ce19fde74.png)

 non-linear한 separation은 support vector machine을 정할 수 있는데 support vector machine hyperplane으로부터 가장 가까운 차이값을 보이는 것을 찾을 수 있다.
 projection된 값이나 higher dimensional로 mapping되는 mapping function이 있고 이것을 kernel 함수로 구현이 되어있다고 생각하면 된다.
 따라서 hyperplane이지만 곡선의 형태로 decision boundary를 그릴 수 있다는 것이다.

# Hyper-Parameters Tuning

 * Hyper-parameters
    - Weight to the sum of slack variables in the soft-margin SVM
    ![image](https://user-images.githubusercontent.com/122149118/225051407-7581e90a-cd7a-4c81-8ba5-f56a8b756a88.png)

 tuning해야 하는 hyper parameter는 soft-margin SVM에서 slack variable의 sum에 사용하는 weight가 있다.
 soft-margin에 얼마나 weight를 줄 것인지, 잘못된 decision boundary를 넘어가는 값들이 있을 때 얼만큼의 가중치를 줄 것인지, 얼마나 이 noise에 대해서 민감하게 반응할 것인지에 대한 hyper parameter이다.   
    - kernel parameters
    ![image](https://user-images.githubusercontent.com/122149118/225051497-dcb6065b-411b-4b4a-a397-d22d124ca6ab.png)
 두 번째는 kernel parameter가 있다.
 이러한 kernel method를 polynomier 값을 얼마나 줄지에 대한 값이 있을 수 있으며 또는 kernel method를 gaussian form을 RBF kernel form으로 사용할 때 σ값을 얼마나 줄지, 민감도 설정을 할 수 있다.

 * How to select appropriate values for the hyper-parameters?
    
 결국 tuning을 해주어야 하는데 training plane을 만들고 test를 해보고 하는 것이 naive approach이다.

 * A naive approach
    - Select hyper-parameter values that minimize training error
    - OVERFITTING
 * A better approach: Cross Validation
    - Divide the training dataset into <i>**K**</i> parts
    - Set aside one of the parts for validation
    - Learn SVMs with the remaining <i>**K</i> - 1** parts by varying hyper-parameters
    - Evaluate errors of the learned models on the validation set
    - Repeat the 2~4 steps and calculate mean errors per hyper-parameter set
    - Select the hyper-parameter set with lowest mean error

 training error가 가장 작아지는 hyper parameter를 계속 찾는 것이다.
 좋은 값을 찾을 수도 있지만, naive하게 찾다보면 overfitting의 우려가 있다.
 기본적으로 data set에서 많이 사용하는 approach인 cross validation을 사용하면 된다.
 process는 다음과 같다.
 먼저, training dataset을 k개의 part로 나눈다.
 그 중의 하나를 validation set으로 정한다.
 k - 1개의 part를 학습하는 데에 사용한다.
 학습된 model에 validation set에 대해 error값을 잰다.
 2번째에서 4번째 과정을 hyper parameter set에 대해서 error를 계속 측정한다.
 가장 낮은 mean값 error를 잡는 hyper parameter를 선택하는 것이다.
 cross validation error를 k-cross validation 방식을 사용하면 좀 더 train set이 overfitting되는 hyper parameter를 찾는 우려를 줄일 수 있다.

 ![image](https://user-images.githubusercontent.com/122149118/225055183-b69207a3-c0ee-4bd8-a487-8df04e0514bf.png)

 training error가 줄어들 때 오히려 overfitting이 되는 경우이다.
 model complexity, 즉 hyper parameter를 더 복잡하게 찾아줄 때 training error는 더 줄어들 수 있다.
 mean absolute error는 더 줄어드는데 training error가 줄어들수록 더 복잡한 model을 사용하면 일반화 시켰을 때 오히려 cross validation error가 더 커지게 되는 것이다.
 따라서 3정도의 model complexity를 가지는 어떤 hyper parameter를 찾았을 때 조금 더 좋은 값을 찾을 수 있는 것이며 cross validation 기법을 사용했을 때 더 적절한 값을 찾을 수 있는 것이다.
 cross validation은 강력한 approach이다.
 예전에는 지금과 같이 large scale data가 없고 적은 data set으로 computer vision 문제를 풀거나 또는 data model들을 만들 때는 cross validation을 반드시했었으며 benchmark가 없을 때 model의 적절성을 찾는 중요하고 효과적인 방법이었다.
 지금은 benchmark들이 잘 적립되어있고 benchmark의 효용성이 검증되었기 때문에 cross validation을 잘 사용하지 않는다.
 하지만 data의 수가 적을 때는 꼭 필요한 approach이다.

# Multi-Class SVM

 * One-versus-all
    - Training: learn an SVM for each class Vs. the others
    - Testing: apply each SVM to test example and assign to it the class of the SVM that returns the highest decision value

 * One-versus-one
    - Training: learn an SVM for each pair of classes
    - Testing: each learned SVM "votes" for a class to assign to the test example

 지금까지 separabel한 SVM을 이야기 했고 그것이 linear하게 separable할 수도 있고 non-linear하게 separabel할 수도 있었다.
 non-linear하게 separabel하면 kernel method를 통해 data들을 high dimensional space로 mapping했고 mapping function을 kernel method를 통해 찾았다.
 이것을 multi-class로 확장시켜 각 classifier는 지금 2개의 class로 나누는 것인데 one-versus-all방식으로 모든 class에 대해서 다 학습을 하는 것이다.
 class 1인지 아닌지, class 2인지 아닌지 모든 classifier를 다 학습하는 것이다.
 class의 개수가 n개라면 n개의 classifier를 학습하는 것이다.
 이 SVM을 모든 test example에 대해서 통과시키는 것이다.
 예를들면 deep learning에서 어떤 CNN model을 통과하여 최종적으로 classifier를 통과하는 어떤 vector가 만약 imageNet이면 1000개가 있는데, 1000개의 vector에 값들이 다 classifier인 것이다.
 각 SVM을 test example에 대해서 통과시켜 가장 decision value가 높은 어떤 값을 class로 할당시켜준다.
 또한, one-versus-one방식도 있는데 각 SVM을 각 class의 pair로 나누는 SVM을 학습하는 것이다.
 class 1인지, class 2인지로 나누는 classifier, class 1인지, class 3인지로 나누는 classifier를 다 학습시키는 것이다.
 class의 개수가 n개라면 n(n - 1)개의 classifier가 학습이 된다.
 그 classifier를 1인지에 대해서는 (n - 1)개의 classifier가 있을 것인데 이 것을 class의 probability에 대해 voting하여 class의 test example에 대해 1일 확률이 (n - 1)개 있고, 2일 확률이 (n - 1)개 있기 때문에 학습된 classifier가 어떤 확률이 제일 높은지 voting을 통해 학습할 수 있고 확인할 수 있어 이 것을 aggregation하여 조금 더 robust한, noise에 insensitive한 classifier를 학습할 수 있다.
 one-versus-one 방식이 조금 더 noise에 robust하지만 모든 classifier의 pair를 다 학습해야한다는 점에서 학습 시간이 오래걸릴 수 있다는 비효율성이 있을 수 있다.
 반면에 one-versus-all은 하나의 classifier만 학습하면 되므로 학습 시간은 줄어들지만 noise에 sensitive한 모습을 보일 수도 있을 것이다.
 multi-class SVM을 기반으로 objective detection의 어떤 localize된 bounding box의 class lable을 판단하는 방식에 대해 알아보았다.
 soft-vector machine은 고전적으로 굉장히 많이 활용되던 방식이다.