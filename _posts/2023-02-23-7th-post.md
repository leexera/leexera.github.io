---
layout: post
title: "Linear Regression"
description: "ì´í™”ì—¬ìëŒ€í•™êµ ê°•ì œì› êµìˆ˜ë‹˜(Electronic & Electrical Engineering)"
date: 2023-02-23
tags: study
comments: true
---

Supervised Learningì— ë¶„ë¥˜ì—ì„œ ì‚´í´ë³¸ ë°”ì™€ ê°™ì´ regressionì€ modelì˜ ì¶œë ¥ì´ ì—°ì†ì¸ ê°’ì„ ê°–ê²Œ ëœë‹¤.
ì—°ì†ë˜ëŠ” ì¶œë ¥ì„ ì˜ˆì¸¡í•˜ê³  ì¶”ë¡ í•˜ê¸° ìœ„í•˜ì—¬ data setì—ì„œ ì…ë ¥ê³¼ ì •ë‹µìœ¼ë¡œ êµ¬ì„±ì´ ë˜ì–´ìˆëŠ” ì¦‰, labelì´ ìˆëŠ” data setì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤.

---
# ì§€ë„ í•™ìŠµ

linear regression modelì„ ì‚´í´ë³´ê¸° ì•ì„œ ë¨¼ì € linear modelì— ëŒ€í•´ ì•Œì•„ë³¸ë‹¤.

## Linear models

 * Hypothesis set Î— : a set of lines
 > <i>h<sub>w</sub>(x) = Î¸<sub>0</sub> + Î¸<sub>1</sub>k<sub>1</sub> + ... + Î¸<sub>d</sub>k<sub>d</sub> = **Î¸<sup>Tx**</i>
 ###### Î¸ : model parameter (learnable parameter)
 > <i> h<sub>w</sub>(x) = Î¸<sub>0</sub> + Î¸<sub>1</sub>k<sub>1</sub>(x<sub>1</sub>) + ... + Î¸<sub>d</sub>k<sub>d</sub>(x<sub>d</sub>) = Î¸<sup>T</sup>k(x) = **x<sup>n</sup>**</i>
 ###### Linear model with a set of arbitary functions (more general case), Linear in Î¸, not necessarily in x

 * Many advantages : good for a first try
    * Simplicity : easy to implement and interpret
    * Generalization : higher chance <i>E<sub>test</sub> â‰ˆ E<sub>train</sub></i>
    * Solve regression and classificatio problems

 ![image](https://user-images.githubusercontent.com/122149118/220711232-f64a28fe-d0d5-462d-95c8-5d9b664ab6b2.png)

 Hypothesis í•¨ìˆ˜ Hê°€ ì…ë ¥ featureì™€ model parameterì˜ linear combinationìœ¼ë¡œ êµ¬ì„±ì´ë˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.
 ì²« ë²ˆì§¸ ìˆ˜ì‹ì—ì„œ ë³´ëŠ” ë°”ì™€ ê°™ì´ hypothesis HëŠ” model parameterì¸ Î¸<sub>0</sub>, Î¸<sub>1</sub>ë¶€í„° Î¸<sub>d</sub>ê¹Œì§€ì˜ parameterë¥¼ ê°€ì§€ê²Œ ë˜ë©° ì´ëŸ¬í•œ parameterë“¤ì€ ì…ë ¥ featureì¸ x<sub>1</sub>ë¶€í„° x<sub>d</sub>ê¹Œì§€ì˜ ì„ í˜• í•©ìœ¼ë¡œ êµ¬ì„±ì´ ë˜ì–´ ìˆëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
 ì„ í˜• í•©ì€ Î¸ Transpose xë¡œ ì •ì˜í•  ìˆ˜ ìˆê²Œ ë˜ëŠ”ë° ì´ ë•Œ xëŠ” d-demensionalí•œ vectorì´ë¯€ë¡œ ì„ì˜ì˜ x<sub>0</sub>ëŠ” 1ì´ë¼ê³  í•˜ëŠ” ê°’ì„ ì„¤ì •í•˜ì—¬ Î¸<sub>0</sub>ì™€ í•¨ê»˜ ê³±í•´ì£¼ëŠ” í˜•íƒœë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.
 ìµœì¢…ì ìœ¼ë¡œ Î¸ Transpose xì™€ ê°™ì€ í˜•íƒœë¡œ vectorë¥¼ ê³±ìœ¼ë¡œ êµ¬ì„±í•  ìˆ˜ê°€ ìˆëŠ” ê²ƒì´ë‹¤.
 <mark>ì´ ì‹ì—ì„œ ìœ ë…í•´ì•¼í•  ê²ƒì€ ì„ í˜• modelì´ë¼ê³  í•´ì„œ ë°˜ë“œì‹œ ì…ë ¥ ë³€ìˆ˜ì˜ ì„ í˜•ì¼ í•„ìš”ëŠ” ì—†ë‹¤ê³  í•˜ëŠ” ê²ƒì´ë‹¤.</mark>
 ë‹¤ìŒ ì‹ì€ xì— ëŒ€í•´ì„œëŠ” ì„ í˜•ì‹ì´ ì•„ë‹˜ì„ ë³¼ ìˆ˜ê°€ ìˆë‹¤.
 kernal í•¨ìˆ˜ kë¼ë˜ê°€ ì…ë ¥ featureì˜ ë³€í™˜ í•¨ìˆ˜ì¸ feeì™€ ê°™ì€ ê²½ìš°ëŠ” xì— ëŒ€í•œ linear í•¨ìˆ˜ê°€ ì•„ë‹ ìˆ˜ë„ ìˆë‹¤.
 í•˜ì§€ë§Œ ì—¬ì „íˆ model parameter Î¸ì— ëŒ€í•´ì„œëŠ” model í•¨ìˆ˜ Hê°€ linear combinationìœ¼ë¡œ êµ¬ì„±ì´ ë˜ì–´ ìˆëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
 ì´ëŸ¬í•œ ì„ í˜• modelì€ ì—¬ëŸ¬ê°€ì§€ ì¥ì ì´ ìˆë‹¤. 
 ì²« ë²ˆì§¸ë¡œëŠ” êµ‰ì¥íˆ ë‹¨ìˆœí•˜ë‹¤ë¼ëŠ” ê²ƒì´ë‹¤.
 ê°€ì¥ ì²˜ìŒìœ¼ë¡œ ì‹œë„ë¥¼ í•´ë³´ê³  ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê¸°ê°€ ì‰½ë‹¤.
 ë˜í•œ ì´ëŸ° ì„ í˜• modelì´ê¸° ë•Œë¬¸ì— ì–´ë– í•œ ì…ë ¥ ë³€ìˆ˜ì—ì„œ í•´ë‹¹í•˜ëŠ” ìš”ì†Œê°€ ì¶œë ¥ì´ ì–¼ë§ˆì •ë„ì˜ ì˜í–¥ì„ ì£¼ê²Œ ë˜ëŠ”ì§€ë¥¼ í•´ì„ì´ ê°€ëŠ¥í•  ìˆ˜ ìˆë‹¤.
 ì˜ˆì»¨ë°, ì…ë ¥ parameter x<sub>i</sub>ê°€ ìˆë‹¤ê³  í•  ë•Œ x<sub>i</sub>ì— ê³±í•´ì ¸ ìˆëŠ” model parameter Î¸<sub>i</sub>ì˜ ê´€ê³„ë¥¼ ìƒê°í•´ë³´ë©´ Î¸<sub>i</sub>ì˜ ê°’ì´ êµ‰ì¥íˆ í¬ë‹¤ë©´ x<sub>i</sub>ê°’ì´ 1 ë‹¨ìœ„ë¡œ ë°”ë€” ë•Œë§ˆë‹¤ êµ‰ì¥íˆ í° ì˜í–¥ì„ ì£¼ê²Œ ë  ê²ƒì´ë‹¤.
 ë‹¤ìŒìœ¼ë¡œëŠ” ì¼ë°˜í™”ì´ë‹¤.
 modelì´ ë‹¨ìˆœí•˜ê¸° ë•Œë¬¸ì— ì„¤ë ¹ ì„±ëŠ¥ì´ ì•„ì£¼ ë†’ì§€ëŠ” ì•Šë”ë¼ë„ êµ‰ì¥íˆ ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ì œê³µí•  ìˆ˜ê°€ ìˆë‹¤.
 ì´ëŸ¬í•œ linear modelì€ regressionì´ë‚˜ classificationì— ë‘ë£¨ ì‚¬ìš©ì„ í•  ìˆ˜ ìˆë‹¤.
 ì„ í˜• modelì´ë¼ê³  í•´ì„œ ë°˜ë“œì‹œ ì…ë ¥ ë³€ìˆ˜ì— ì„ í˜•ì¼ í•„ìš”ëŠ” ì—†ë‹¤. 

## Feature orgavization

 ![image](https://user-images.githubusercontent.com/122149118/220715031-1ddf592d-6b71-41d5-82a6-10dbb1d445e6.png)

 > <i>h<sub>w</sub>(x) = Î¸<sub>0</sub> + Î¸<sub>1</sub>Î¦<sub>1</sub>(x<sub>1</sub>) + ... + Î¸<sub>d</sub>Î¦<sub>d</sub>(x<sub>d</sub>) = **Î¸<sup>TÎ¦(x)**</i>
 ###### Î¸ : model parameter (linear combination of features)
 > <i> h<sub>w</sub>(x) = Î¸<sub>0</sub> + Î¸<sub>1</sub>k<sub>1</sub>Î¦(x<sub>1</sub>) + ... + Î¸<sub>d</sub>k<sub>d</sub>Î¦(x<sub>d</sub>) = **Î¸<sup>T</sup>kÎ¦(x)**</i>

 ê·¸ì— ê´€í•œ ì…ë ¥ dataì˜ ë³€í™˜ í•¨ìˆ˜ì— ê´€í•œ ì˜ˆì‹œë¡œ ì…ë ¥ìœ¼ë¡œ ë©”ì¼ ì£¼ì†Œë¥¼ ë°›ì•˜ë‹¤ê³  ìƒê°í•´ë³¸ë‹¤. 
 ì´ëŸ¬í•œ ì…ë ¥ ë©”ì¼ ì£¼ì†Œ xë¥¼ ë°›ì•˜ë‹¤ê³  í•  ë•Œ, ì´ xë¥¼ linear modelì— ê·¸ã…ë¡œ í™œìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì´ xë¡œë¶€í„° featureë¥¼ ë½‘ëŠ” ë°”ë¡œ feature extracter Î¦ë¥¼ ìƒê°í•´ë³¸ë‹¤ê³  í•˜ë©´ ì´ ë•Œ feature extracterëŠ” ì´ ë©”ì¼ì˜ ê¸¸ì´ë¼ë˜ê°€ ì•„ë‹ˆë©´ íŠ¹ì •í•œ ê¸°í˜¸ë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ ë“±ê³¼ ê°™ì€ ì…ë ¥ featureë¥¼ êµ¬ì„±í•˜ê²Œ ë˜ê³  ì´ ì…ë ¥ featureë¡œ ë¶€í„° ìš°ë¦¬ê°€ Î¦ì™€ì˜ linear combinationì„ í†µí•´ hypothesis í•¨ìˆ˜ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆê²Œ ëœë‹¤.

## Example: happiness

 * Predict real valued output <i>y</i> (happiness) from <i>x</i> when <i>D = (x, y)</i> is given

 ![image](https://user-images.githubusercontent.com/122149118/220717294-73255831-b580-40f8-97d3-1c9672430081.png)

 linear regression ë¬¸ì œì˜ ì˜ˆì‹œë¥¼ ì‚´í´ë³¸ë‹¤.
 regressionì€ Supervised Learningì˜ ë¬¸ì œì— í¬í•¨ë˜ê¸° ë•Œë¬¸ì— ì‚¬ìš©í•˜ëŠ” data sampleì˜ êµ¬ì„±ì€ ì…ë ¥ xì™€ ì¶œë ¥ yì˜ pairë¡œ êµ¬ì„±ë˜ê²Œ ëœë‹¤.
 ì´ ë•Œ yëŠ” ì—°ì†ì ì¸ ê°’ì´ ëœë‹¤.
 ì˜ˆë¥¼ ë“¤ì–´ ì™¼ìª½ì˜ ë¬¸ì œëŠ” ì§€ê¸ˆ í˜„ì¬ ìš°ë¦¬ì˜ ì—°ë´‰ ìˆ˜ì…ì— ë”°ë¥¸ í–‰ë³µë„ì— ê´€í•œ ì¡°ì‚¬ ìë£Œì´ë‹¤.
 ì¼ë°˜ì ìœ¼ë¡œ ìƒê°í•˜ê¸°ì— ìš°ë¦¬ì˜ ìˆ˜ì…ì´ ëŠ˜ì–´ë‚¨ì— ë”°ë¼ì„œ í–‰ë³µë„ê°€ ì¦ê°€í•  ìˆ˜ ìˆë‹¤ê³  ìƒê°í•˜ê¸° ë•Œë¬¸ì— ì¶©ë¶„íˆ ì„ í˜• modelì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
 ì´ ë•Œ ì…ë ¥ ë³€ìˆ˜ê°€ í•˜ë‚˜ì´ê¸° ë•Œë¬¸ì— ì´ë¥¼ univariate problemì´ë¼ê³  í•œë‹¤.
 í•˜ì§€ë§Œ ìš°ë¦¬ì˜ í–‰ë³µë„ê°€ ë¹„ë‹¨ ìˆ˜ì…ì— ì˜í•´ì„œë§Œ ê²°ì •ëœë‹¤ê³  ë³´ê¸°ëŠ” ì–´ë ¤ìš¸ ê²ƒì´ë‹¤.
 ê·¸ë˜ì„œ ë³´ë‹¤ ë‹¤ì–‘í•œ ë³€ìˆ˜ë¥¼ ìƒê°í•´ë³¼ ìˆ˜ ìˆë‹¤.
 ì˜ˆì»¨ë° ìš°ë¦¬ê°€ ê·¸ë¦¼ì—ì„œì™€ ê°™ì´ ì¸ìƒì˜ ë§Œì¡±ë„ë¼ë˜ê°€ ì•„ë‹ˆë©´ ììœ , ì¢…êµ ë“±ê³¼ ê°™ì€ ê²½ìš°ë„ ì¶©ë¶„íˆ í–‰ë³µì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” factorë¡œì¨ ë™ì‘í•  ìˆ˜ ìˆë‹¤.
 ë‹¤ì–‘í•œ ë³€ìˆ˜ë“¤ì„ regression modelì— í¬í•¨í•˜ê²Œ ë˜ëŠ” ê²ƒì„ multicariate problemì´ë¼ê³  í•œë‹¤.
 ì´ ë‘ ê°€ì§€ëŠ” ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì œê°€ ì•„ë‹ˆë¼ ì…ë ¥ ë³€ìˆ˜ì˜ ê°¯ìˆ˜ì— ì˜í•´ì„œë§Œ êµ¬ë¶„ì„ í•˜ê²Œ ëœ ê²ƒì´ë‹¤.
 linear regressionì€ ê·¸ë¦¼ì—ì„œì™€ ê°™ì´ ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•´ ì¶œë ¥ê³¼ì˜ ì„ í˜•ì ì¸ ê´€ê³„ë¥¼ ì¶”ë¡ í•˜ëŠ” ë¬¸ì œì´ë‹¤.

## Linear regression framework

 **Hypothesis function to map from <i>x</i> to <i>y</i>**

 ![image](https://user-images.githubusercontent.com/122149118/220719107-466c10e9-5a97-4458-b3e5-5c956c8ac66c.png)

 training data setì—ì„œ ì£¼ì–´ì§„ ì…ë ¥ê³¼ ì¶œë ¥ ìƒìœ¼ë¡œì„œ ì„ í˜• modelì„ í•™ìŠµì„ í•˜ê²Œ ë˜ê³  ìƒˆë¡œìš´ ì…ë ¥, ì˜ˆë¥¼ ë“¤ë©´ ìˆ«ì 3ì´ ë“¤ì–´ì™”ì„ ë•Œ ì¶œë ¥ 2.71ê³¼ ê°™ì€ outputì„ ì‚°ì¶œí•˜ëŠ” modelì´ë‹¤. 

![image](https://user-images.githubusercontent.com/122149118/220720324-9bd07b00-45d0-461a-863a-b772d14a560b.png)

 ì´ëŸ¬í•œ ëª©í‘œë¥¼ ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.
 ì²« ë²ˆì§¸ëŠ” ì–´ë–¤ predictorë¥¼ ì´ìš©í•˜ê²Œ ë  ê²ƒì¸ê°€ë¥¼ ê²°ì •í•˜ëŠ” ê²ƒì´ë‹¤. 
 ì´ ì˜ˆì‹œì—ì„œëŠ” ì…ë ¥ì´ í•˜ë‚˜ì´ê¸° ë•Œë¬¸ì— Univariate linear modelì„ ì‚¬ìš©í–ˆë‹¤.
 ì…ë ¥ featureê°€ í•˜ë‚˜ì´ê¸´ í•˜ì§€ë§Œ ìš°ë¦¬ì˜ model parameterëŠ” Î¸<sub>0</sub>, Î¸<sub>1</sub> 2ê°œ ì„ì„ ì£¼ëª©í•´ì•¼í•œë‹¤.
 Î¸<sub>0</sub>ëŠ” ì´ linear modelì˜ offsetì— í•´ë‹¹í•˜ëŠ” parameterë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. 
 linear modelì—ì„œëŠ” ë³´í†µ lossë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ Mean Square Error(MSE)ë¥¼ í™œìš©í•˜ê²Œ ëœë‹¤. 
 yëŠ” data setì—ì„œì˜ ì •ë‹µì´ ë˜ëŠ” ê²ƒì´ê³  x<sup>(i)</sup>ëŠ” modelì˜ ì¶œë ¥ì´ ëœë‹¤.
 ê·¸ ì°¨ì´ëŠ” modelì˜ ì¶œë ¥ê³¼ ì •ë‹µê³¼ì˜ ì˜¤ì°¨ì´ë‹¤.
 ë‹¤ìŒì€ ì–´ë–»ê²Œ parameterë¥¼ êµ¬í•˜ëŠ”ì§€ì— ê´€í•œ ë‚´ìš©ì´ë‹¤.
 Gradient descent alforithmì´ë¼ë˜ê°€ ì•„ë‹ˆë©´ Nomal equationì„ í†µí•´ ì–»ê²Œ ëœë‹¤.
 ì‚¬ì‹¤ ì´ë¯¸ ì„ í˜• modelì€ ì‚¬ìš©í•˜ê² ë‹¤ê³  í•œ ìˆœê°„ ì´ë¯¸ model í˜•íƒœê°€ ì£¼ì–´ì§€ëŠ” ê²ƒì´ê³ , ì•ì„œ ë³¸ ë°”ì™€ ê°™ì´ Î¸<sub>0</sub> + Î¸<sub>1</sub>xì— ëŒ€í•œ í•¨ìˆ˜ê°€ ë˜ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  loss function ì—­ì‹œ MSEë¥¼ ì‚¬ìš©í•˜ë‹ˆê¹Œ ì£¼ì–´ì§„ ì •ì˜ì™€ ê°™ì´ ì‚¬ìš©í•˜ê²Œ ëœë‹¤.

## Linear regression: parameter opt.

 Idea: choose Î¸<sub>0</sub>, Î¸<sub>1</sub> so that <i>h<sub>Î¸</sub>(x)</i> is close to <i>y</i> using our training set
   <i>h<sub>Î¸</sub>(x) = Î¸<sub>0</sub> + Î¸<sub>1</sub>x</i>

 ![image](https://user-images.githubusercontent.com/122149118/220721928-776cbadd-53db-425f-9626-bba02c239b8a.png)

 ì£¼ì–´ì§„ ì„ í˜• modelì—ì„œ ê°€ì¥ ì™¼ìª½ì€ ê¸°ìš¸ê¸°(Î¸<sub>0</sub>)ê°€ 0ì´ê³  offset(Î¸<sub>1</sub>)ì´ 1.5ì¸ modelì´ë‹¤. 
 ê°ê° linear modelì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. 
 ì´ë ‡ê²Œ model parameterê°€ ë‹¬ë¼ì§ì— ë”°ë¼ì„œ ì£¼ì–´ì§„ dataì— fittingí•˜ëŠ” ê³¼ì •ì—ì„œ ì˜¤ì°¨ê°€ ë°œìƒí•˜ê²Œ ë  ê²ƒì´ë‹¤. 

## L<sub>2</sub> cost function (Goal : minimizing MSE)

 ![image](https://user-images.githubusercontent.com/122149118/220723132-7c37f72d-c090-499c-9e83-ab5f8fffe852.png)

 ì„œë¡œ ë‹¬ë¼ì§„ Î¸ì— ë”°ë¼ì„œ ì˜¤ë¥¸ìª½ê³¼ ê°™ì€ curveë¥¼ ê·¸ë ¤ë³´ë©´ ê·¸ë¦¼ì—ì„œ ë³´ëŠ” ë°”ì™€ ê°™ì´ ì´ loss functionì€ model parameterì— ì˜í•œ í•¨ìˆ˜ê°€ ë˜ê²Œ ëœë‹¤. 
 ì¦‰ Î¸<sub>0</sub>, Î¸<sub>1</sub>ì´ ë°”ë€œì— ë”°ë¼ error í•¨ìˆ˜ëŠ” ë°”ë€Œê²Œ ë˜ê¸° ë•Œë¬¸ì— ì´ëŸ¬í•œ error surface í•¨ìˆ˜ë¥¼ ê°–ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤. 
 ì´ëŸ¬í•œ cost functionì„ ìµœì†Œí™”í•˜ëŠ” model parameter, Î¸<sub>0</sub>, Î¸<sub>1</sub>ì„ ì°¾ëŠ”ê²ƒì´ ëª©ì ì´ë‹¤.
 ê·¸ ì§€ì ì€ ê·¸ë¦¼ì—ì„œ í˜„ì¬ ê°€ì¥ ì›€í‘¹ íŒ¨ì¸ ì§€ì ì— ì¡´ì¬í•œë‹¤. 
 ê·¸ì— mappingí•˜ëŠ” Î¸<sub>0</sub>ì™€ Î¸<sub>1</sub>ì˜ ê°’ì„ ì°¾ë„ë¡ í•˜ëŠ” ê²ƒì´ ë°”ë¡œ linear modelì´ ê°€ì¥ dataì— fittingì„ í•˜ë„ë¡ í•˜ëŠ” linear modelì„ ì°¾ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.

 ![image](https://user-images.githubusercontent.com/122149118/220724966-f33644d3-2f85-4782-94dd-e60873444d77.png)

 ![image](https://user-images.githubusercontent.com/122149118/220725251-4589f42f-c66d-4e7d-80cc-a780021aa539.png)

 ì˜ˆë¥¼ ë“¤ì–´ ê°€ì¥ ì´ˆê¸°ì— Î¸<sub>0</sub>ì™€ Î¸<sub>1</sub>ì´ ì™¼ìª½ì—ì„œì™€ ê°™ì´ ì£¼ì–´ì§„ë‹¤ë©´ ì˜¤ë¥¸ìª½ì˜ ë“±ê³ ì„ ì—ì„œ ë³´ì´ëŠ” ë°”ì™€ ê°™ì´ cost ê°’ì´ ë†’ê²Œ ë‚˜íƒ€ë‚  ê²ƒì´ë‹¤.
 ê·¸ëŸ¬ë‚˜ í•™ìŠµì´ ì§„í–‰ ë˜ë©´ì„œ ì ì°¨ Î¸<sub>0</sub>ì™€ Î¸<sub>1</sub>ì´ ì£¼ì–´ì§„ dataì— fittingì´ ë˜ë©´ì„œ ë“±ê³ ì„ ì—ì„œ ë‚®ì€ ê°’ì„ ê°–ë„ë¡ ë°”ë€ŒëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.
 ê·¸ë¦¬ê³  loss functionì´ ê°€ì¥ ë‚®ì€ ê°’ì„ ê°€ì§ˆ ë•Œ í•´ë‹¹ parameter Î¸<sub>0</sub>ì™€ Î¸<sub>1</sub>ì´ ê°€ì¥ ìµœì í™”ëœ ì„ í˜• modelì„ ì œê³µí•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
 ğŸ” Î¸<sub>0</sub>ì™€ Î¸<sub>1</sub>ì„ ì–´ë–»ê²Œ êµ¬í•  ìˆ˜ ìˆëŠ”ê°€?
 â¡ ì´ëŸ¬í•œ ê³¼ì •ì„ parameter ìµœì í™”ë¼ê³  í•œë‹¤. 

## Optimization

 **-Matrix representation in data**

 <i>m</i> samples <i>(x<sup>(i)</sup>,y<sup>(i)</sup>),...,(x<sup>(m)</sup>,y<sup>(m)</sup>) ; **d</i>-dimensional features.**

 ![image](https://user-images.githubusercontent.com/122149118/220728413-b412afea-26b7-4722-8f30-209a78038dcb.png)

 ![image](https://user-images.githubusercontent.com/122149118/220728512-d2c390c9-3c21-4fd0-a897-5ab86928f990.png)

  * Data matrix X âˆˆ **R**<sup>N X (d + 1)</sup>
     - rows vector : inputs as x<sup>m</sup> âˆˆ **R**<sup>N X (d + 1)</sup>
  * Target vector y âˆˆ **R**<sup>N
     - column vectors y<sup>m</sup>
  * Weight vector Î¸ âˆˆ **R**<sup>(d + 1)</sup>
  * In-sample error is a function of <i>Î¸</i> and data <i>X, y</i>

     ||<i>y - XÎ¸</i>||<sub>2</sub>

 ë‹¤ìŒê³¼ ê°™ì´ ì…ë ¥ dataë¥¼ matrixí˜•íƒœë¡œ ì •ë¦¬ë¥¼ í•´ë³´ë©´ ì´ ë•Œ ì…ë ¥ vector xëŠ” d-demensionalí•œ vectorë¼ê³  ìƒê°í–ˆê¸° ë•Œë¬¸ì— x<sub>1</sub>ë¶€í„° x<sub>d</sub>ê¹Œì§€ë¡œ í‘œí˜„ì´ ë˜ì–´ìˆì§€ë§Œ offset(Î¸<sub>0</sub>)ë¥¼ í¬í•¨í•˜ê¸° ìœ„í•´ì„œ ì•ì— 1ì„ ë„£ì–´ì£¼ì–´ì•¼ í•œë‹¤.
 ì´ëŸ¬í•œ ê°œê°œ vector xëŠ” matrixì˜ ê° low wiseí•˜ê²Œ ì­‰ ìŒ“ì•„ matrix xë¥¼ êµ¬ì„±í•˜ê²Œ ëœë‹¤. 
 ë§ˆì°¬ê°€ì§€ë¡œ linear regressionì—ì„œì˜ ì •ë‹µì¸ y ì—­ì‹œ n-dimensinoalí•œ vectorê°€ ë˜ê²Œ ëœë‹¤. 
 ê·¸ ì´ìœ ëŠ” nê°œì˜ sampleë§ˆë‹¤ í•˜ë‚˜ì”©ì˜ ì •ë‹µì´ ìˆê¸° ë•Œë¬¸ì´ë‹¤. 
 y ì—­ì‹œ ìœ„ì™€ ê°™ì€ í˜•íƒœë¡œ vectorë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆê²Œ ëœë‹¤. 
 parameter vector Î¸ëŠ” Î¸<sub>0</sub>ë¶€í„° Î¸<sub>d</sub>ê¹Œì§€ ìˆê¸° ë•Œë¬¸ì— d + 1ì˜ dimensional vectorì´ë‹¤. 
 Î¸ì™€ xì˜ ì„ í˜• ê²°í•©ì„ í†µí•´ scoreë¥¼ ì‹ì„ í†µí•´ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.
 ì´ì™€ ê°™ì´ Î¸ì™€ xë¥¼ ê³±í•œ ê°’ì„ scoreì´ë¼ê³  í•œë‹¤. ì´ score ê°’ê³¼ yì˜ ì°¨ì´ë¥¼ í†µí•´ lossë¥¼ í†µí•œ matrix í˜•íƒœë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. 
 ì¦‰, modelì˜ ì¶œë ¥ê³¼ yì˜ ì°¨ì´ë¥¼ ì œê³±í•˜ê³  í‰ê· í•˜ì—¬ ê³„ì‚°í•œë‹¤.
 ìµœì í™”ëœ íŒŒë¼ë¯¸í„° Î¸ëŠ” cost functionì„ ê°€ì¥ ìµœì†Œí™” í•˜ëŠ” ê²ƒì´ë‹¤. 

 **- Getting a solution Î¸**

 * Î¸<sup>*</sup>: the solution to linear regression
    - Derived by minimizing <i>E<sub>Î¸</sub></i> over all possible <i>Î¸ âˆˆ **R<sup>d + 1</sup></i>

 ![image](https://user-images.githubusercontent.com/122149118/220732692-228cc2fd-2531-497e-8cf5-f5155a9586e4.png)

 ![image](https://user-images.githubusercontent.com/122149118/220732819-cb1b0b9c-fcbe-4d3c-91a7-571b25664653.png)

 * <i>E</i> is continous,differentiable and convex
 * General optimization techniques
 * Gradient descent

 ì´ ì‹ì„ ê°„ë‹¨íˆ í’€ì–´ì„œ ì„¤ëª…í•˜ë©´ ë¨¼ì € ìµœì í™”ëœ parameter Î¸ì¸ loss function, E(Î¸)ë¥¼ ê°€ì¥ ìµœì†Œí™”í•˜ë„ë¡ í•˜ëŠ” model parameterì´ë‹¤. 

## Normal equation (Least Square)

 **- Analytic solution of Î¸**

 * Î¸<sup>*</sup>: the solution to linear regression
    - Derived by minimizing <i>E<sub>Î¸</sub></i> over all possible <i>Î¸ âˆˆ **R<sup>d + 1</sup></i>

 ![image](https://user-images.githubusercontent.com/122149118/220733589-b10a116e-a5a0-42e8-b621-1ec519dfeb43.png)

 <i>E</i> is continuous, differentiable and convex

 ì•ì—ì„œ ìˆ˜ì‹ì„ í’€ê¸° ìœ„í•´ Î¸ì— ê´€í•œ derivative termì„ êµ¬í•œ ë’¤ì— 0ì´ ë˜ë„ë¡í•˜ëŠ” Î¸ì˜ ê°’ì„ êµ¬í•˜ë©´ ìµœì í™”ëœ parameter ê°’ì„ êµ¬í•  ìˆ˜ ìˆë‹¤.
 ì´ëŸ¬í•œ ê³¼ì •ì€ Least Square Problemì´ë¼ê³  í•˜ë©° ì´ëŸ¬í•œ ë°©ì •ì‹ì„ Normal Equqtionì´ë¼ê³  í•œë‹¤.
 ë¬¼ë¡  ì´ë•Œ loss functionì´ differentiableí•˜ê³  convexí•´ì•¼í•œë‹¤ëŠ” ê°€ì •ì´ ìˆë‹¤.

 ![image](https://user-images.githubusercontent.com/122149118/220734294-edd0e725-fb73-40cc-91b9-1f65be2bf249.png)

 Î¸ì— ëŒ€í•´ì„œ ë¯¸ë¶„ì„ ìˆ˜í–‰í•˜ê²Œ ë˜ë¯€ë¡œ ì²« ë²ˆì§¸ ì‹ì—ì„œëŠ” Î¸ì™€ ë¬´ê´€í•œ termë“¤ì´ ëª¨ë‘ 0ìœ¼ë¡œ ë°”ë€Œê²Œ ëœë‹¤.
 ê·¸ë¦¬ê³  Î¸ì— ëŒ€í•´ì„œ ë‚˜ë¨¸ì§€ ì‹ë“¤ì€ ìœ„ì™€ ê°™ì€ ì „ê°œ ê³¼ì •ì„ ê±°ì³ ìœ ë„ê°€ ëœë‹¤.
 ì¤‘ê°„ ê³¼ì •ì€ ê´€ê³„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ìœ ë„ë˜ì—ˆë‹¤.
 ê²°ë¡ ì ìœ¼ë¡œ ìœ„ì˜ ìˆ˜ì‹ì„ í’€ì–´ ìµœì¢… í•´ë¥¼ êµ¬í•˜ê²Œ ë˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ìµœì  íŒŒë¼ë¯¸í„° Î¸ë¥¼ êµ¬í•  ìˆ˜ ìˆê²Œ ëœë‹¤.

 ![image](https://user-images.githubusercontent.com/122149118/220734901-3078d78b-6cdb-470e-a1f3-99a775237ef1.png)

 : one-step solution via matrix inversion and multiplications

 ![image](https://user-images.githubusercontent.com/122149118/220735062-23c9a466-f402-4651-b26c-8d88a21c54d6.png)

 ì´ ë•Œ X Transpose Xì˜ ê³±ì— inverseë¥¼ ì·¨í•˜ê²Œ ë˜ê³  ê±°ê¸°ì— X Transpose matrixë¥¼ ê³±í•œ í˜•íƒœë¥¼ <sup>+</sup>ë¼ê³  í•œë‹¤.
 ì´ëŸ¬í•œ ë°©ì‹ì˜ ì¥ì ì€ one-stepìœ¼ë¡œ í•´ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. 
 ì¦‰ ì•ì„œ êµ¬ì„±í•œ matrixì— yë¥¼ ê³±í•˜ê²Œ ë˜ë©´ í•œ ë²ˆì— ë¶„ì„ì ìœ¼ë¡œ í•˜ë‚˜ì˜ stepì„ í†µí•´ êµ¬í•  ìˆ˜ ìˆë‹¤.
 í•˜ì§€ë§Œ ì´ë ‡ê²Œ normal equationì„ ì´ìš©í•˜ì—¬ í•´ë¥¼ êµ¬í•˜ëŠ” ê²ƒì€ ìµœê·¼ì˜ Machine Learning ë¬¸ì œì—ì„œ Dataì˜ sample ìˆ«ìê°€ ëŠ˜ì–´ë‚˜ëŠ” ê²½ìš°ì— ëŒ€ë‹¨íˆ ë¹„íš¨ìœ¨ì ì¸ ë¬¸ì œë¥¼ ì•¼ê¸°í•œë‹¤. 

 **In practice**
 * What if the dimension of the input vector hugely increases (huge computational complexity)?
 * What if the matrix is not inverible (redundant features ; linearly dependent)?
 â†’ Needs ierative algorithm (gradient descent)

 data sampleì˜ ìˆ«ìê°€ ëŠ˜ì–´ë‚˜ê²Œ ë˜ë©´ ì¦‰, nì´ ëŠ˜ì–´ë‚˜ê²Œ ë˜ë©´ ì´ matrixì˜ dimensionì´ ì¦ê°€í•˜ê²Œ ë˜ê³  ê·¸ì— ë”°ë¼ matrix inverse ë“±ì˜ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ”ë° ëŒ€ë‹¨íˆ í° ë³µì¡ë„ê°€ ì†Œìš”ë˜ê¸° ë•Œë¬¸ì´ë‹¤.
 ë”ìš±ì´ matrixì˜ inverseê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°ë„ ìˆì„ ìˆ˜ ìˆë‹¤. 
 ì´ëŸ° ê²½ìš°ì— ì–´ë–»ê²Œ í•´ë¥¼ êµ¬í•  ê²ƒì¸ê°€?
 ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ gradient descentë°©ì‹ì„ ì ìš©í•œë‹¤. 
 Gradient descentëŠ” iterativeí•˜ê²Œ ìµœì  parameter Î¸ë¥¼ ì°¾ì•„ê°€ëŠ” ê³¼ì •ì´ë‹¤.
 Gradientë¼ëŠ” ê²ƒì€ í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•˜ì—¬ ì–»ëŠ” termìœ¼ë¡œ í•´ë‹¹ í•¨ìˆ˜ì˜ ë³€í™”í•˜ëŠ” ì •ë„ë¥¼ í‘œí˜„í•˜ëŠ” ê°’ì´ë‹¤.

## Iterative optimization by gradient descent

 * Gradient the derivative of vector functions
    - Direction of greatest increase (or decrease) of a function
    - Zero at (local) max/min

 Iteratively set the gradient to zero instead of analysically setting it to zero
 Gradient descent: a very general algorithm
   * Can train many other models with error measures

 ![image](https://user-images.githubusercontent.com/122149118/220738961-64945b57-3d8e-4508-94a4-f1e4eb720cf9.png)

 **Two things to decide:**
   * Which direction?
   * How much?

 ìœ„ì™€ ê°™ì´ ìš°ë¦¬ê°€ ìµœì†Œë¡œ í•˜ê³  ì‹¶ì€ loss functionê³¼ ê·¸ error surfaceê°€ ìˆë‹¤.
 ì„ì˜ì˜ pointì—ì„œ, ì´ error surfaceì—ì„œ ìµœì†Œì¸ pointë¥¼ ì°¾ì•„ê°€ëŠ” ê²ƒì´ ëª©ì ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. 
 ì´ error surfaceì—ì„œ ê°’ì´ ìµœì†Œì¸ pointì˜ íŠ¹ì§•ì€ gradientê°€ 0ì¸ ì§€ì ì´ë‹¤. 
 gradient descentì—ì„œëŠ” gradientê°€ 0ì¸ ì§€ì ê¹Œì§€ ë°˜ë³µì ìœ¼ë¡œ Î¸ë¥¼ ë°”ê¾¸ì–´ ë‚˜ê°€ë©´ì„œ íƒìƒ‰ì„ í•˜ê²Œ ëœë‹¤.
 ì˜ˆë¥¼ ë“¤ì–´ ì‚°ë“±ì„± ì–´ë”˜ê°€ì— ì¡´ì¬í•  ë•Œ, ì‚°ì—ì„œ ì•„ë˜ ì§€ì ê¹Œì§€ ë‚´ë ¤ì˜¨ë‹¤ë©´ ë‹¤ìŒ ë°œê±¸ìŒì€ ë°”ë¡œ í•´ë‹¹ ì§€ì ì—ì„œ í•˜ì‚°í•˜ëŠ” ê¸°ìš¸ê¸°ê°€ ê°€ì¥ í° ë°©í–¥ìœ¼ë¡œ í•œ ê±¸ìŒ ë”›ëŠ” ê²ƒì´ íš¨ê³¼ì ì¸ ì „ëµì´ ë  ê²ƒì´ë‹¤.
 Gradient descentë„ ë§ˆì°¬ê°€ì§€ë¡œ Gradient, ì¦‰ í•¨ìˆ˜ì˜ ë³€í™”ë„ê°€ ê°€ì¥ í° ë°©í–¥ìœ¼ë¡œ ì´ë™í•´ë‚˜ê°€ë©° ë°˜ë³µí•˜ê²Œ ëœë‹¤.

## Gradient descent algorithm

 **Method to solve numerically**

 ![image](https://user-images.githubusercontent.com/122149118/220740453-81fc4dc6-31ff-4895-a519-c1a03ea89a73.png)
 ###### Î± : Learning rate

 **Two things to decide:**
   * Which direction?
      Steepest gradient descent with a greedy method
   * How much?
    Step size

 ![image](https://user-images.githubusercontent.com/122149118/220740910-6d7fdad9-ad0e-4fc7-99d9-a0ace9c24a9b.png)

 ![image](https://user-images.githubusercontent.com/122149118/220740976-8e250d84-a1b1-460b-ac8c-1f05b275a1d5.png)

 gradient descentì—ì„œ parameterë¥¼ updateí•˜ëŠ” ê³¼ì •ì„ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ì—ˆë‹¤.
 Î¸<sub>new</sub>ëŠ” ìƒˆë¡œìš´ parameterë¥¼ ì˜ë¯¸í•œë‹¤. 
 Î¸<sub>old</sub>ëŠ” update ì´ì „ì˜ parameterë¥¼ ì˜ë¯¸í•œë‹¤. 
 ìƒˆë¡œìš´ parameterë¥¼ updateí•˜ê¸° ìœ„í•´ ì¶”ê°€í•˜ëŠ” termì´ gradient descentì—ì„œ ì´ì•¼ê¸°í•˜ê²Œ ë˜ëŠ” gradient ê°’ì´ ëœë‹¤. 
 ì´ Î±ì˜ ê°’ì„ step sizeë¼ê³  ì´ì•¼ê¸° í•œë‹¤.
 ë°”ë¡œ parameterë¥¼ updateí•˜ëŠ”ë° ì†ë„ë¥¼ ì¡°ì ˆí•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤. 
 ë§Œì•½ Î±ê°€ ë„ˆë¬´ ì‘ê²Œë˜ë©´ ìˆ˜ë ´í•˜ëŠ”ë° ë„ˆë¬´ ë§ì€ ì‹œê°„ì´ ê±¸ë¦¬ê²Œ ë  ê²ƒì´ë‹¤. 
 ë°˜ëŒ€ë¡œ Î±ê°€ ë„ˆë¬´ í¬ê²Œë˜ë©´ gradientê°€ 0ì¸ ê³³ì„ ë†“ì¹˜ê¸°ê°€ ì‰¬ì›Œ ìˆ˜ë ´í•˜ê¸°ê°€ ì–´ë ¤ì›Œì§„ë‹¤.
 gradient descent algorithmì˜ íŠ¹ì§• ì¤‘ í•˜ë‚˜ëŠ” greedy methodë¼ê³  í•˜ëŠ” ê²ƒì´ë‹¤.
 í˜„ì¬ ì§€ì ì—ì„œ ê°€ì¥ ë³€í™”ë„ê°€ ê°€íŒŒë¥¸ ë°©í–¥ìœ¼ë¡œ updateí•´ ë‚˜ê°€ê¸° ë•Œë¬¸ì´ë‹¤.
 ì´ëŸ¬í•œ Greedy algorithmì˜ íŠ¹ì„±ìœ¼ë¡œ Gradient descent alogorithmì€ ê²½ìš°ì— ë”°ë¼ Local optimumë§Œì„ ë‹¬ì„±í•˜ê¸° ì‰½ë‹¤. 

## Illustration: Error surface

 <i>E<sub>train</sub>(Î¸)</i> in high-dimensional space

 ![image](https://user-images.githubusercontent.com/122149118/220742633-43c00337-2070-4192-b54d-a4c5000634ea.png)

 Idea: get a step into the direction the stepest gradient descent
 Property: local optimum, and the result depens on an initial position.

 ğŸ” Global optimumê³¼ Local optimumì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€?
 â¡ Global optimumì€ ì „ì²´ error surfaceì—ì„œ ê°€ì¥ ìµœì†Œì¸ ê°’ì„ ê°–ëŠ” ì§€ì ì´ë‹¤.
 ë°˜ëŒ€ë¡œ Local optimumì€ ì§€ì—­ì ìœ¼ë¡œ ìµœì†Œì´ì§€ë§Œ ì „ì²´ ì˜ì—­ì„ ë†“ê³  ë³´ì•˜ì„ ë•ŒëŠ” ìµœì†Œê°€ ì•„ë‹ ìˆ˜ ìˆëŠ” ì§€ì ì´ë‹¤.
 ê·¸ë¦¼ì—ì„œì™€ ê°™ì´ ì„œë¡œ ë‹¤ë¥¸ ë‘ ì‹œì‘ì ì—ì„œ Gradient descentë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²½ìš° ì–´ëŠ í•˜ë‚˜ëŠ” global optimumì„, ì–´ëŠ í•˜ë‚˜ëŠ” local optimumì„ ë‹¬ì„±í•˜ì—¬ ì „í˜€ ë‹¤ë¥¸ ê°’ì„ ê°–ê²Œ ë  ìˆ˜ ìˆë‹¤.
 í˜„ì¬ ì§€ì ì—ì„œ ê°€ì¥ ê°€íŒŒë¥¸ ë³€í™”ë„ë¥¼ ë³´ê³  ì›€ì§ì´ê¸° ë•Œë¬¸ì— ì´ë™í•˜ëŠ” ë°©í–¥ì´ ì „ì²´ì—ì„œ ìµœì í™”ëœ pointê°€ ì•„ë‹ˆê²Œ ë  ê°€ëŠ¥ì„±ì´ ìˆë‹¤. 

 **Outline:** The function <i><u>J</i> is the objective function</u> that we want to optimize.
 <u>Î±: the step size to control</u> the rate to move down the error surface.
 It is a hyper parameter, which is a positive number (c.f. <i>Î¸</i> is a learnable parameter)
 * Start with initial parameters <i>Î¸<sub>0</sub>, Î¸<sub>1</sub></i>
 * Keep changing the parameters to reduce / until achieving the minimal cost
 ![image](https://user-images.githubusercontent.com/122149118/220744565-a441a19c-1d60-44e2-a7b8-5441d1b66b25.png)
 ìš°ì„  ìµœì í™”í•˜ê³ ì í•˜ëŠ” loss function ë˜ëŠ” objective functionì„ ì„¸ìš´ë‹¤. 
 ê·¸ë¦¬ê³  ë‚˜ì„œ ì‚¬ì „ì— Î±ì™€ ê°™ì€ algorithm ìˆ˜í–‰ì— í•„ìš”í•œ parameterë¥¼ ì„¤ì •í•œë‹¤.
 ì´ë ‡ê²Œ ì‚¬ì „ì— ì„¤ì •í•˜ê²Œ ëœ parameterë¥¼ hyper parameterë¼ê³  í•œë‹¤.
 ì´ hyper parameterëŠ” í•­ìƒ ì–‘ì˜ ê°’ì„ ê°–ê²Œ ëœë‹¤.
 ë°˜ë©´ì— gradient descent algorithmì„ í†µí•´ í•™ìŠµì„ í•˜ê³ ì í•˜ëŠ” Î¸ë¥¼ leanable parameterë¼ê³  í•œë‹¤.
 gradient descent algorithmì˜ ì‹œì‘ì€ initialí•œ point Î¸<sub>0</sub>ì™€ Î¸<sub>1</sub>ì„ ê²°ì •í•˜ëŠ” ê²ƒë¶€í„° ì‹œì‘í•˜ê²Œ ëœë‹¤.
 ì´ì–´ì„œ ëª©ì  í•¨ìˆ˜ê°€ ìˆ˜ë ´í•  ë•Œê¹Œì§€ parameterë¥¼ ì§€ì†ì ìœ¼ë¡œ ë°”ê¾¸ì–´ ë‚˜ê°€ëŠ” ê³¼ì •ì„ ë°˜ë³µí•˜ê²Œ ëœë‹¤.

## Gradient descent algorithm

 linear regression modelì—ì„œ parameter updateë¥¼ ë³´ì—¬ì¤€ë‹¤.

 **for linear regression**

 **Linear regression model**

 ![image](https://user-images.githubusercontent.com/122149118/220748052-8106cc87-f7eb-4595-8ff3-23bb15b3cbf2.png)

 **â†“**{: .center}

 **Gradient descent algorithm**

 ![image](https://user-images.githubusercontent.com/122149118/220748328-58b8a2c4-b1ea-41e1-a17f-c0007c3ecd32.png)

 ![image](https://user-images.githubusercontent.com/122149118/220748416-4f49b8f3-5a8b-49d4-90ac-4ee50ae51d5d.png)

 gradient descentì— loss functionì˜ partial derivate termì„ êµ¬í•œ ê²ƒì´ë‹¤.
 ëª©ì  í•¨ìˆ˜ Jë¥¼ Î¸<sub>J</sub>ì— ëŒ€í•´ì„œ ë¯¸ë¶„í•˜ê³  ê°ê°ì˜ 2ê°œì˜ parameterë¥¼ ìœ„í•´ j = 0ì¸ ê²½ìš°, j = 1ì¸ ê²½ìš°ì— ëŒ€í•´ ê°ê° Î¸<sub>0</sub>ì™€ Î¸<sub>1</sub>ì— ëŒ€í•´ì„œ ë¯¸ë¶„ì„ í•˜ê²Œ ë˜ë©´ ìµœì¢…ì ì¸ ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.
 ì´ëŸ¬í•œ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ convergeí•  ë•Œê¹Œì§€ Î¸<sub>0</sub>ì™€ Î¸<sub>1</sub>ì„ updateë¥¼ í•´ ë‚˜ì•„ê°„ë‹¤.

 ![image](https://user-images.githubusercontent.com/122149118/220749203-3b4e6333-9b14-4c4c-9e0f-ff17303b1ba0.png)

 ì´ ë•Œ linear regressionì˜ ê° partial derivate termì„ êµ¬í•˜ê²Œ ë˜ë©´ ìœ„ì™€ ê°™ì€ í˜•íƒœë¥¼ ì–»ê²Œëœë‹¤.
 ì´ ì‹ì€ ê°ê°ì˜ ê³ ìœ í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ê³  ìˆë‹¤.
 ë¨¼ì € h<sub>Î¸</sub>(x<sup>(i)</sup>)ì™€ y<sup>(i)</sup>ì˜ ì°¨ì´ëŠ” error termì„ì„ ë‚˜íƒ€ë‚¸ë‹¤. 
 ì¦‰, Î¸<sub>0</sub>ì™€ ê°™ì€ ê²½ìš°ëŠ” sampleë“¤ì˜ ëª¨ë“  errorë¥¼ ì¸¡ì •í•œ ë‹¤ìŒì— ë”í•˜ê²Œ ë˜ê³  ê·¸ ë‹¤ìŒ hyper parameter Î±ë¥¼ ê³±í•˜ì—¬ updateë¥¼ ì§„í–‰í•´ ë‚˜ì•„ê°€ê²Œ ëœë‹¤.
 ë°˜ëŒ€ë¡œ Î¸<sub>1</sub>ì˜ ê²½ìš°ì— error term ì´ì™¸ì—ë„ sampleì„ ê³±í•˜ì—¬ ëª¨ë‘ ë”í•œ ë‹¤ìŒ updateë¥¼ ì§„í–‰í•œë‹¤.

## Gradient descent algorithm Vs Normal equation

 **<u>Gradient Descent</u>**
 * needs a number of iterations.
 * works well even when <i>n</i> is large
 * all examples (batch) are examined at each iteration
    * Use stochastic gradient descent(SGD) or mini batch
 * Several advances such as AdaGrad, RMSProp, Adam for optimization

 ![image](https://user-images.githubusercontent.com/122149118/220750841-03913b78-4788-436f-b758-679d5ee86352.png)

 **<u>Normal Equation</u>**
 * Need to compute an inverse matrics and slow if the number of samples is very large
 <i>(X<sup>T</sup>X)<sup>-1</sup></i>
 Gradient descentì™€ normal equation ì‚¬ì´ì—ëŠ” ì°¨ì´ê°€ ìˆë‹¤.
 ê·¸ ì°¨ì´ëŠ” Gradient descent ë°©ì‹ê³¼ ê°™ì€ ê²½ìš°ëŠ” ì—¬ëŸ¬ ë²ˆì˜ ë°˜ë³µì ì¸ ê³¼ì •ì„ ìˆ˜í–‰ì„ í†µí•´ í•´ë¥¼ ì–»ì–´ ë‚˜ê°„ë‹¤ê³  í•˜ëŠ” ê²ƒì´ë‹¤. 
 ë°˜ë©´ normal equation ê°™ì€ ê²½ìš° í•œ ë²ˆì—, 1 stepìœ¼ë¡œ í•´ë¥¼ êµ¬í•œë‹¤.
 í•˜ì§€ë§Œ ê·¸ í•´ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ì„œëŠ” ìœ„ì™€ ê°™ì€ matrixë¥¼ ê³±í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— sample sizeê°€ êµ‰ì¥íˆ ëŠ˜ì–´ë‚˜ê²Œ ë˜ëŠ” ê²½ìš°ì— inverse matrix ë“±ì„ êµ¬í•˜ê¸°ê°€ êµ‰ì¥íˆ ì–´ë ¤ì›Œì§€ëŠ” ë¬¸ì œê°€ ë°œìƒí•˜ê²Œ ëœë‹¤.
 Gradient descent algorithmê³¼ ê°™ì€ ê²½ìš°ëŠ” ì„¤ë ¹ nì´ êµ‰ì¥íˆ í¬ë”ë¼ë„ ë°˜ë³µì ìœ¼ë¡œ í•´ë¥¼ êµ¬í•´ ë‚˜ê°ˆ ìˆ˜ ìˆë‹¤. 
 ì•½ê°„ì˜ ë¶ˆí¸í•¨ì´ ì¡´ì¬í•˜ì§€ë§Œ ê·¸ëŸ¬í•œ ë¬¸ì œì ë“¤ì„ í•´ê²°í•˜ê¸° ìœ„í•´ stochastic gradient descent ë˜ëŠ” minibatch algorithmì´ ìˆë‹¤.
 ì´ëŸ¬í•œ ë°©ë²•ë“¤ì„ í†µí•´ gradient descent algorithmì´ Local minimaì— ë¹ ì§€ì§€ ì•Šë„ë¡ í•˜ëŠ” ì—¬ëŸ¬ê°€ì§€ íš¨ê³¼ì ì¸ ë°©ë²•ì´ ìˆë‹¤.

## Quiz

 **What answers are correct?**

 **A.** In linear regression, the solution is interpretable with input features
 linear regressionì—ì„œ solutionì´ í•´ì„ ê°€ëŠ¥í•˜ë‹¤.
 **A Correct.** The score is computed as a linear combination of input features and weights; the weight explains the importance of an input feature to the final output
 scoreê°€ ê³„ì‚°ë˜ì–´ ì…ë ¥ featureì™€ weightì˜ combinationìœ¼ë¡œ í™œìš©ì´ ë  ë•Œ ê·¸ì— í•´ë‹¹í•˜ëŠ” weightì˜ ê°’ì„ í†µí•´ì„œ í˜„ì¬ scoreê°€ ì „ì²´ outputì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ëŠ”ì§€ ê²°ì •í•  ìˆ˜ ìˆë‹¤.
 **B.** In linear regression, a hypothesis is not necessarily to be a linear form of leanable parameters
 linear regressionì´ hypothesisê°€ learnable parameter Î¸ì— ëŒ€í•´ì„œ ë°˜ë“œì‹œ linearí•œ í˜•íƒœì¼ í•„ìš”ëŠ” ì—†ë‹¤.
 **B False.** Linear regression model may not be a linear form of a raw data but it should be a linear form of parameters
 learnable parameterê°€ linearí•œ í˜•íƒœë¡œ ë§ì¶°ì ¸ì•¼ í•œë‹¤.

## Summary

 * Linear regression model

 **Y** = real number
 <i>h(x) = Î¸<sup>T</sup>Î¦(x)
 e = (y - h(x))<sup>2</sup></i>

 ![image](https://user-images.githubusercontent.com/122149118/220753556-75728259-25c4-437e-aeb1-9abd16f4b94d.png)

  * Can be readily solved using gradient descent
  * Interpretable and lightweight; worth to try first

 Linear regression modelì— ëŒ€í•´ì„œëŠ” Supervised Learningì´ê¸° ë•Œë¬¸ì— ì…ë ¥ feature xì™€ ì¶œë ¥ data yì˜ 1ìŒìœ¼ë¡œ í•™ìŠµì´ ì§„í–‰ë˜ëŠ”ë° ì´ ë•Œ yì˜ ê°’ì€ ì‹¤ìˆ˜, ì—°ì†ì ì¸ ê°’ìœ¼ë¡œ êµ¬ì„±ë˜ë©° modelì€ output h(x)ì™€ ì •ë‹µì¸ yì˜ ê·¸ ì°¨ì´ë¡œ í•˜ì—¬ê¸ˆ ê·¸ model errorë¥¼ ê°€ì¥ ìµœì†Œí™” í•˜ê²Œë” í•™ìŠµì´ ì§„í–‰ ë˜ê²Œ ëœë‹¤. 
 Model optimizationì„ ìœ„í•´ì„œëŠ” normal equationì„ ì´ìš©í•´ì„œ parameterë¥¼ êµ¬í•  ìˆ˜ë„ ìˆì§€ë§Œ ë³´í†µ ìµœê·¼ Machine Learning ë¬¸ì œì—ì„œ Gradient descent algorithmì„ í†µí•´ iterativeí•˜ê²Œ ê·¸ solutionì„ ì°¾ì•„ë‚˜ê°€ê²Œ ëœë‹¤.
 ì´ëŸ¬í•œ í•´ëŠ” interpretableí•˜ê³  ê°„í¸í•˜ê¸° ë•Œë¬¸ì— ì´ modelì€ ë¬¸ì œë¥¼ ê°€ì¥ ì²˜ìŒìœ¼ë¡œ ì ‘ê·¼ í•˜ê¸°ì— ëŒ€ë‹¨íˆ ì¢‹ì€ ë°©ì‹ì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.