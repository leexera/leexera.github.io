---
layout: post
title: "Linear Regression"
description: "이화여자대학교 강제원 교수님(Electronic & Electrical Engineering)"
date: 2023-02-23
tags: study
comments: true
---

Supervised Learning에 분류에서 살펴본 바와 같이 regression은 model의 출력이 연속인 값을 갖게 된다.
연속되는 출력을 예측하고 추론하기 위하여 data set에서 입력과 정답으로 구성이 되어있는 즉, label이 있는 data set을 사용하게 된다.

---
# 지도 학습

linear regression model을 살펴보기 앞서 먼저 linear model에 대해 알아본다.

## Linear models

 * Hypothesis set Η : a set of lines
 > <i>h<sub>w</sub>(x) = θ<sub>0</sub> + θ<sub>1</sub>k<sub>1</sub> + ... + θ<sub>d</sub>k<sub>d</sub> = **θ<sup>Tx**</i>
 ###### θ : model parameter (learnable parameter)
 > <i> h<sub>w</sub>(x) = θ<sub>0</sub> + θ<sub>1</sub>k<sub>1</sub>(x<sub>1</sub>) + ... + θ<sub>d</sub>k<sub>d</sub>(x<sub>d</sub>) = θ<sup>T</sup>k(x) = **x<sup>n</sup>**</i>
 ###### Linear model with a set of arbitary functions (more general case), Linear in θ, not necessarily in x

 * Many advantages : good for a first try
    * Simplicity : easy to implement and interpret
    * Generalization : higher chance <i>E<sub>test</sub> ≈ E<sub>train</sub></i>
    * Solve regression and classificatio problems

 ![image](https://user-images.githubusercontent.com/122149118/220711232-f64a28fe-d0d5-462d-95c8-5d9b664ab6b2.png)

 Hypothesis 함수 H가 입력 feature와 model parameter의 linear combination으로 구성이되는 것을 의미한다.
 첫 번째 수식에서 보는 바와 같이 hypothesis H는 model parameter인 θ<sub>0</sub>, θ<sub>1</sub>부터 θ<sub>d</sub>까지의 parameter를 가지게 되며 이러한 parameter들은 입력 feature인 x<sub>1</sub>부터 x<sub>d</sub>까지의 선형 합으로 구성이 되어 있는 것을 알 수 있다.
 선형 합은 θ Transpose x로 정의할 수 있게 되는데 이 때 x는 d-demensional한 vector이므로 임의의 x<sub>0</sub>는 1이라고 하는 값을 설정하여 θ<sub>0</sub>와 함께 곱해주는 형태로 구성되어 있다.
 최종적으로 θ Transpose x와 같은 형태로 vector를 곱으로 구성할 수가 있는 것이다.
 <mark>이 식에서 유념해야할 것은 선형 model이라고 해서 반드시 입력 변수의 선형일 필요는 없다고 하는 것이다.</mark>
 다음 식은 x에 대해서는 선형식이 아님을 볼 수가 있다.
 kernal 함수 k라던가 입력 feature의 변환 함수인 fee와 같은 경우는 x에 대한 linear 함수가 아닐 수도 있다.
 하지만 여전히 model parameter θ에 대해서는 model 함수 H가 linear combination으로 구성이 되어 있는 것을 알 수 있다.
 이러한 선형 model은 여러가지 장점이 있다. 
 첫 번째로는 굉장히 단순하다라는 것이다.
 가장 처음으로 시도를 해보고 성능을 측정하기가 쉽다.
 또한 이런 선형 model이기 때문에 어떠한 입력 변수에서 해당하는 요소가 출력이 얼마정도의 영향을 주게 되는지를 해석이 가능할 수 있다.
 예컨데, 입력 parameter x<sub>i</sub>가 있다고 할 때 x<sub>i</sub>에 곱해져 있는 model parameter θ<sub>i</sub>의 관계를 생각해보면 θ<sub>i</sub>의 값이 굉장히 크다면 x<sub>i</sub>값이 1 단위로 바뀔 때마다 굉장히 큰 영향을 주게 될 것이다.
 다음으로는 일반화이다.
 model이 단순하기 때문에 설령 성능이 아주 높지는 않더라도 굉장히 다양한 환경에서 안정적인 성능을 제공할 수가 있다.
 이러한 linear model은 regression이나 classification에 두루 사용을 할 수 있다.
 선형 model이라고 해서 반드시 입력 변수에 선형일 필요는 없다. 

## Feature orgavization

 ![image](https://user-images.githubusercontent.com/122149118/220715031-1ddf592d-6b71-41d5-82a6-10dbb1d445e6.png)

 > <i>h<sub>w</sub>(x) = θ<sub>0</sub> + θ<sub>1</sub>Φ<sub>1</sub>(x<sub>1</sub>) + ... + θ<sub>d</sub>Φ<sub>d</sub>(x<sub>d</sub>) = **θ<sup>TΦ(x)**</i>
 ###### θ : model parameter (linear combination of features)
 > <i> h<sub>w</sub>(x) = θ<sub>0</sub> + θ<sub>1</sub>k<sub>1</sub>Φ(x<sub>1</sub>) + ... + θ<sub>d</sub>k<sub>d</sub>Φ(x<sub>d</sub>) = **θ<sup>T</sup>kΦ(x)**</i>

 그에 관한 입력 data의 변환 함수에 관한 예시로 입력으로 메일 주소를 받았다고 생각해본다. 
 이러한 입력 메일 주소 x를 받았다고 할 때, 이 x를 linear model에 그ㅐ로 활용하는 것이 아니라 이 x로부터 feature를 뽑는 바로 feature extracter Φ를 생각해본다고 하면 이 때 feature extracter는 이 메일의 길이라던가 아니면 특정한 기호를 가지고 있는지 등과 같은 입력 feature를 구성하게 되고 이 입력 feature로 부터 우리가 Φ와의 linear combination을 통해 hypothesis 함수를 구성할 수 있게 된다.

## Example: happiness

 * Predict real valued output <i>y</i> (happiness) from <i>x</i> when <i>D = (x, y)</i> is given

 ![image](https://user-images.githubusercontent.com/122149118/220717294-73255831-b580-40f8-97d3-1c9672430081.png)

 linear regression 문제의 예시를 살펴본다.
 regression은 Supervised Learning의 문제에 포함되기 때문에 사용하는 data sample의 구성은 입력 x와 출력 y의 pair로 구성되게 된다.
 이 때 y는 연속적인 값이 된다.
 예를 들어 왼쪽의 문제는 지금 현재 우리의 연봉 수입에 따른 행복도에 관한 조사 자료이다.
 일반적으로 생각하기에 우리의 수입이 늘어남에 따라서 행복도가 증가할 수 있다고 생각하기 때문에 충분히 선형 model을 사용할 수 있다.
 이 때 입력 변수가 하나이기 때문에 이를 univariate problem이라고 한다.
 하지만 우리의 행복도가 비단 수입에 의해서만 결정된다고 보기는 어려울 것이다.
 그래서 보다 다양한 변수를 생각해볼 수 있다.
 예컨데 우리가 그림에서와 같이 인생의 만족도라던가 아니면 자유, 종교 등과 같은 경우도 충분히 행복에 영향을 줄 수 있는 factor로써 동작할 수 있다.
 다양한 변수들을 regression model에 포함하게 되는 것을 multicariate problem이라고 한다.
 이 두 가지는 서로 다른 문제가 아니라 입력 변수의 갯수에 의해서만 구분을 하게 된 것이다.
 linear regression은 그림에서와 같이 주어진 입력에 대해 출력과의 선형적인 관계를 추론하는 문제이다.

## Linear regression framework

 **Hypothesis function to map from <i>x</i> to <i>y</i>**

 ![image](https://user-images.githubusercontent.com/122149118/220719107-466c10e9-5a97-4458-b3e5-5c956c8ac66c.png)

 training data set에서 주어진 입력과 출력 상으로서 선형 model을 학습을 하게 되고 새로운 입력, 예를 들면 숫자 3이 들어왔을 때 출력 2.71과 같은 output을 산출하는 model이다. 

![image](https://user-images.githubusercontent.com/122149118/220720324-9bd07b00-45d0-461a-863a-b772d14a560b.png)

 이러한 목표를 위해 다음과 같은 과정이 필요하다.
 첫 번째는 어떤 predictor를 이용하게 될 것인가를 결정하는 것이다. 
 이 예시에서는 입력이 하나이기 때문에 Univariate linear model을 사용했다.
 입력 feature가 하나이긴 하지만 우리의 model parameter는 θ<sub>0</sub>, θ<sub>1</sub> 2개 임을 주목해야한다.
 θ<sub>0</sub>는 이 linear model의 offset에 해당하는 parameter라고 생각할 수 있다. 
 linear model에서는 보통 loss를 측정하기 위해 Mean Square Error(MSE)를 활용하게 된다. 
 y는 data set에서의 정답이 되는 것이고 x<sup>(i)</sup>는 model의 출력이 된다.
 그 차이는 model의 출력과 정답과의 오차이다.
 다음은 어떻게 parameter를 구하는지에 관한 내용이다.
 Gradient descent alforithm이라던가 아니면 Nomal equation을 통해 얻게 된다.
 사실 이미 선형 model은 사용하겠다고 한 순간 이미 model 형태가 주어지는 것이고, 앞서 본 바와 같이 θ<sub>0</sub> + θ<sub>1</sub>x에 대한 함수가 되게 되는 것이다. 그리고 loss function 역시 MSE를 사용하니까 주어진 정의와 같이 사용하게 된다.

## Linear regression: parameter opt.

 Idea: choose θ<sub>0</sub>, θ<sub>1</sub> so that <i>h<sub>θ</sub>(x)</i> is close to <i>y</i> using our training set
   <i>h<sub>θ</sub>(x) = θ<sub>0</sub> + θ<sub>1</sub>x</i>

 ![image](https://user-images.githubusercontent.com/122149118/220721928-776cbadd-53db-425f-9626-bba02c239b8a.png)

 주어진 선형 model에서 가장 왼쪽은 기울기(θ<sub>0</sub>)가 0이고 offset(θ<sub>1</sub>)이 1.5인 model이다. 
 각각 linear model이라고 할 수 있다. 
 이렇게 model parameter가 달라짐에 따라서 주어진 data에 fitting하는 과정에서 오차가 발생하게 될 것이다. 

## L<sub>2</sub> cost function (Goal : minimizing MSE)

 ![image](https://user-images.githubusercontent.com/122149118/220723132-7c37f72d-c090-499c-9e83-ab5f8fffe852.png)

 서로 달라진 θ에 따라서 오른쪽과 같은 curve를 그려보면 그림에서 보는 바와 같이 이 loss function은 model parameter에 의한 함수가 되게 된다. 
 즉 θ<sub>0</sub>, θ<sub>1</sub>이 바뀜에 따라 error 함수는 바뀌게 되기 때문에 이러한 error surface 함수를 갖게 되는 것이다. 
 이러한 cost function을 최소화하는 model parameter, θ<sub>0</sub>, θ<sub>1</sub>을 찾는것이 목적이다.
 그 지점은 그림에서 현재 가장 움푹 패인 지점에 존재한다. 
 그에 mapping하는 θ<sub>0</sub>와 θ<sub>1</sub>의 값을 찾도록 하는 것이 바로 linear model이 가장 data에 fitting을 하도록 하는 linear model을 찾게 되는 것이다.

 ![image](https://user-images.githubusercontent.com/122149118/220724966-f33644d3-2f85-4782-94dd-e60873444d77.png)

 ![image](https://user-images.githubusercontent.com/122149118/220725251-4589f42f-c66d-4e7d-80cc-a780021aa539.png)

 예를 들어 가장 초기에 θ<sub>0</sub>와 θ<sub>1</sub>이 왼쪽에서와 같이 주어진다면 오른쪽의 등고선에서 보이는 바와 같이 cost 값이 높게 나타날 것이다.
 그러나 학습이 진행 되면서 점차 θ<sub>0</sub>와 θ<sub>1</sub>이 주어진 data에 fitting이 되면서 등고선에서 낮은 값을 갖도록 바뀌는 것을 볼 수 있다.
 그리고 loss function이 가장 낮은 값을 가질 때 해당 parameter θ<sub>0</sub>와 θ<sub>1</sub>이 가장 최적화된 선형 model을 제공하는 것을 알 수 있다.
 🔍 θ<sub>0</sub>와 θ<sub>1</sub>을 어떻게 구할 수 있는가?
 ➡ 이러한 과정을 parameter 최적화라고 한다. 

## Optimization

 **-Matrix representation in data**

 <i>m</i> samples <i>(x<sup>(i)</sup>,y<sup>(i)</sup>),...,(x<sup>(m)</sup>,y<sup>(m)</sup>) ; **d</i>-dimensional features.**

 ![image](https://user-images.githubusercontent.com/122149118/220728413-b412afea-26b7-4722-8f30-209a78038dcb.png)

 ![image](https://user-images.githubusercontent.com/122149118/220728512-d2c390c9-3c21-4fd0-a897-5ab86928f990.png)

  * Data matrix X ∈ **R**<sup>N X (d + 1)</sup>
     - rows vector : inputs as x<sup>m</sup> ∈ **R**<sup>N X (d + 1)</sup>
  * Target vector y ∈ **R**<sup>N
     - column vectors y<sup>m</sup>
  * Weight vector θ ∈ **R**<sup>(d + 1)</sup>
  * In-sample error is a function of <i>θ</i> and data <i>X, y</i>

     ||<i>y - Xθ</i>||<sub>2</sub>

 다음과 같이 입력 data를 matrix형태로 정리를 해보면 이 때 입력 vector x는 d-demensional한 vector라고 생각했기 때문에 x<sub>1</sub>부터 x<sub>d</sub>까지로 표현이 되어있지만 offset(θ<sub>0</sub>)를 포함하기 위해서 앞에 1을 넣어주어야 한다.
 이러한 개개 vector x는 matrix의 각 low wise하게 쭉 쌓아 matrix x를 구성하게 된다. 
 마찬가지로 linear regression에서의 정답인 y 역시 n-dimensinoal한 vector가 되게 된다. 
 그 이유는 n개의 sample마다 하나씩의 정답이 있기 때문이다. 
 y 역시 위와 같은 형태로 vector를 구성할 수 있게 된다. 
 parameter vector θ는 θ<sub>0</sub>부터 θ<sub>d</sub>까지 있기 때문에 d + 1의 dimensional vector이다. 
 θ와 x의 선형 결합을 통해 score를 식을 통해 계산할 수 있다.
 이와 같이 θ와 x를 곱한 값을 score이라고 한다. 이 score 값과 y의 차이를 통해 loss를 통한 matrix 형태를 계산할 수 있다. 
 즉, model의 출력과 y의 차이를 제곱하고 평균하여 계산한다.
 최적화된 파라미터 θ는 cost function을 가장 최소화 하는 것이다. 

 **- Getting a solution θ**

 * θ<sup>*</sup>: the solution to linear regression
    - Derived by minimizing <i>E<sub>θ</sub></i> over all possible <i>θ ∈ **R<sup>d + 1</sup></i>

 ![image](https://user-images.githubusercontent.com/122149118/220732692-228cc2fd-2531-497e-8cf5-f5155a9586e4.png)

 ![image](https://user-images.githubusercontent.com/122149118/220732819-cb1b0b9c-fcbe-4d3c-91a7-571b25664653.png)

 * <i>E</i> is continous,differentiable and convex
 * General optimization techniques
 * Gradient descent

 이 식을 간단히 풀어서 설명하면 먼저 최적화된 parameter θ인 loss function, E(θ)를 가장 최소화하도록 하는 model parameter이다. 

## Normal equation (Least Square)

 **- Analytic solution of θ**

 * θ<sup>*</sup>: the solution to linear regression
    - Derived by minimizing <i>E<sub>θ</sub></i> over all possible <i>θ ∈ **R<sup>d + 1</sup></i>

 ![image](https://user-images.githubusercontent.com/122149118/220733589-b10a116e-a5a0-42e8-b621-1ec519dfeb43.png)

 <i>E</i> is continuous, differentiable and convex

 앞에서 수식을 풀기 위해 θ에 관한 derivative term을 구한 뒤에 0이 되도록하는 θ의 값을 구하면 최적화된 parameter 값을 구할 수 있다.
 이러한 과정은 Least Square Problem이라고 하며 이러한 방정식을 Normal Equqtion이라고 한다.
 물론 이때 loss function이 differentiable하고 convex해야한다는 가정이 있다.

 ![image](https://user-images.githubusercontent.com/122149118/220734294-edd0e725-fb73-40cc-91b9-1f65be2bf249.png)

 θ에 대해서 미분을 수행하게 되므로 첫 번째 식에서는 θ와 무관한 term들이 모두 0으로 바뀌게 된다.
 그리고 θ에 대해서 나머지 식들은 위와 같은 전개 과정을 거쳐 유도가 된다.
 중간 과정은 관계식을 사용하여 유도되었다.
 결론적으로 위의 수식을 풀어 최종 해를 구하게 되면 다음과 같이 최적 파라미터 θ를 구할 수 있게 된다.

 ![image](https://user-images.githubusercontent.com/122149118/220734901-3078d78b-6cdb-470e-a1f3-99a775237ef1.png)

 : one-step solution via matrix inversion and multiplications

 ![image](https://user-images.githubusercontent.com/122149118/220735062-23c9a466-f402-4651-b26c-8d88a21c54d6.png)

 이 때 X Transpose X의 곱에 inverse를 취하게 되고 거기에 X Transpose matrix를 곱한 형태를 <sup>+</sup>라고 한다.
 이러한 방식의 장점은 one-step으로 해를 구할 수 있다는 것이다. 
 즉 앞서 구성한 matrix에 y를 곱하게 되면 한 번에 분석적으로 하나의 step을 통해 구할 수 있다.
 하지만 이렇게 normal equation을 이용하여 해를 구하는 것은 최근의 Machine Learning 문제에서 Data의 sample 숫자가 늘어나는 경우에 대단히 비효율적인 문제를 야기한다. 

 **In practice**
 * What if the dimension of the input vector hugely increases (huge computational complexity)?
 * What if the matrix is not inverible (redundant features ; linearly dependent)?
 → Needs ierative algorithm (gradient descent)

 data sample의 숫자가 늘어나게 되면 즉, n이 늘어나게 되면 이 matrix의 dimension이 증가하게 되고 그에 따라 matrix inverse 등의 연산을 수행하는데 대단히 큰 복잡도가 소요되기 때문이다.
 더욱이 matrix의 inverse가 존재하지 않는 경우도 있을 수 있다. 
 이런 경우에 어떻게 해를 구할 것인가?
 이러한 문제를 해결하기 위해 gradient descent방식을 적용한다. 
 Gradient descent는 iterative하게 최적 parameter θ를 찾아가는 과정이다.
 Gradient라는 것은 함수를 미분하여 얻는 term으로 해당 함수의 변화하는 정도를 표현하는 값이다.

## Iterative optimization by gradient descent

 * Gradient the derivative of vector functions
    - Direction of greatest increase (or decrease) of a function
    - Zero at (local) max/min

 Iteratively set the gradient to zero instead of analysically setting it to zero
 Gradient descent: a very general algorithm
   * Can train many other models with error measures

 ![image](https://user-images.githubusercontent.com/122149118/220738961-64945b57-3d8e-4508-94a4-f1e4eb720cf9.png)

 **Two things to decide:**
   * Which direction?
   * How much?

 위와 같이 우리가 최소로 하고 싶은 loss function과 그 error surface가 있다.
 임의의 point에서, 이 error surface에서 최소인 point를 찾아가는 것이 목적이라고 할 수 있다. 
 이 error surface에서 값이 최소인 point의 특징은 gradient가 0인 지점이다. 
 gradient descent에서는 gradient가 0인 지점까지 반복적으로 θ를 바꾸어 나가면서 탐색을 하게 된다.
 예를 들어 산등성 어딘가에 존재할 때, 산에서 아래 지점까지 내려온다면 다음 발걸음은 바로 해당 지점에서 하산하는 기울기가 가장 큰 방향으로 한 걸음 딛는 것이 효과적인 전략이 될 것이다.
 Gradient descent도 마찬가지로 Gradient, 즉 함수의 변화도가 가장 큰 방향으로 이동해나가며 반복하게 된다.

## Gradient descent algorithm

 **Method to solve numerically**

 ![image](https://user-images.githubusercontent.com/122149118/220740453-81fc4dc6-31ff-4895-a519-c1a03ea89a73.png)
 ###### α : Learning rate

 **Two things to decide:**
   * Which direction?
      Steepest gradient descent with a greedy method
   * How much?
    Step size

 ![image](https://user-images.githubusercontent.com/122149118/220740910-6d7fdad9-ad0e-4fc7-99d9-a0ace9c24a9b.png)

 ![image](https://user-images.githubusercontent.com/122149118/220740976-8e250d84-a1b1-460b-ac8c-1f05b275a1d5.png)

 gradient descent에서 parameter를 update하는 과정을 수식으로 나타내었다.
 θ<sub>new</sub>는 새로운 parameter를 의미한다. 
 θ<sub>old</sub>는 update 이전의 parameter를 의미한다. 
 새로운 parameter를 update하기 위해 추가하는 term이 gradient descent에서 이야기하게 되는 gradient 값이 된다. 
 이 α의 값을 step size라고 이야기 한다.
 바로 parameter를 update하는데 속도를 조절하는데 사용된다. 
 만약 α가 너무 작게되면 수렴하는데 너무 많은 시간이 걸리게 될 것이다. 
 반대로 α가 너무 크게되면 gradient가 0인 곳을 놓치기가 쉬워 수렴하기가 어려워진다.
 gradient descent algorithm의 특징 중 하나는 greedy method라고 하는 것이다.
 현재 지점에서 가장 변화도가 가파른 방향으로 update해 나가기 때문이다.
 이러한 Greedy algorithm의 특성으로 Gradient descent alogorithm은 경우에 따라 Local optimum만을 달성하기 쉽다. 

## Illustration: Error surface

 <i>E<sub>train</sub>(θ)</i> in high-dimensional space

 ![image](https://user-images.githubusercontent.com/122149118/220742633-43c00337-2070-4192-b54d-a4c5000634ea.png)

 Idea: get a step into the direction the stepest gradient descent
 Property: local optimum, and the result depens on an initial position.

 🔍 Global optimum과 Local optimum의 차이점은 무엇인가?
 ➡ Global optimum은 전체 error surface에서 가장 최소인 값을 갖는 지점이다.
 반대로 Local optimum은 지역적으로 최소이지만 전체 영역을 놓고 보았을 때는 최소가 아닐 수 있는 지점이다.
 그림에서와 같이 서로 다른 두 시작점에서 Gradient descent를 수행하는 경우 어느 하나는 global optimum을, 어느 하나는 local optimum을 달성하여 전혀 다른 값을 갖게 될 수 있다.
 현재 지점에서 가장 가파른 변화도를 보고 움직이기 때문에 이동하는 방향이 전체에서 최적화된 point가 아니게 될 가능성이 있다. 

 **Outline:** The function <i><u>J</i> is the objective function</u> that we want to optimize.
 <u>α: the step size to control</u> the rate to move down the error surface.
 It is a hyper parameter, which is a positive number (c.f. <i>θ</i> is a learnable parameter)
 * Start with initial parameters <i>θ<sub>0</sub>, θ<sub>1</sub></i>
 * Keep changing the parameters to reduce / until achieving the minimal cost
 ![image](https://user-images.githubusercontent.com/122149118/220744565-a441a19c-1d60-44e2-a7b8-5441d1b66b25.png)
 우선 최적화하고자 하는 loss function 또는 objective function을 세운다. 
 그리고 나서 사전에 α와 같은 algorithm 수행에 필요한 parameter를 설정한다.
 이렇게 사전에 설정하게 된 parameter를 hyper parameter라고 한다.
 이 hyper parameter는 항상 양의 값을 갖게 된다.
 반면에 gradient descent algorithm을 통해 학습을 하고자 하는 θ를 leanable parameter라고 한다.
 gradient descent algorithm의 시작은 initial한 point θ<sub>0</sub>와 θ<sub>1</sub>을 결정하는 것부터 시작하게 된다.
 이어서 목적 함수가 수렴할 때까지 parameter를 지속적으로 바꾸어 나가는 과정을 반복하게 된다.

## Gradient descent algorithm

 linear regression model에서 parameter update를 보여준다.

 **for linear regression**

 **Linear regression model**

 ![image](https://user-images.githubusercontent.com/122149118/220748052-8106cc87-f7eb-4595-8ff3-23bb15b3cbf2.png)

 **↓**{: .center}

 **Gradient descent algorithm**

 ![image](https://user-images.githubusercontent.com/122149118/220748328-58b8a2c4-b1ea-41e1-a17f-c0007c3ecd32.png)

 ![image](https://user-images.githubusercontent.com/122149118/220748416-4f49b8f3-5a8b-49d4-90ac-4ee50ae51d5d.png)

 gradient descent에 loss function의 partial derivate term을 구한 것이다.
 목적 함수 J를 θ<sub>J</sub>에 대해서 미분하고 각각의 2개의 parameter를 위해 j = 0인 경우, j = 1인 경우에 대해 각각 θ<sub>0</sub>와 θ<sub>1</sub>에 대해서 미분을 하게 되면 최종적인 식을 얻을 수 있다.
 이러한 과정은 다음과 같은 형식으로 converge할 때까지 θ<sub>0</sub>와 θ<sub>1</sub>을 update를 해 나아간다.

 ![image](https://user-images.githubusercontent.com/122149118/220749203-3b4e6333-9b14-4c4c-9e0f-ff17303b1ba0.png)

 이 때 linear regression의 각 partial derivate term을 구하게 되면 위와 같은 형태를 얻게된다.
 이 식은 각각의 고유한 의미를 가지고 있다.
 먼저 h<sub>θ</sub>(x<sup>(i)</sup>)와 y<sup>(i)</sup>의 차이는 error term임을 나타낸다. 
 즉, θ<sub>0</sub>와 같은 경우는 sample들의 모든 error를 측정한 다음에 더하게 되고 그 다음 hyper parameter α를 곱하여 update를 진행해 나아가게 된다.
 반대로 θ<sub>1</sub>의 경우에 error term 이외에도 sample을 곱하여 모두 더한 다음 update를 진행한다.

## Gradient descent algorithm Vs Normal equation

 **<u>Gradient Descent</u>**
 * needs a number of iterations.
 * works well even when <i>n</i> is large
 * all examples (batch) are examined at each iteration
    * Use stochastic gradient descent(SGD) or mini batch
 * Several advances such as AdaGrad, RMSProp, Adam for optimization

 ![image](https://user-images.githubusercontent.com/122149118/220750841-03913b78-4788-436f-b758-679d5ee86352.png)

 **<u>Normal Equation</u>**
 * Need to compute an inverse matrics and slow if the number of samples is very large
 <i>(X<sup>T</sup>X)<sup>-1</sup></i>
 Gradient descent와 normal equation 사이에는 차이가 있다.
 그 차이는 Gradient descent 방식과 같은 경우는 여러 번의 반복적인 과정을 수행을 통해 해를 얻어 나간다고 하는 것이다. 
 반면 normal equation 같은 경우 한 번에, 1 step으로 해를 구한다.
 하지만 그 해를 구하기 위해서는 위와 같은 matrix를 곱해야 하기 때문에 sample size가 굉장히 늘어나게 되는 경우에 inverse matrix 등을 구하기가 굉장히 어려워지는 문제가 발생하게 된다.
 Gradient descent algorithm과 같은 경우는 설령 n이 굉장히 크더라도 반복적으로 해를 구해 나갈 수 있다. 
 약간의 불편함이 존재하지만 그러한 문제점들을 해결하기 위해 stochastic gradient descent 또는 minibatch algorithm이 있다.
 이러한 방법들을 통해 gradient descent algorithm이 Local minima에 빠지지 않도록 하는 여러가지 효과적인 방법이 있다.

## Quiz

 **What answers are correct?**

 **A.** In linear regression, the solution is interpretable with input features
 linear regression에서 solution이 해석 가능하다.
 **A Correct.** The score is computed as a linear combination of input features and weights; the weight explains the importance of an input feature to the final output
 score가 계산되어 입력 feature와 weight의 combination으로 활용이 될 때 그에 해당하는 weight의 값을 통해서 현재 score가 전체 output에 어떤 영향을 주는지 결정할 수 있다.
 **B.** In linear regression, a hypothesis is not necessarily to be a linear form of leanable parameters
 linear regression이 hypothesis가 learnable parameter θ에 대해서 반드시 linear한 형태일 필요는 없다.
 **B False.** Linear regression model may not be a linear form of a raw data but it should be a linear form of parameters
 learnable parameter가 linear한 형태로 맞춰져야 한다.

## Summary

 * Linear regression model

 **Y** = real number
 <i>h(x) = θ<sup>T</sup>Φ(x)
 e = (y - h(x))<sup>2</sup></i>

 ![image](https://user-images.githubusercontent.com/122149118/220753556-75728259-25c4-437e-aeb1-9abd16f4b94d.png)

  * Can be readily solved using gradient descent
  * Interpretable and lightweight; worth to try first

 Linear regression model에 대해서는 Supervised Learning이기 때문에 입력 feature x와 출력 data y의 1쌍으로 학습이 진행되는데 이 때 y의 값은 실수, 연속적인 값으로 구성되며 model은 output h(x)와 정답인 y의 그 차이로 하여금 그 model error를 가장 최소화 하게끔 학습이 진행 되게 된다. 
 Model optimization을 위해서는 normal equation을 이용해서 parameter를 구할 수도 있지만 보통 최근 Machine Learning 문제에서 Gradient descent algorithm을 통해 iterative하게 그 solution을 찾아나가게 된다.
 이러한 해는 interpretable하고 간편하기 때문에 이 model은 문제를 가장 처음으로 접근 하기에 대단히 좋은 방식이라고 생각할 수 있다.