---
layout: post
title: "Linear Classification"
description: "ì´í™”ì—¬ìëŒ€í•™êµ ê°•ì œì› êµìˆ˜ë‹˜(Electronic & Electrical Engineering)"
date: 2023-02-24
tags: study
comments: true
---

Supervised Learningì˜ í•œ ì¢…ë¥˜ì¸ Linear Classificationì— ëŒ€í•´ ì‚´í´ë³¸ë‹¤.
classificationì€ modelì˜ ì¶œë ¥ì´ disvreteí•œ ê°’ì„ ê°€ì§€ê²Œ ëœë‹¤.
ë¬¸ì œì—ì„œ ì…ë ¥ì˜ categoryë¥¼ ê²°ì •í•˜ê³  ë¶„ë¥˜í•˜ê¸° ìœ„í•´ì„œ data setì—ì„œ ì…ë ¥ê³¼ ì •ë‹µìœ¼ë¡œ êµ¬ì„±ëœ, ì¦‰ labelì´ ìˆëŠ” data setì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤.

---

# ì§€ë„ í•™ìŠµ

## Linear classification

 * Predict a discrete output <i>y</i> (classification ID) from <i>x</i> when <i>D = (x, y)</i> is given
    - ID = 0 or 1 (binary classification)
    - ID = 0, 1, ..., N - 1(multi classification)

 * Hypothesis set Î— : a set of lines
 > <i>h<sub>w</sub>x = w<sub>0</sub> + w<sub>1</sub>x<sub>1</sub> + ... + w<sub>d</sub>x<sub>d</sub> = w<sup>T</sup>x</i>
 ###### <i>w</i> : model parameter (learnable parameter)
 > <i>h<sub>w</sub>x = w<sub>0</sub> + w<sub>1</sub>Î¦(x<sub>1</sub>) + ... + w<sub>d</sub>Î¦(x<sub>d</sub>) = w<sup>T</sup>Î¦(x)</i>
 ###### Linear model witha set of features

 **Hyper plane(decision boundary)**

 ![image](https://user-images.githubusercontent.com/122149118/221138126-25043da0-a7c6-43fd-87c4-982edf013600.png)

 Linear classification ì—­ì‹œ linear regression modelê³¼ ê°™ì´ ìœ„ ìˆ˜ì‹ê³¼ ê°™ì€ linear modelì„ ì‚¬ìš©í•œë‹¤.
 linear modelì´ë€ hypothsis í•¨ìˆ˜ Hê°€ ì…ë ¥ featureì™€ ê·¸ì— í•´ë‹¹í•˜ëŠ” parameter setì— linear combinationìœ¼ë¡œ êµ¬ì„±ì´ ë˜ëŠ” modelì„ ì˜ë¯¸í•œë‹¤.
 ì´ ì‹ì—ì„œ wëŠ” model parameterë¥¼ ì˜ë¯¸í•˜ê³  xëŠ” ì…ë ¥ featureë¥¼ ì˜ë¯¸í•œë‹¤.
 linear regressionì—ì„œì™€ ê°™ì´ ì…ë ¥ featureì™€ Î¸ë¥¼ d + 1, dimensionalí•œ vectorë¡œ êµ¬ì„±í•˜ê²Œ ëœë‹¤.
 ì´ë ‡ê²Œ êµ¬ì„±í•œ linear modelê°™ì€ ê²½ìš° ì…ë ¥ featureì˜ coordinateì—ì„œ hyper planeìœ¼ë¡œ ì‚¬ìš©í•˜ê²Œ ëœë‹¤.
 ìœ„ì˜ ê·¸ë¦¼ì—ì„œ ì ì„ ìœ¼ë¡œ í‘œì‹œí•œ ì§ì„ ì´ í˜„ì¬ 2ì°¨ì› coordinateë¥¼ êµ¬ë¶„í•˜ê²Œ ë˜ëŠ” Hyper planeì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.
 ì´ ê·¸ë¦¼ì—ì„œ positive sampleì€ í•¨ìˆ˜ h(x)ì— positive sampleì„ ì…ë ¥í•˜ì˜€ì„ ë•Œ ì´ ê°’ì´ 0ë³´ë‹¤ í¬ê²Œë˜ë©´ positive sampleì´ë¼ê³  ê²°ì •í•  ìˆ˜ ìˆë‹¤.(h(x) > 0)
 ë°˜ëŒ€ë¡œ negative sample ì—­ì‹œ h(x)ì—ë‹¤ê°€ ì…ë ¥ì„ ë„£ì—ˆì„ ë•Œ ì´ ê°’ì´ 0ë³´ë‹¤ ì‘ë‹¤ë©´ negative sampleë¡œ êµ¬ì„±í•  ìˆ˜ ìˆë‹¤.(h(x) < 0)
 ì´ëŸ¬í•œ hyper planeì€ ì–´ë– í•œ ë°©ì •ì‹ì„ ê°€ì§€ê²Œ ë˜ëŠ”ê°€?
 ì´ ì‹ì€ h(x)ê°€ 0ì¸ ì‹ì´ ë  ê²ƒì´ë‹¤.(h(x) = 0)
 ìš°ë¦¬ì˜ ëª©ì ì€ ì´ëŸ¬í•œ hyper planeì„ êµ¬í•´ data setì— ìˆëŠ” positive sampleê³¼ negative sampleì„ linear combinationì— ì˜í•´ì„œ êµ¬ë¶„ì„ í•˜ëŠ” ê²ƒì´ ëª©ì ì´ ëœë‹¤.
 linear regressionì—ì„œì™€ ê°™ì´ linear modelì´ ê°–ëŠ” ì—¬ëŸ¬ ì¥ì , ì¦‰ ë‹¨ìˆœí•˜ë©° í•´ì„ ê°€ëŠ¥ì„±ì´ ìˆê³  ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ì œê³µí•  ìˆ˜ ìˆê²Œ ëœë‹¤.

## Example: image recognition

 ![image](https://user-images.githubusercontent.com/122149118/221142228-8761775d-d686-44f5-ad73-cf614cbaaaaf.png)

 multiclass classificationì˜ ë¬¸ì œë¥¼ ë³´ì¸ë‹¤.
 Binary classificationê³¼ ë¹„ìŠ·í•˜ì§€ë§Œ ì•½ê°„ ë‹¤ë¥¸ ì ì€ ì…ë ¥ ì‹ í˜¸ ê³µê°„ì—ì„œ hyper planeì´ ë‹¤ìˆ˜ ì¡´ì¬í•˜ì—¬ classificationì„ ìˆ˜í–‰í•œë‹¤ëŠ” ê²ƒì´ë‹¤.

## Problem formulation

 * X = R<sup>d</sup> is an input space
    - R<sup>d</sup> : a <i>d</i>-dimensional Eucildean space
    - input vector <i>x</i> âˆˆ <i>X</i>: <i>x</i> = (x<sub>1</sub>, x<sub>2</sub>,..., x<sub>d</sub>), e.g. <i>d</i> = 2
 * Y = {+1, -1} is an output space
    - Binary (yes/no) decision
 * Now, we want to approximate a target function <i>f</i>
    - <i>f</i> : <i>X â†’ Y</i> Unknown ideal function)
    - Data (x<sup>1</sup>, y<sup>1</sup>), ..., (x<sup>N</sup>, y<sup>N</sup>) ; dataset where y<sup>N</sup> = <i>f(x<sup>N</sup>)</i>
    - Correct label is ready for a training set
    - <u>**Hypothesis**</u> <i>h: **X â†’ Y**</i> (ML model to approximate <i>f</i>) : <i>h âˆˆ **Î—**</i>

 ì•ì—ì„œ ì–¸ê¸‰í•œ ë¬¸ì œë¥¼ ì¡°ê¸ˆ ë” ì •í˜•í™”í•˜ê³  ì¼ë°˜í™”í•œë‹¤ë©´ ìœ„ì™€ ê°™ì´ í‘œí˜„ ê°€ëŠ¥í•˜ë‹¤.
 ì•ì˜ ë¬¸ì œì—ì„œëŠ” ì˜ˆì‹œë¡œ 2ì°¨ì› ê³µê°„ì—ì„œì˜ ë¬¸ì œë¥¼ ì •ì˜í–ˆì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ dì°¨ì›ì˜ ê³µê°„ì— ì…ë ¥ feature vectorê°€ ìˆë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.
 ì´ ë•Œ ì¶œë ¥ì€ binary classificationì˜ ë¬¸ì œì¸ ê²½ìš° 0ì•„ë‹ˆë©´1, ì¦‰ yes or no ë¬¸ì œë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.
 ì•ì„œ ì„¤ëª…í•œ ë°”ì™€ ê°™ì´ target function fë¥¼ approximationí•˜ëŠ” hypothesis hë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ ëª©ì ì´ë‹¤.

## Linear classification framework

 **Hypothesis function to build a decision boundary**

 ![image](https://user-images.githubusercontent.com/122149118/221147384-09957081-e5d6-43f7-afde-417d6956e2ff.png)

 linear classificationì€ ê·¸ë¦¼ì—ì„œì™€ ê°™ì´ decision boundaryì˜ ì •í™•í•œ ì¶”ì •ì„ í†µí•´ì„œ ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•´ ì¶œë ¥ì˜ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë¬¸ì œê°€ ëœë‹¤.
 training data setì—ì„œ ì£¼ì–´ì§„ ì…ë ¥ê³¼ ì¶œë ¥ìƒìœ¼ë¡œ ì„ í˜•ì˜ boundaryë¥¼ í•™ìŠµí•˜ê²Œ ë˜ê³  ê·¸ë¡œë¶€í„° ì…ë ¥ featureê°€ hyper planeìœ¼ë¡œë¶€í„° ì–´ëŠ ìœ„ì¹˜ì— ìˆëŠ”ì§€ë¥¼ ê²°ì •í•˜ê²Œ ëœë‹¤.
 ì˜ˆë¥¼ ë“¤ì–´ ìƒˆë¡œìš´ ì…ë ¥ (2, 0)ì´ ë“¤ì–´ì™”ì„ ë•Œ ì´ pointëŠ” h(x)ê°€ 0ë³´ë‹¤ ì‘ì€ ê³³, ì¦‰ -1ì„ ì¶œë ¥í•˜ê¸°ë¥¼ ê¸°ëŒ€í•œë‹¤.
 ì´ëŸ¬í•œ ê´€ê³„ì‹ì€ (2, 0)ì„ ë„£ìœ¼ë©´ -1ì˜ labelì´ ë‚˜ì˜¤ë„ë¡ classifier hë¥¼ í•™ìŠµì„ í•¨ìœ¼ë¡œì¨ ê¸°ëŒ€í•  ìˆ˜ ìˆë‹¤.
 ì´ëŸ¬í•œ ëª©í‘œë¥¼ ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.
 ![image](https://user-images.githubusercontent.com/122149118/221149071-1fb1021a-e31b-496b-b2b8-ed6a20612f5b.png)
 ì–´ë–¤ predictorë¥¼ ì´ìš©í•  ê²ƒì¸ê°€ë¥¼ ê²°ì •í•˜ê³  model parameterë¥¼ fittingí•˜ê¸° ìœ„í•œ loss functionì€ ì–´ë–»ê²Œ ì„¤ì •í•˜ëŠ”ê°€ì´ë‹¤.
 linear regression ë¬¸ì œì—ì„œëŠ” MSEë¥¼ ì‚¬ìš©í•œë‹¤.
 í•˜ì§€ë§Œ classificationì„ ì‚¬ìš©í•˜ëŠ” lossëŠ” Zero-one loss, Hinge loss, Cross-entropy lossì™€ ê°™ì€ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ loss functionì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤.
 ë‹¤ìŒìœ¼ë¡œ ì–´ë–»ê²Œ parameterë¥¼ ìµœì í™”í•˜ëŠ”ì§€ ì—­ì‹œ ê³ ë ¤í•´ì•¼í•œë‹¤.
 gradient descent algorithmì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤.
 ì•ì„  ì˜ˆì‹œë¥¼ ìˆ˜ì‹ì„ í†µí•´ ì¡°ê¸ˆ ë” ì •í™•í•˜ê²Œ í‘œí˜„í•œë‹¤.

## Linear classification model

 * The linear formula <i>g âˆˆ **H**</i> can be wrtten as

 ![image](https://user-images.githubusercontent.com/122149118/221150503-128556e4-dcba-4ae1-b990-953e37cfb95a.png)
 ###### <i>w<sub>0</sub> : a bias term</i>
 ###### <i>x<sub>0</sub></i> : 1

 ![image](https://user-images.githubusercontent.com/122149118/221150715-079a17c9-9a2e-4735-9d38-b6ead9c7425a.png)

 <i>h(x) = w<sub>0</sub> + w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> = 0</i>

 ![image](https://user-images.githubusercontent.com/122149118/221151069-5846544b-70ab-4e8a-8d33-9fb4cdae379b.png)

 ì…ë ¥ ë³€ìˆ˜ì™€ parameterì˜ ê³±ìœ¼ë¡œ ë¨¼ì € scoreë¥¼ ê³„ì‚°í•œë‹¤.
 ì´ ë•Œ w<sub>0</sub>ëŠ” bias ë˜ëŠ” offsetê°’ì´ë‹¤. 
 ë§ˆì°¬ê°€ì§€ë¡œ ì…ë ¥ featureì™€ parameterëŠ” d + 1 dimensionalì˜ vectorê°€ ë˜ê²Œ ëœë‹¤.
 score ê°’ì„ ê³„ì‚°ì„ í•œ ì´í›„ì—ëŠ” ê·¸ ì¶œë ¥ì— sine í•¨ìˆ˜ë¥¼ ì ìš©í•˜ê²Œ ëœë‹¤.
 sine í•¨ìˆ˜ëŠ” ë‚´ë¶€ì˜ ìˆëŠ” ê°’ì´ ìŒì´ë©´ -1, 0ì´ê±°ë‚˜ ì–‘ì´ë©´ 1ì„ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤.
 ê·¸ë¦¼ì—ì„œëŠ” ì˜ˆì‹œë¥¼ ë³´ì—¬ì¤€ë‹¤.
 h(x)ëŠ” 1ì°¨ ì„ í˜• í•¨ìˆ˜ë¡œ êµ¬ì„±ëœ hypothesisì´ê³  ê·¸ ê¸°ì¤€ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì™¼ìª½ì€ h(x)ê°€ 0ë³´ë‹¤ í° positive sampleì„ ì •ì˜í•˜ê²Œ ë˜ê³  ì˜¤ë¥¸ìª½ì€ h(x)ê°€ 0ë³´ë‹¤ ì‘ì€ negative smapleì„ ì •ì˜í•˜ê²Œ ëœë‹¤.

## Example of linear classifier

 ![image](https://user-images.githubusercontent.com/122149118/221152098-b2c6bc04-1812-490a-be0d-229fd22bb186.png)

 hyper planeì˜ parameter w<sub>1</sub>ê³¼ w<sub>2</sub>ì´ ê°ê° -1ê³¼ 1ì´ë¼ê³  ìƒê°í•˜ë©´, ì´ ë•Œ 0ê³¼ 2ì˜ ìƒˆë¡œìš´ ì…ë ¥ ê°’ì´ ë“¤ì–´ì˜¬ ê²½ìš°ì— ì´ modelì€ ì–´ë–»ê²Œ sampleì„ íŒë³„í•  ê²ƒì¸ê°€?
 ë°”ë¡œ [-1, 1], [0, 2]ë¥¼ ë‚´ì ì„ í•œ ê°’ì„ ê³„ì‚°í•˜ê²Œ ë˜ë©´ ì´ í•¨ìˆ˜ì— ìˆëŠ” ë‚´ë¶€ì˜ ê°’ì€ 2ê°€ ëœë‹¤.
 ì—¬ê¸°ì— sine í•¨ìˆ˜ë¥¼ ì·¨í•˜ê²Œ ë˜ë©´ ì´ ê°’ì€ 1ì„ ì¶œë ¥í•˜ê²Œ ëœë‹¤.
 ê·¸ë˜ì„œ ì´ hypothsisì— ì˜í•´ì„œ ì´ decision boundaryì—ì„œ modelì€ ì´ ì¢Œí‘œë¥¼ positive sampleë¡œ ë¶„ë¥˜ë¡œ í•œ ê²°ê³¼ê°€ ëœë‹¤.
 ë°˜ëŒ€ë¡œ, ê°™ì€ modelì„ í™œìš©í•˜ê¸° ë•Œë¬¸ì— decision boundaryê°€ [-1, 1]ì¸ hyper planeì„ ì´ìš©í•´ [1, -1]ì„ ë¶„ë¥˜í•´ë³´ë©´ [1, -1]ê°™ì€ ê²½ìš° ì§€ê¸ˆ í˜„ì¬ í‘¸ë¥¸ìƒ‰ì˜ pointë¡œ ê²°ì •ë˜ì–´ìˆê³  ì´ ë•Œ model, ì¦‰ model parameterê°€ [-1, 1]ì„ ê°–ê²Œ ë˜ëŠ” planeê³¼ ë¹„êµí–ˆì„ ë•Œ ê·¸ ê²°ê³¼ê°’ì´ -1ë¡œ negative sampleë¡œ ì˜ ë¶„ë¥˜ë˜ì—ˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.
 ì´ í•¨ìˆ˜ì— ë“¤ì–´ê°€ëŠ” ì…ë ¥ score, ì¦‰ w<sup>T</sup>xì˜ ê°’ì€ ì´ score ê°’ì´ ì»¤ì§€ê²Œ ë˜ë©´ ì´ h(x)ë¡œ ë¶€í„° í•´ë‹¹í•˜ëŠ” ì¢Œí‘œê¹Œì§€ì˜ ê±°ë¦¬ê°€ ì»¤ì§€ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
 ê° ì ì—ì„œ h(x)ì˜ projectionì„ ìˆ˜í–‰í–ˆì„ ë•Œ ë‘ ì ì€ ê°™ì€ ê±°ë¦¬ì— ìˆìŒì„ ì‚´í´ë³¼ ìˆ˜ ìˆë‹¤.
 graphì—ì„œëŠ” ì˜ˆì»¨ë° í‘¸ë¥¸ìƒ‰ì˜ ì ì„ í˜„ì¬ hyper planeì— projectionì„ í•œ ê³³ê³¼ì˜ ê±°ë¦¬ì™€ ì§€ê¸ˆ ë…¸ë€ìƒ‰ì— ìˆëŠ” pointë¥¼ projectioní–ˆì„ ë•Œ hyper planeê¹Œì§€ ë‹¿ëŠ” ê±°ë¦¬ê°€ ê°™ì€ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
 ë°”ë¡œ score ê°’ì´ ë™ì¼í•˜ê¸° ë•Œë¬¸ì´ë‹¤.
 
 ì›ë˜ ë‹¤ì‹œ í’€ê³ ìí•˜ëŠ” ë¬¸ì œë¡œ ëŒì•„ì˜¤ë©´ ë¬¸ì œëŠ” parameter wë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤.
 wê°€ ë°”ë€œì— ë”°ë¼ sampleë“¤ì˜ íŒë³„ ê²°ê³¼ê°€ ë‹¹ì—°íˆ ë°”ë€” ê²ƒì´ë‹¤.

## Hypothesis class : which classifier?

 <i>h(x) = sign([-1, 1][x<sub>1</sub>, x<sub>2</sub>]<sup>T</sup>)</i>
 <i>h(x) = sign([0.5, 1][x<sub>1</sub>, x<sub>2</sub>]<sup>T</sup>)</i>

 <i>h(x) = w<sub>0</sub> + w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> = 0</i>

 ![image](https://user-images.githubusercontent.com/122149118/221157458-b8c317f0-bd65-40fd-87a5-df448981f173.png)

 ì˜ˆë¥¼ ë“¤ì–´ w<sub>0</sub>ì™€ w<sub>1</sub>ì´ [-1, 1]ì´ë©´ sampleì„ ë°”ë¡œ íŒë³„í•˜ê² ì§€ë§Œ [0.5, 1]ì´ë©´ í˜„ì¬ ê·¸ë¦¼ì—ì„œ ë³´ë¼ìƒ‰ ì„ ê³¼ ê°™ì€ hyper planeì´ ê·¸ë ¤ì§€ê²Œ ë˜ê³  ì´ ë•Œì—ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí•˜ê²Œ ëœë‹¤.
 ì¼ë¶€ positive sampleì„ negative sampleë¡œ íŒë³„í•˜ê²Œë˜ëŠ” ê²ƒì´ë‹¤.
 regressionì—ì„œë„ ì´ëŸ¬í•œ model ì˜¤ë¥˜ê°€ parameterì— ì˜í•´ì„œ í•¨ìˆ˜ ê¼´ë¡œ ë‚˜íƒ€ëƒˆë˜ ê²ƒì„ ê¸°ì–µí•´ì•¼ í•œë‹¤.
 ğŸ” Classification ë¬¸ì œì—ì„œëŠ” Errorë¥¼ ì–´ë–»ê²Œ íŒë‹¨í•  ìˆ˜ ìˆëŠ”ê°€?
 â¡ í•˜ë‚˜ì˜ ì˜ˆì‹œë¡œ Zero-One Lossë¥¼ ìƒê°í•  ìˆ˜ ìˆë‹¤.
 ì´ lossëŠ” ë‚´ë¶€ì˜ logicì„ íŒë³„í•˜ì—¬ ë§ìœ¼ë©´ 0 í‹€ë¦¬ë©´ 1ì„ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤.
 
 **For optimization**
 **Define a metric and compute an error**
 
  Loss<sub>0-1</sub> = 1[<i>f<sub>w</sub></i>(x) â‰  <i>y</i>] **zero-one loss**
  Loss([0, 2], 1, [0.5, 1]) = 1[sign([0.5, 1] Â· [0, 2] â‰  1)] = 0
  Loss([-2, 0], 1, [0.5, 1]) = 1[sign([0.5, 1] Â· [-2, 0] â‰  1)] = 1
  Loss([1, -1], 1, [0.5, 1]) = 1[sign([0.5, 1] Â· [1, -1] â‰  1)] = 0

 ìœ„ ìˆ˜ì‹ì€ zero-one lossë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì´ë‹¤.
 1ë¡œ í‘œí˜„í•œ ì´ í•¨ìˆ˜ë¥¼ zero-one loss functionì´ë‹¤.
 ë‚´ë¶€ì— ë“¤ì–´ê°€ëŠ” ê°’ì€ [0.5, 1]ê³¼ [0, 2]ë¥¼ ë‚´ì í–ˆì„ ë•Œ ì´ ë•Œ ê°’ì€ 1ê³¼ ë‹¤ë¥¸ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
 ë”°ë¼ì„œ 1ì€ zero-one lossê°€ ëœë‹¤.
 [0.5, 1]ê³¼ [0, 2]ë¥¼ ë‚´ì ì„ í•œ ê°’ì„ signì„ ì·¨í–ˆì„ ë•Œ ë‚˜íƒ€ë‚˜ê²Œ ë˜ëŠ” ê°’ì€ ë°”ë¡œ 1ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤.
 1 ê³¼ 1ì´ ì„œë¡œ ë‹¤ë¥´ë‹¤ë¼ê³  í•˜ëŠ” logicì„ íŒë³„í•˜ê²Œ ë˜ë©´ Falseì¸ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
 Falseì´ê¸° ë•Œë¬¸ì— ê²°ê³¼ê°’ìœ¼ë¡œ 0ì„ ì¶œë ¥í•œë‹¤.
 ì¦‰ lossê°€ 0ì´ ë˜ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.
 modelì´ classificationì„ ìˆ˜í–‰í•˜ëŠ” ë™ì•ˆì— score ê°’ì„ ì´ìš©í•´ì„œ íŒë³„í•œë‹¤ëŠ” ê²ƒì„ ì•Œì•˜ë‹¤.

## Score and margin

 * Input data : <i>x</i>
 * Predicted label : <i>h(x) = sign(w<sup>T</sup>Î¦(x))</i>
 * Target label : <i>y</i>
 
 > âœ” Score : the score on an example <i>(x, y)</i> is <i>w Â· Î¦(x)</i>, how **confident** we are in prediction + 1
 âœ” Margin : the margin on an example <i>(x, y)</i> is <i>(w Â· Î¦(x))y</i>, how **correct** we are.

 ì´ score ê°’ì€ ê²°ì • ê³¼ì •ì—ì„œ modelì´ ì–¼ë§ˆë‚˜ confidentí•œì§€ë¥¼ ì¸¡ì •í•  ìˆ˜ ìˆë‹¤.
 ì´ì™€ ìœ ì‚¬í•˜ê²Œ marginì´ë¼ê³  í•˜ëŠ” ê²ƒì€ ë‹¤ìŒ ìˆ˜ì‹ì—ì„œì™€ ê°™ì´ scoreì— y ê°’ì„ ê³±í•˜ì—¬ ê³„ì‚°í•˜ê²Œ ëœë‹¤.
 (w Â· Î¦(x))yëŠ” ì•ì— wì™€ Î¦(x)ë¥¼ ê³±í•œ ê²ƒì€ modelì˜ confidenceë¥¼ ì˜ë¯¸í•˜ëŠ” scoreë¼ê³  ì´í•´í•  ìˆ˜ ìˆë‹¤.
 ì´ ë•Œ yë¥¼ ê°™ì´ ê³±í•œë‹¤ê³  í•˜ëŠ” ê²ƒì€ yê°€ ë§Œì•½ 1ì¸ ê²½ìš° ê·¸ë¦¬ê³  score ê°’ì´ positive real ê°’ì¸ ê²½ìš°ì—ëŠ” ì´ margineì´ êµ‰ì¥íˆ ì»¤ì§€ê²Œ ë  ê²ƒì´ë‹¤.
 scoreê°€ +ë¼ê³  í•˜ëŠ” ê²ƒì€ model predictionì´ positive sampleì„ ì˜ë¯¸í•œë‹¤ê³  ì´ì•¼ê¸° í•  ìˆ˜ ìˆë‹¤.
 ì¦‰, yê°€ 1ì¸ ê²½ìš°ëŠ” ì •ë‹µì„ ë§í˜”ë‹¤ê³  ì´ì•¼ê¸°í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.
 ê·¸ ë•Œì—ëŠ” margin ê°’ì´ êµ‰ì¥íˆ ëŠ˜ì–´ë‚˜ê²Œ ëœë‹¤.
 ë˜í•œ ë§ˆì°¬ê°€ì§€ë¡œ score ê°’ì´ -ì´ë©´ì„œ yê°€ ë™ì‹œì— -1ì„ ê°–ê²Œ ë˜ëŠ” ê²ƒ ì—­ì‹œ scoreê°€ -ì´ê¸° ë•Œë¬¸ì— negative sampleë¡œ íŒë³„í•˜ê²Œ ë˜ê³  yê°€ -1ì´ê¸° ë•Œë¬¸ì— ì •ë‹µì„ ë§í˜”ë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.
 ë‘˜ì˜ ê°’ì„ ê³±í•˜ê²Œ ë˜ë©´ ë§ˆì°¬ê°€ì§€ë¡œ ê°•í•œ positive ê°’ì„ ì‚°ì¶œí•˜ê²Œ ëœë‹¤.
 ì´ ì—­ì‹œ margin ê°’ì´ êµ‰ì¥íˆ ì¦ê°€í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.
 í•˜ì§€ë§Œ ë°˜ëŒ€ì˜ ê²½ìš°ë„ ì¡´ì¬í•œë‹¤.
 scoreê°€ ë§Œì•½ì— -ê°’ì„ ê°€ì§€ê²Œ ëê³  yê°€ +1ì„ ê°€ì§€ê²Œ ë˜ëŠ” ê²½ìš°ì´ë‹¤.
 scoreê°€ -ë¼ê³  í•˜ëŠ” ê²ƒì€ modelì´ negative sampleì„ ì˜ˆì¸¡í•˜ì˜€ë‹¤ëŠ” ê²ƒì´ì§€ë§Œ yê°€ 1ì´ë‹¤ë¼ê³  í•˜ëŠ” ê²ƒì€ ì •ë‹µì´ 1ì´ì—ˆë‹¤, ì¦‰ positive sampleì´ì—ˆë‹¤ëŠ” ê²ƒì´ë‹¤.
 ì´ ë‘ ê°œì˜ ê°’ì„ ê³±í•˜ê²Œ ë˜ë©´ marginì´ -ê°’ì„ ê°€ì§€ê²Œ ë˜ê³  model predictionì´ ì‹¤íŒ¨í–ˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.
 ë§ˆì°¬ê°€ì§€ë¡œ scoreê°€ +ì´ë©´ì„œ yê°€ -1ì¸ ê²½ìš°ë„ ìœ ì‚¬í•œ ê²½ìš°ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.
 ì´ëŸ¬í•œ zero-one lossë¥¼ gradient descent algorithmì— ì ìš©í•˜ë ¤ë©´ ì´ loss functionì˜ partial derivative termì„ êµ¬í•´ì•¼ í•œë‹¤.

## Zero-one loss

 ![image](https://user-images.githubusercontent.com/122149118/221162673-100f19eb-0d4d-4073-b364-793d11c62e3b.png)

 ![image](https://user-images.githubusercontent.com/122149118/221162750-e3106277-c091-41ac-b3fc-aa877c6d4649.png)
 Gradient is zero almost everywhere!

 The goal is to minimize the loss
 To run gradient descent, compute the gradient:
 ![image](https://user-images.githubusercontent.com/122149118/221163537-c75d6969-6dd7-4d2f-9617-3d0ade7ab385.png)

 ê·¸ëŸ¬ë‚˜ ì´ í•¨ìˆ˜ì˜ ë¯¸ë¶„ ê²°ê³¼ëŠ” gradientê°€ 0ì´ ëœë‹¤.
 gradientê°€ 0ì´ ë˜ë©´ modelì´ í•™ìŠµì„ í•  ìˆ˜ ì—†ëŠ” ë¬¸ì œê°€ ìƒê¸°ê²Œ ëœë‹¤.
 ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•˜ì—¬ classificationì—ì„œëŠ” hinge lossë¥¼ ì‚¬ìš©í•œë‹¤.

## Hinge loss

 **Loss<sub>hinge</sub>(<i>x, y, z</i>) = max{1 - (w Â· Î¦(x))y, 0}**

 * Zero loss if it is classified confidently and correctly
 * Misclassification incurs a linear penalty w.r.t confidence

 ![image](https://user-images.githubusercontent.com/122149118/221761117-e86c7df3-866a-43a5-8e49-b3af597b1899.png)

  Hinge lossì˜ í•¨ìˆ˜ëŠ” maxê°’ì„ ì·¨í•˜ê²Œ ë˜ëŠ”ë°, 1 - (w Â· Î¦(x))y, 0 ë‘ ê°’ ì¤‘ì— ê°€ì¥ í° ê°’ì„ ê³ ë¥´ê²Œ ëœë‹¤.
 ë§Œì•½ margin ê°’ì´ í¬ë‹¤ê³  í•˜ë©´ ì¦‰, modelì´ ì •ë‹µì„ ì˜ ë§ì¶”ê³  ìˆë‹¤ê³  í•œë‹¤ë©´ 1 - (w Â· Î¦(x))yì˜ ê°’ì€ negative ê°’ì„ ê°€ì§ˆ ê²ƒì´ë‹¤.
 graph ìƒì—ì„œ yì¶•ì´ 0ì¸ ê°’ì„ ë‚˜íƒ€ë‚´ê²Œ ë  ê²ƒì´ë‹¤.
 ì´ ê²½ìš°ì—ëŠ” ê°•í•œ minus ê°’ê³¼ 0ì™€ì˜ maxê°’ì„ ì·¨í•˜ê¸° ë•Œë¬¸ì— ì´ ë•Œ loss ê°’ì€ 0ì„ ê°€ì§„ë‹¤.
 ë°˜ëŒ€ë¡œ (w Â· Î¦(x))yê°’ì´ ìŒì˜ ê°’ì„ ê°–ê²Œ ë˜ì–´ 1 - (w Â· Î¦(x))y ê°’ì´ +ê°’ì´ ëœë‹¤ê³  í•˜ë©´ maxê°’ì€ margin ê°’ì— ì„ í˜•ì ìœ¼ë¡œ ë¹„ë¡€í•˜ì—¬ ì¦ê°€í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.
 ì´ì— ëŒ€í•´ì„œ loss ì—­ì‹œ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•˜ê²Œ ë˜ëŠ” í•¨ìˆ˜ë¥¼ ê°€ì§€ê²Œ ëœë‹¤.

 ![image](https://user-images.githubusercontent.com/122149118/221761199-7f5648be-bd13-408c-8be0-6c3acab310e7.png)

 (w Â· Î¦(x))y ê°’ì´ 1ë³´ë‹¤ ì‘ê²Œ ë˜ë©´ -Î¦(x)y ê°’ì´ ë˜ê³  marginê°’ì´ ì¶©ë¶„íˆ í¬ë‹¤ë©´ gradientê°€ 0ì´ë  ê²ƒì´ë‹¤.

## Cross-entropy loss

 * Considers two probability mass functions (pmf) {p, 1 - p} and {q, 1 - q} with abinary outcomes
 * Cross entropy for these two pmfs : defined by

 cross-entropy lossëŠ” classification modelì„ í•™ìŠµí•  ë•Œ ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ëŒ€í‘œì ì¸ loss functionì´ë‹¤.
 ì´ í•¨ìˆ˜ì˜ ì›í˜•ì€ ì •ë³´ ë‰´ëŸ°ì—ì„œ ë‘ ê°œ ì„œë¡œ ë‹¤ë¥¸ pmfë¥¼ ê°€ì§€ëŠ” í™•ë¥  í•¨ìˆ˜ ì‚¬ì´ì— ê°€ê¹Œìš´ ì •ë„, ë˜ëŠ” ì„œë¡œ ë‹¤ë¥¸ ì •ë„ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•œ ì´ë¥¸ë°” K-L divergenceì—ì„œ í‘œí˜„ì´ ëœë‹¤.

 ![image](https://user-images.githubusercontent.com/122149118/221764400-8887df40-0d7f-449f-b34f-04adb9bdc33f.png)

 ![image](https://user-images.githubusercontent.com/122149118/221763467-ff257f5e-2b9b-418e-85e3-cf17d868976a.png)
 ìœ„ ì‹ì—ì„œ ì™¼ìª½ í•­ì€ relative entropyë¼ê³  í•˜ëŠ” ê°’ìœ¼ë¡œ ì´ ê°’ì€ pì— ëŒ€í•œ entropy,H(p)ì— K-L divergenceê°’ì„ ë”í•´ì„œ ì–»ê²Œ ëœë‹¤.
 ì´ ë•Œ entropy(H(p))ê°’ì€ ë³€í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì™¼ìª½ì— ìˆëŠ” ìˆ˜ì‹ì€ K-L divergenceì— ì˜í•´ì„œ ê°’ì´ ë°”ë€ë‹¤.
 ![image](https://user-images.githubusercontent.com/122149118/221763628-ec45e601-f5e9-491e-b39e-dcd4cb3844b1.png)
 K-L divergence í•¨ìˆ˜ DëŠ” ìœ„ì™€ ê°™ì´ í‘œí˜„í•œë‹¤.
 ì´ ë•Œ K-L divergenceëŠ” ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ pmfì˜ ì°¨ì´ì´ë‹¤.

 * Cross entropy measures the error when approximating an observed pmf {p, 1 - p} between a fitted pmf {q, 1 - q}

 ì¦‰, cross-entropy ì—­ì‹œ ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ pmf pì™€ qê°€ ì„œë¡œ ìœ ì‚¬í•œì§€, ê·¸ë ‡ì§€ ì•Šì€ì§€ ì •ë„ì— ë”°ë¼ì„œ errorì˜ ì •ë„ê°€ ë°”ë€Œê²Œ ëœë‹¤.
 ë§Œì•½ pì™€ qê°€ ì„œë¡œ ìœ ì‚¬í•˜ë‹¤ë©´ ì´ lossëŠ” ì¤„ì–´ë“¤ê²Œ ë  ê²ƒì´ê³ , ì„œë¡œ êµ‰ì¥íˆ ë‹¤ë¥´ë‹¤ë©´ ì´ lossëŠ” ì»¤ì§ˆ ê²ƒì´ë‹¤.

 ë¬¸ì œëŠ” ì§€ê¸ˆê¹Œì§€ ê³„ì‚°í•œ modelì˜ score ê°’ì€ ì‹¤ìˆ˜ ê°’ì´ë¼ëŠ” ê²ƒì´ë‹¤.
 cross entropy ê°’ì€ í™•ë¥  ê°’ì„ ì„œë¡œ ê°„ì— ë¹„êµ í•˜ê²Œ ëœë‹¤.
 ğŸ” ê³„ì‚°í•œ Score ê°’ì€ ì–´ë–»ê²Œ ì´ëŸ¬í•œ í™•ë¥  ê°’ìœ¼ë¡œ Mappingí•  ìˆ˜ ìˆëŠ”ê°€?
 â¡ fittingì„ í•˜ê³ ì í•˜ëŠ” labelì€ 1ë˜ëŠ” 0ì¸ ê°’ì´ ë˜ê²Œ ë˜ëŠ”ë° scoreëŠ” ì‹¤ìˆ˜ ê°’ì´ê¸° ë•Œë¬¸ì— ê·¸ ì‹¤ìˆ˜ ê°’ì„ í™•ë¥  í•¨ìˆ˜ë¥¼ í†µí•´ì„œ mappingì„ í•´ì•¼ ëœë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.
 mappingì— ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜ê°€ sigmoidí•¨ìˆ˜ì´ë‹¤.

 Real value <i>h = w<sup>T</sup>Î¦(x)</i> 0 or 1
 ![image](https://user-images.githubusercontent.com/122149118/221765368-021663ac-db9d-44f7-bced-26c5e03a4f5f.png)

 ![image](https://user-images.githubusercontent.com/122149118/221765463-5620cf50-7482-4c6f-8eb6-87d8752466d0.png)

 ![image](https://user-images.githubusercontent.com/122149118/221765524-e0b5dd65-63fc-48ec-ba79-3036ed524770.png)
 Ïƒ(z)ë¼ê³  í•˜ëŠ” í•¨ìˆ˜ê°€ sigmoidí•¨ìˆ˜ë¡œì„œ ì´ í•¨ìˆ˜ëŠ” s curveì™€ ê°™ì€ í˜•íƒœë¥¼ ê°€ì§€ê³  ìˆë‹¤.
 ì´ í•¨ìˆ˜ì˜ ê°œí˜•ì„ ì‚´í´ë³´ê²Œ ë˜ë©´ ë§Œì•½ì— + ë°©í–¥ìœ¼ë¡œ real ê°’ì´ êµ‰ì¥íˆ ì¦ê°€í•˜ê²Œ ëœë‹¤ë©´ í™•ë¥  ê°’ 1ì— ê·¼ì‚¬í•˜ê²Œ ë  ê²ƒì´ë©°, ë°˜ëŒ€ë¡œ - ê°’ìœ¼ë¡œ êµ‰ì¥íˆ ì»¤ì§€ê²Œ ëœë‹¤ë©´ í™•ë¥  ê°’ 0ì— ê·¼ì‚¬í•˜ê²Œ ë  ê²ƒì´ë‹¤.
 ë°˜ëŒ€ë¡œ 0ì˜ ê°’ì„ ê°€ì§€ê²Œ ë˜ë©´ ì´ ê°’ì€ Â½ì˜ í™•ë¥ ì„ ê°–ëŠ” í•¨ìˆ˜ê°€ ëœë‹¤.
 ê³„ì‚°í•œ score ê°’ ì¦‰, h(x)ëŠ” w<sup>T</sup>Î¦(x)ì˜ ê°’ì„ sigmoid í•¨ìˆ˜ì— ì§‘ì–´ë„£ê²Œ ë˜ë©´ score ì‹¤ìˆ˜ ê°’ì„ 0ë¶€í„° 1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ mappingì„ í•  ìˆ˜ ìˆê²Œ ë  ê²ƒì´ë‹¤.
 ê·¸ëŸ¬í•œ í˜•íƒœë¥¼ logistic modelì´ë¼ê³  ì´ì•¼ê¸° í•œë‹¤.

## Sigmoid function

 * Squash the output of the linear function
    Ïƒ(-w<sup>T</sup>x) = <sup>1</sup>/<sub>1 + e<sup>-w<sup>T</sup>x</sup></sub>
 * A better approach : interpret as a probability
    <i>P<sub>w</sub></i>(y = 1|x) = Ïƒ(-w<sup>T</sup>x) = <sup>1</sup>/<sub>1 + e<sup>-w<sup>T</sup>x</sup></sub>
    <i>P<sub>w</sub></i>(y = 0|x) = 1 - Ïƒ(-w<sup>T</sup>x) = <sup>e<sup>-w<sup>T</sup>x</sup></sup>/<sub>1 + e<sup>-w<sup>T</sup>x</sup></sub>
 sigmoidí•¨ìˆ˜ëŠ” score ê°’ì„ 0ì—ì„œ 1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ëˆŒëŸ¬ì£¼ê¸° ìœ„í•˜ì—¬ ì‚¬ìš©í•˜ê²Œ ëœë‹¤.
 ë§Œì•½ yê°€ 1ì´ë©´ ì¦‰, positive sampleì— ëŒ€í•´ì„œëŠ” í™•ë¥  ê°’ì´ <sup>1</sup>/<sub>1 + e<sup>-w<sup>T</sup>x</sup></sub>ìœ¼ë¡œ ê³„ì‚°ëœë‹¤.
 ë°˜ëŒ€ë¡œ yê°€ -1ì¸ ê²½ìš° negative sampleì— ëŒ€í•´ì„œëŠ” í™•ë¥ ì— ëŒ€í•œ ê°’ì´ <sup>e<sup>-w<sup>T</sup>x</sup></sup>/<sub>1 + e<sup>-w<sup>T</sup>x</sup></sub>ìœ¼ë¡œ ê²°ì •ëœë‹¤.

## Cross-entropy loss

 ![image](https://user-images.githubusercontent.com/122149118/221767630-dee8c07d-ecd2-493e-8d66-631071124eb7.png)

 ì™¼ìª½ì—ëŠ” Y(bar) ì¦‰, estimatedëœ í™•ë¥  ê°’ì´ sigmoidí•¨ìˆ˜ë¥¼ í†µí•´ì„œ ì‚°ì¶œëœ ê°’ì„ ë³´ì´ê³  ìˆë‹¤.
 ì´ ë•Œ ì²« ë²ˆì§¸ vectorì— í•´ë‹¹í•˜ëŠ” 0.7, ë‘ ë²ˆì§¸ 0.2, ì„¸ ë²ˆì§¸ 0.1ì€ ê° 3ì°¨ì› ê³µê°„ìƒì—ì„œì˜ í•´ë‹¹í•˜ëŠ” ê°’ì„ í™•ë¥ ì ìœ¼ë¡œ í‘œí˜„í•œ ê°’ì´ë‹¤.
 ì‹¤ì œ label ê°’ì€ ë§ˆì°¬ê°€ì§€ë¡œ 3ì°¨ì› ê³µê°„ì—ì„œ 1.0, 0.0, 0.0ê³¼ ê°™ì€ ì¢Œí‘œ ê³µê°„ì— í‘œí˜„ë˜ì–´ ìˆë‹¤.
 ì¦‰, ì²« ë²ˆì§¸ categoryì— 1ë¡œ í‘œí˜„ë˜ì–´ ìˆëŠ” ì–´ë– í•œ classification idë¥¼ í‘œí˜„í•œë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.
 ì˜¤ë¥¸ìª½ì—ì„œ ë³¼ ìˆ˜ ìˆëŠ” ê²ƒì²˜ëŸ¼ 1ê³¼ 0ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆì§€ë§Œ ì´ ê°’ë“¤ ì—­ì‹œ ì´ í•©ì´ 1ì¸ í™•ë¥ (âˆ‘y<sub>i</sub> = 1)ì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.
 ì–´ë–¤ labelì€ ê°•í•˜ê²Œ 1ì´ê³ , ì–´ë–¤ labelì€ ê°•í•˜ê²Œ 0ì¸ ê·¸ëŸ° í•¨ìˆ˜ê°€ ë˜ëŠ” ê²ƒì´ë‹¤.
 ì™¼ìª½ì— ìˆëŠ” estimation ì—­ì‹œ ì˜¤ë¥¸ìª½ì— ìˆëŠ” label ì •ë³´ë¥¼ ì¶”ì •í•˜ê²Œë” ê°’ì´ ì‚°ì¶œí•˜ê²Œ ëœë‹¤.
 ë§Œì•½ modelì´ ë”ìš± ì •í™•í•´ì§„ë‹¤ë©´ ì´ ê°’ì€ 1, 0, 0ì— ê°€ê¹Œì›Œì§€ë„ë¡, ì˜ˆì»¨ë° 0.9, 0.1, 0 ì ì  ë” ê°’ì´ updateê°€ ë˜ê²Œë” cross entropy lossê°€ ì¤„ì–´ë“¤ê²Œ ë  ê²ƒì´ë‹¤.
 corss entropy lossëŠ” ì„œë¡œ ë‹¤ë¥¸ pì™€ qì˜ pmfì˜ ê±°ë¦¬ì´ë‹¤.
 ì˜¤ë¥¸ìª½ì—ëŠ” 1, 0, 0ì˜ ê°’ì„ ê°–ëŠ” í™•ë¥ ì´ë¼ê³  í•œë‹¤ë©´ ì™¼ìª½ì€ ê·¸ì— ìœ ì‚¬í•˜ê²Œë” ìƒˆë¡œìš´ loss ê°’ì„ updateë¥¼ í•´ë‚˜ì•„ê°€ë©´ì„œ í•™ìŠµì´ ì§„í–‰ë˜ê²Œ ëœë‹¤.

 **Understanding this Cost Function**
 Suppose that <i>L</i> = [1, 0, 0],
 * If Y(bar) = [1, 0, 0], then D = -1 Â· log 1 - 0 Â· log 0 - 0 Â· log 0 = -1 Â· 0 - 0 Â· (-âˆ) - 0 Â· (-âˆ) = 0 (no cost)
 * If Y(bar) = [0, 1, 0], then D = -1 Â· log 0 - 0 Â· log 1 - 0 Â· log 0 = -1 Â· (-âˆ) - 0 Â· 0 - 0 Â· (-âˆ) = âˆ (huge cost)
 * If Y(bar) = [0, 0, 1], then D = -1 Â· log 0 - 0 Â· log 0 - 0 Â· log 1 = -1 Â· (-âˆ) - 0 Â· (-âˆ) - 0 Â· 0 = âˆ (huge cost)

 loss functionì— label ì •ë³´ê°€ í˜„ì¬ 1, 0, 0ì¸ ê²½ìš°ì— Y(bar)ê°€ ìœ„ì™€ ê°™ì€ ê²½ìš°ì˜ ê°’ì„ ê°€ì§ˆ ë•Œ ì–´ë– í•œ lossë¥¼ ê°–ëŠ”ì§€ ì‚´í´ë³¸ë‹¤.
 ë§Œì•½ Y(bar)ê°€ [1, 0, 0]ì´ë¼ê³  í•œë‹¤ë©´ ì •í™•í•˜ê²Œ ìš°ë¦¬ì˜ labelê°’ê³¼ ë™ì¼í•˜ë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.
 ì´ ê²½ìš°ì—ëŠ” êµ‰ì¥íˆ ì‘ì€, costê°€ ì—†ëŠ”, lossê°€ ì—†ëŠ” í˜•íƒœê°€ ë˜ì§€ë§Œ, ë°˜ëŒ€ë¡œ label ì •ë³´ê°€ ë‹¤ë¥¸ [0, 1, 0]ì˜ ê°’ì„ ë§ì¶”ê³  ìˆë‹¤ë©´ ì´ ë•Œ lossëŠ” ë§ˆì°¬ê°€ì§€ë¡œ ì‚°ì¶œì„ í•´ë³´ë©´ âˆë¡œ êµ‰ì¥íˆ ë†’ì€ costë¥¼ ê°€ì§€ê²Œ ë  ê²ƒì´ë‹¤.

 **Gradient Descent Method**
 > <i>W â† W - Î±<sup>âˆ‚</sup>/<sub>âˆ‚W</sub>CE</i>

 ì´ë ‡ê²Œ ì–»ê²Œëœ lossì˜ ê°’ë“¤ì€ gradient descent algorithmì— ì˜í•´ì„œ cross entropyì˜ loss termì„ ì´ìš©í•˜ì—¬ ìƒˆë¡œìš´ parameter Wë¥¼ updateí•˜ëŠ”ë° ì‚¬ìš©í•œë‹¤.

## Training a linear classifier

 linear classifierë¥¼ í•™ìŠµí•˜ëŠ”ë° ì‹¤ì œ gradient descent algorithmì´ ì–´ë–»ê²Œ ì‚¬ìš©ì´ ë˜ê³  ìˆëŠ”ì§€ì— ëŒ€í•œ ë°©ì‹ì„ ë³¸ë‹¤.

 * Iterative optimization using gradient descent
    1. Initialize weights at time step <i>t = 0</i>
    2. Compute the gradients
    ![image](https://user-images.githubusercontent.com/122149118/221772949-52a83dd2-d9d0-4ed5-b019-8a525fba53c7.png)
    3. Set the direction to move : 
       <i>v<sub>t</sub> = -âˆ‡E<sub>train</sub>(w<sub>t</sub>)</i>
    4. Update weights
       <i>w<sub>t + 1</sub> = w<sub>t</sub> + Î±v<sub>t</sub></i>
    5. Iterate to next step until converging

 gradient descent algorithmì˜ frameworkì™€ ìƒë‹¹íˆ ìœ ì‚¬í•œ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.
 ë¨¼ì € weighê°’ì„ initializationí•œë‹¤.
 ê·¸ ë‹¤ìŒì—” gradient ê°’ì„ ê³„ì‚°í•´ì•¼ í•œë‹¤.
 ì´ ë•Œ í•„ìš”ë¡œ í•˜ëŠ” ê°’ì€ cross entropy loss ì¦‰,train lossì˜ ë¯¸ë¶„ termì´ ë˜ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.
 ìƒˆë¡œìš´ parameter w<sub>t</sub>ëŠ” Î±ì—ë‹¤ ìƒˆë¡œìš´ update term v<sub>t</sub>ì„ ê³±í•˜ê²Œ ë˜ì–´ ìƒˆë¡œìš´ w<sub>t + 1</sub>ì„ ì–»ê²Œ ëœë‹¤.
 ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ weightë¥¼ update í•´ ë‚˜ê°€ë©´ì„œ ìˆ˜ë ´í•  ë•Œê¹Œì§€ ë‹¤ìŒ ë‹¨ê³„ë¡œ ê³„ì†í•´ì„œ ì§„í–‰ í•˜ê²Œ ëœë‹¤.

## Multiclass classification

 * Not all classification predictive models support multi-class classication.
 * Split the multi-clas classification dataset into multiple binary classification datasets and fit a binary classification model on each.

 ![image](https://user-images.githubusercontent.com/122149118/221775255-6c6e8024-11b8-4058-8a88-f07696671b21.png)

 ìœ„ ì˜ˆì‹œëŠ” ì„ì‹œ recognition ë¬¸ì œì—ì„œì˜ multi-class classification ë¬¸ì œì— ëŒ€í•œ ë¬¸ì œì´ë‹¤.
 ì…ë ¥ìœ¼ë¡œ ì…ë ¥ imageë“¤ì´ ë‹¤ìŒê³¼ ê°™ì´ ë“¤ì–´ì˜¤ê²Œ ë  ë•Œ ì´ ì…ë ¥ imageë“¤ì´ ì–´ë– í•œ labelì— ì†í•˜ê²Œ ë˜ëŠ”ì§€ ì˜ˆì»¨ë° catì¸ì§€, dogì¸ì§€, mugì¸ì§€, hatì¸ì§€ ê°ê°ì˜ categoryë¥¼ ë§ì¶”ëŠ” ë¬¸ì œë¼ê³  ì´í•´í•  ìˆ˜ ìˆë‹¤.
 ì´ ì—­ì‹œ binary classificationì˜ ë¬¸ì œì—ì„œì™€ ê°™ì´ nì°¨ì›ì˜ coordinateìƒì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ signal space ìƒì—ì„œ ì…ë ¥ featureë¥¼ plottingí•œ ë‹¤ìŒ ì´ ì…ë ¥ featureë“¤ì„ ì ì ˆí•˜ê²Œ êµ¬ë¶„ì§€ì„ ìˆ˜ ìˆëŠ” hyper planeì„ í•™ìŠµì„ í•˜ë©´ì„œ ì–»ê²Œ ëœë‹¤.
 ì–´ë– í•œ modleê°™ì€ ê²½ìš°ì—ëŠ” multiclass classificationë¬¸ì œì—ì„œ ë°”ë¡œ class id ê°’ì„ ì¶œë ¥ì„ í•˜ëŠ” ê²½ìš°ë“¤ë„ ìˆì§€ë§Œ ë³´í†µì€ binary classification ë¬¸ì œë¥¼ multiclass classificationë¬¸ì œë¡œ í™•ì¥ì„ í•˜ëŠ” ê²½ìš°ë„ ìƒê°í•  ìˆ˜ ìˆë‹¤.

 **One-Vs-All**

 ![image](https://user-images.githubusercontent.com/122149118/221776509-46263841-ea8d-4840-8546-2239e61bcb59.png)

 ê·¸ë¦¼ì—ì„œ ë³´ëŠ” ë°”ì™€ ê°™ì´ 3ê°œì˜ hyper planeì„ ê·¸ì–´ì„œ multiclass classification ë¬¸ì œë¥¼ binary class classificationì˜ modelë¡œ í•˜ì—¬ê¸ˆ í’€ ìˆ˜ ìˆê²Œë” í•  ìˆ˜ ìˆë‹¤.
 ì²« ë²ˆì§¸ hyper planeì€ Cì¸ì§€ ê·¸ë ‡ì§€ ì•Šì€ì§€ë¥¼ êµ¬ë¶„í•˜ëŠ” modelì´ë‹¤.
 ë‹¤ìŒ binary classiciation modelì€ Bì¸ì§€ ê·¸ë ‡ì§€ ì•Šì€ì§€ë¥¼ í•™ìŠµí•˜ê²Œ ë˜ê³ , ë˜ ë‹¤ë¥¸ modelì€ Aì¸ì§€ ê·¸ë ‡ì§€ ì•Šì€ì§€ë¥¼ íŒë‹¨í•˜ê²Œ ëœë‹¤.
 ê·¸ì— ê´€í•œ í•™ìŠµ modelë“¤ì´ ê·¸ë¦¼ ì•„ë˜ì™€ ê°™ì´ ê°ê°ì˜ parameter wì— ê´€í•´ì„œ êµ¬ì„±ë˜ì–´ìˆë‹¤ê³  ìƒê°í•´ë³´ë©´ model Aë¥¼ ìœ„í•œ parameter termì„ w<sub>A1</sub>, w<sub>A2</sub>, label Bë¥¼ ìœ„í•œ model parameterë“¤ì„ w<sub>B1</sub>, w<sub>B2</sub>, ë§ˆì°¬ê°€ì§€ë¡œ sample Cë¥¼ ë¶„ë¥˜í•˜ê¸° ìœ„í•œ model parameterë¡œì¨ w<sub>C1</sub>, w<sub>C2</sub>ê°€ ìˆë‹¤ê³  ìƒê°í•  ë•Œ metrixë¥¼ ì—°ì‚°í•˜ê²Œ ë˜ë©´ ê°ê° sample A, B, Cì˜ score ê°’ì„ ì–»ì„ ìˆ˜ ìˆê²Œ ëœë‹¤.
 ì´ë ‡ê²Œ ì–»ê²Œëœ score ê°’ë“¤ì— sigmoidí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ í™•ë¥  ê°’ìœ¼ë¡œ mappingì„ í•  ìˆ˜ê°€ ìˆê²Œ ëœë‹¤.
 ê·¸ë¦¬ê³  labelì„ ì§€ì •í•˜ê²Œ ë  ë•Œ ì‚¬ìš©í–ˆë˜ One Hot Encodingì„ ì´ìš©í•˜ê²Œ ë˜ë©´ ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ í‘œ ì‚¬ì´ì— ê±°ë¦¬ë¥¼ ê°€ê¹ê²Œ í•˜ë©´ì„œ í•™ìŠµì„ í•  ìˆ˜ ìˆë‹¤.
 one hot encodingì´ë¼ê³  í•˜ëŠ” ìš©ì–´ëŠ” ì „í˜€ ë‹¤ë¥¸ ê²ƒì´ ì•„ë‹ˆë¼ ì•ì„œ ë³´ì•˜ë˜ multiclass classificaitonì—ì„œì˜ labelì„ ì§€ì •í•˜ëŠ” ë¬¸ì œì—ì„œ ê°ê° vectorë§ˆë‹¤ [1, 0, 0], [0, 1, 0], [0, 0, 1]ê³¼ ê°™ì´ í•´ë‹¹í•˜ëŠ” ìœ„ì¹˜ì— 1ì„ signalingì„ í•¨ìœ¼ë¡œì¨ labelì˜ ì •ë³´ë¥¼ ê¸°ë¡í•˜ëŠ” ê²ƒì„ one hot encodingì´ë¼ê³  í•œë‹¤.
 ![image](https://user-images.githubusercontent.com/122149118/221778368-a657d354-e36b-49ca-97a9-3f2f65d5d53f.png)

 ì´ë ‡ê²Œ one hot encodingëœ labelê°’ê³¼ ê·¸ë¦¬ê³  sigmoid modelì´ ì¶œë ¥í•˜ëŠ” í™•ë¥  ê°’ì„ ì„œë¡œ ê°„ì— ë¹„êµí•˜ì—¬ loss functionì„ í†µí•´ errorë¥¼ ê³„ì‚°í•¨ìœ¼ë¡œì¨ í•™ìŠµì„ í•  ìˆ˜ ìˆê²Œ ëœë‹¤.

## Advantage of liear classiciation

 * Simple
 * Interpretability (example in Murphy 2012)
    - x<sub>1</sub> : the number of cigarettes per day, x<sub>2</sub> : minutes of exercise per day
    - The goal is to predict <i>P(Y</i> = lung cancer<i>)</i>
    - Assume we have estimated the best parameter w = (1.3, -1.1) to have <i>h(x) = 1.3x<sub>1</sub> - 1.1x<sub>2</sub></i>
    â†’ For every cigarettes per day, the risk increased by a factor of e<sup>1.3</sup>
    <i><sup>p(y = +1|x)</sup>/<sub>p(y = -1|x)</sub> = e<sup>w<sup>T</sup>x</sup> = e<sup>w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub></sup></i>

 linear classification modelì˜ ì¥ì  ì—­ì‹œ linear regression modelì—ì„œì˜ ì¥ì ê³¼ ë¹„ìŠ·í•œ ì¥ì ë“¤ì´ ì—¬ëŸ¿ ìˆë‹¤.
 ì˜ˆë¥¼ ë“¤ì–´ linear classification modelê³¼ ê°™ì€ ê²½ìš°ëŠ” ëŒ€ë‹¨íˆ ê°„ë‹¨í•˜ë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤.
 ê°„ë‹¨í•˜ë‹¤ê³  í•˜ëŠ” ê²ƒì€ ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆê³  ì‰½ê²Œ testí•  ìˆ˜ê°€ ìˆë‹¤ê³  í•˜ëŠ” ê²ƒì´ë‹¤.
 ë”°ë¼ì„œ Linear Classification Modelì€ ê°€ì¥ ì²˜ìŒì— ì‹œë„í•˜ê¸°ì— ì í•©í•œ í˜•íƒœì˜ modelì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.
 ë‘ ë²ˆì¬ë¡œëŠ” intergretability ì¦‰, í•´ì„ ê°€ëŠ¥ì„± ì—­ì‹œ ìƒë‹¹íˆ ì¦ê°€í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.
 ì˜ˆë¥¼ ë“¤ì–´ í•˜ë£¨ì— í”¼ëŠ” ë‹´ë°°ì˜ ê°œìˆ˜ë¥¼ x<sub>1</sub>ì´ë¼ê³  ê¸°ë¡í•˜ê³ , í•˜ë£¨ì— ìš´ë™í•˜ëŠ” ì‹œê°„ì„ x<sub>2</sub>ë¼ê³  ê¸°ë¡í•œë‹¤.
 ê·¸ë¦¬ê³  ëª©ì ì€ ì´ ì‚¬ëŒì´ íì•”ì„ ê°€ì§€ê²Œ ë ì§€ ì•„ë‹ì§€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” modelì„ ìˆ˜ë¦½í•œë‹¤ê³  ìƒê°í•œë‹¤.
 ì´ ì˜ˆì¸¡ modelì˜ ê²°ê³¼ ì¶”ì •í•œ best parameter wëŠ” 1.3ê³¼ -1.1 ì¦‰, hyper plane modelì¸ h(x)ê°€ 1.3x<sub>1</sub> - 1.1x<sub>2</sub>ë¼ê³  ì´í•´í•œë‹¤.
 ë‚ ë§ˆë‹¤ ë‹´ë°°ë¥¼ í”¼ëŠ” ê²½ìš° riskë¥¼ ê³„ì‚° í•´ë³¸ë‹¤ë©´ í™•ë¥  ê°’ì— ì˜í•´ì„œ given xì— ëŒ€í•´ì„œ íì•”ì¼ í™•ë¥ ê³¼ íì•”ì´ ì•„ë‹ í™•ë¥ ì˜ ë¹„ìœ¨ì„ ê³„ì‚°í•´ë³´ë©´ ìœ„ì™€ ê°™ì€ ìˆ˜ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.
 ì´ ë•Œ w<sup>T</sup>xê°€ ë°”ë¡œ hyper plane w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub>ê°€ ëœë‹¤.
 ì´ ì‹ì„ í†µí•´ ê³„ì‚°ì„ í•´ë³´ë©´ ë‹´ë°°ì˜ ìœ„í—˜ì„±ì€ e<sup>1.3</sup>ì´ë‹¤.
 ì´ì™€ ê°™ì´ ê° ìš”ì†Œ ë³„ë¡œ ìš”ì†Œê°€ 1 ë‹¨ìœ„ ì¦ê°€ë¥¼ í•  ë•Œë§ˆë‹¤ ì „ì²´ score ê°’ì´ ì–´ë–»ê²Œ ë³€í•˜ê²Œ ë˜ëŠ”ì§€ë¥¼ ì¶”ì •í•¨ìœ¼ë¡œì„œ í•´ì„ ê°€ëŠ¥ì„±ì„ ì œê³µí•  ìˆ˜ ìˆê²Œ ëœë‹¤.

## Quiz

 **What answers are correct?**

 **A.** Ina linear classification model, a hyperplane is used for a decesion boundary to classify training samples, assuming samples are linearly separable.
 linear classification modelì—ì„œëŠ” hyperplaneì´ decesion boundaryë¡œ ì‚¬ìš©ì´ ëœë‹¤ê³  í–ˆì„ ë•Œ sampleeë“¤ì€ ì„ í˜•ì ìœ¼ë¡œ separableí•˜ë‹¤ë¼ê³  ê°€ì •í•  ìˆ˜ ìˆë‹¤.
 **A. Correct** 

 ![image](https://user-images.githubusercontent.com/122149118/221785035-c51ed5ea-68cd-4f68-a168-4d73cb654b22.png)
 ì„ í˜• modelì—ì„œëŠ” positive, negative sampleë“¤ì´ ì„ í˜•ì„ hyperplaneì— ì˜í•´ì„œ ì„ í˜•ì ìœ¼ë¡œ êµ¬ë¶„ì´ëœë‹¤ê³  ê°€ì •ì„í•˜ê³  linear classification modelì„ ìˆ˜ë¦½í•˜ê²Œ ëœë‹¤.

 **B.** Cross-entropy loss represents an error or a dissimilarity between two real values, and therefore it can be directly used to compute an error of a score value
 cross entropy lossê°€ errorì˜ ê°’ì„ í‘œí˜„í•˜ê²Œ ë˜ëŠ”ë° ì´ ë•Œ ì„œë¡œ ë‹¤ë¥¸ ë‘ ê°œì˜ ì‹¤ìˆ˜ê°’ì˜ ì‚¬ì´ì— ì„œë¡œ ë‹¤ë¥¸ ì°¨ì´ë¥¼ í‘œí˜„í•œë‹¤.
 **B. False** Cross-entropy loss measures a dissimilarity of two pmfs.
 A sigmoid function is first applied to convert a real value into a probability between 0 and 1, and the the loss is used to compute the error value.
 ![image](https://user-images.githubusercontent.com/122149118/221786334-306fafd7-19e7-4ec9-afff-1293f3a8efc3.png)
 cross entropy lossëŠ” ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ pmf ì‚¬ì´ì— dissimilarityë¥¼ ì¸¡ì •í•˜ëŠ” ë„êµ¬ì´ë‹¤.
 ë”°ë¼ì„œ ì„ì˜ì˜ ì‹¤ìˆ˜ê°’ì´ ìˆë‹¤ê³  í•œë‹¤ë©´ sigmoid í•¨ìˆ˜ ë“±ì„ í†µí•´ í™•ë¥  ê°’ìœ¼ë¡œ mappingì„ í•œ ë’¤ì— lossë¥¼ ê³„ì‚°í•´ì•¼ í•œë‹¤.

 **C.** A binary linear classifier can be extended to a multiclass linear classifier
 binary linear classifierê°™ì€ ê²½ìš° multiclass linear classifierë¡œ í™•ì¥ì´ ê°€ëŠ¥í•˜ë‹¤.
 **C. Correct** A binary linear classifier can be extended to a multiclass classifier by applying an one-Vs-all classification per class, although a binary linear classifier ingerently estimates only true/false.
 ![image](https://user-images.githubusercontent.com/122149118/221787180-c2779658-22b0-44d7-ba93-b45e2f2978a1.png)
 ì´ëŸ¬í•œ ë°©ì‹ì„ One-Vs-All ì´ë¼ê³  í•´ì„œ binary linear classifierë¥¼ ì—¬ëŸ¬ ê°œ ì‚¬ìš©ì„ í•¨ìœ¼ë¡œì¨ multiclass classificationë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹¤.

## Summary

 * Linear classification model
    - Uses a hyperplane as a decision boundary to classify samples based on a linear combination of its explanatory variables
    - It has several advantages ; simplicity and interpretability
    - Cross-entropy loss measures the performance of a classification model whose output is a prbability value between 0 and 1.
    A sigmoid function is used to map a score value into a probability value.
    - Multi-classificatio problem can be solved through one-vs-all.

 hyperplaneì„ ì´ìš©í•´ì„œ decision boundaryë¥¼ ìˆ˜ë¦½í•˜ê²Œ ë˜ëŠ”ë° ì´ë•Œ hyperplaneì€ ë°”ë¡œ ì…ë ¥ ë³€ìˆ˜ì™€ parameterì˜ linear combinationìœ¼ë¡œ êµ¬ì„±ëœë‹¤.
 linear classification model ì—­ì‹œ ëª‡ ê°€ì§€ì˜ ì¥ì ì´ ìˆë‹¤.
 ì²« ë²ˆì§¸ëŠ” ë‹¨ìˆœí•˜ë©°,ë‘ ë²ˆì§¸ëŠ” í•´ì„ ê°€ëŠ¥í•˜ë‹¤ê³  í•˜ëŠ” ê²ƒì´ë‹¤.
 ì´ëŸ¬í•œ linear classification modelì„ í•™ìŠµì„ í•˜ê¸° ìœ„í•´ì„œëŠ” cross entropy loss ë“±ê³¼ ê°™ì€ loss í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ëŠ”ë° íŠ¹íˆ cross entropy í•¨ìˆ˜ê°™ì€ ê²½ìš° ëŒ€í‘œì ì¸ loss í•¨ìˆ˜ë¡œì¨ í™•ë¥  ê°’ì„ ê³„ì‚°í•œ ì´í›„ì— ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ í™•ë¥  í•¨ìˆ˜ì˜ dissimilarityë¥¼ ê³„ì‚°í•˜ì—¬ ê·¸ errorë¡œ í•˜ì—¬ê¸ˆ hyper planeì„ í•™ìŠµí•œë‹¤.
 ë§ˆì§€ë§‰ìœ¼ë¡œ multi classification ë¬¸ì œëŠ” one-vs-allê³¼ ê°™ì€ í˜•íƒœë¡œ binary classification ë¬¸ì œë¥¼ í™•ì¥í•˜ì—¬ í•´ê²°í•  ìˆ˜ ìˆë‹¤.