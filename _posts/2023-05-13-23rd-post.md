---
layout: post
title: "One stage detector - YOLO v1, v2, v3, v4"
description: "Part 2. 컴퓨터비전 특화 이론과 실습", "Ch05. 객체탐지(Object Detection)와 분할(Segmentation)", "Object Detection - Object Detection by CNN"
date: 2023-05-13
tags: study
comments: true
---
# One-Stage Detector: YOLO
 * You Only Look Once
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/8dcb7003-197e-41b9-b28b-5407d9ea7a8b)
 <b>2-Stage Detector</b> Regional Proposal와 classification이 순차적으로 이루어짐
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/85a66e25-3388-452a-b15f-954595756447)
 Region Proposal을 Selective Search나 Region proposal network, Edgebox등의 idea로 통과를 시킨 후 box proposal들에 대해 classification을 하게 되고 한 쪽 branch는 multi-class classification을 하고 다른 한 쪽 branch는 bounding box regression을 통해서 localize와 object가 무엇인지에 대한 detection을 수행한다.
 <b>1-Stage Detector</b> Regional Proposal와 classification이 동시에 이루어짐
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/02e06268-924b-45be-a291-22cc4fbf25b4)
 input image가 있을 때 feature extraction이 Conv Layer를 통과하고 feature map에 통과되어 각 special grid 또는 special location에 대해 multi-class classification과 bounding box regression이 동시에 수행된다.
 Faster R-CNN과 비슷한 시기에 논문이 출간되었다.

 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/9f4ee548-4c6a-4f45-995e-f066489f016b)
 Figure 1: The YOLO Detection System
 (1) resizes the input image to 448 x 448
 (2) runs a single convolutional network on the image
 (3) thresholds the resulting detections by the model's confidence
 어떤 image가 있을 때 input image를 448 by 448 size로 resize한다.
 존재하는 single image convolutional network를 통과시켜 결과를 만들고 feature map에서 thresholding하여 detection을 수행한다.
 model의 confidence score로 class label을 prediction하게 된다.
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/3b033ff8-1cee-4669-8b25-24809f3a7b2b)
 Figure 2: The Model. Our system models detection as a regression problem.
 It divides the image into an S x S grid and for each grid cell predics B bounding boxes, confidence for those boxes, and C class probabilites. These predictions are encoded as an S x S x (B * 5 + C) tensor.
 output feature map에 대해 어떻게 detection을 수행하는지 나타낸 것이다.
 model detection을 regression problem으로 다시 정의하였다.
 S by S grid가 있을 때 한 쪽 branch에서는 bounding box를 찾고 이 bounding box의 localization score, confidence score를 정한다.
 다른 한 쪽 branch에서는 class의 probability map을 구한다.
 영역별로 dominate한 object를 나누게 된다.
 S by S cell로 잘려진 gird에서 B개의 bounding box를 찾게되고 그 confidence score를 구하게 된다.
 진한 선으로 되어있을수록 confidence score가 높은 것으로 실제로는 regression같은 것으로 구하게 될 것이다.
 output probability map을 사용하여 영역이 어떤 object class를 가리키는지에 대해서 classification task를 수행한다.

 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/c9cefc35-bc90-417e-8a97-7c83d23b64d3)
 Figure 3: The Architecture. Our detection network has 24 convolutional layers followed by 2 fully connected layers. Alternating 1 x 1 convolutional layers reduce the features sapce from preceding layers. We pretrain the convolutional layers on the ImageNet classification at half the resolution (224 x 224 input image) and then double the resolution for detection.
 YOLO의 전체적인 architecture로 input image 448 by 448 size image를 넣었을 때 처음에는 7 by 7 convolution kernel이 sliding하여 다음 feature를 만들게 된다.
 7 by 7 size이기 때문에 ¼ size로 feature map의 size가 줄어들게 되고 192의 kernel을 사용하여 처음 resolution을 확 줄이고 3 by 3 kernel의 256개를 사용하여 그 다음 feature map을 만들고 3 by 3으로 feature map을 쭉 만들어 최종적으로 7 by 7 by 1014 size feature map을 만든다.
 최종적인 conv layer 한개를 1 by 1 convolutional layer를 통과시키거나 MLP를 통과시켜 feature map을 만들고 30개의 class에 대해서 7 by 7 결과를 가지는 detection network가 사용된다.
 뒤에 2개의 fully connected layer가 사용되는데 대안적으로 1 by 1 convolutional layer를 사용해서 pixel wise로 MLP를 돌려주는 효과를 얻는다.
 1 by 1 convolution을 sliding 하는 것은 각 pixel에 대해 서로 다른 classification vector에 대해 MLP를 sliding하는 것과 비슷한 효과를 주게된다.
 ImageNet classification task로 ½ size로 pre-training한 classification network를 사용한다.
 detection할 때 resolution을 지역적인 부분을 찾아내야되기 때문에 2배로 늘려 fine-tuning을 하게 된다.

 * Loss functions
    - Classification loss, Confidence loss, Localization loss
    ![image](https://github.com/leexera/leexera.github.io/assets/122149118/b35b7144-e112-40e2-9c73-17e9ab994e48)
    ![image](https://github.com/leexera/leexera.github.io/assets/122149118/b33a2776-e529-4196-be83-4c6712d3601a)
    ![image](https://github.com/leexera/leexera.github.io/assets/122149118/bb105dee-8884-436c-b7e9-2bcf92e04552)

 YOLO에 사용되는 loss는 총 3가지 loss function이 있다.
 첫 번째는 classification loss로 어떤 cell에서 prediction한 class probability가 실제 ground-truth class probability와 얼마나 비슷한지에 대해 맞으면 1, 아니면 0을 주는, 모든 cell들에 대해 classificaiton socre를 구해주는 값이다. 
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/89d07725-17cd-4374-858d-b7d13c97c746)
 object가 없는 곳에는 위의 1대신 0이 들어갈 것이다.
 즉, object가 없는 영역은 계산하지 않고 object가 있는 영역에 대해서만 classification loss를 계산을 해주는 것이다.
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/0c60d97d-8abd-4c55-bbea-8a7450a349fb)
 위 부분이 MSE loss처럼 되어 있지만 cross entropy로 바꾸어 class probaibility를 구해도 좋을 것이다.
 두 번째 loss term은 confidence loss term이다.
 C는 confidence로 class proability score가 얼마나 자신감있게 되는지를 나타낸다.
 마지막 confidence loss는 localization loss로 각 x, y, width, height에서 계산을 해준다.
 최종적으로는 세 가지 loss를 더하여 multi-task loss를 사용해주는 일을 한다.

 * Non-maximal suppressions
    - 중복되는 박스 제거
    - Confidence score 순으로 sorting하여 제거함
 이 때 YOLO에서 중요한 점은 non-maximal suppression을 했다는 것인데, 어떤 image가 있을 때 중복되는 box를 제거하는 것이다.
 image에 대한 여러가지 object proposal이 나올 때 일정 overlap 이상이 되는 부분, objectness score, confidence score가 낮은 부분들의 box를 제거하는 것이다.
 non-maximal suppresion(NMS)은 object detection에서 뿐만 아니라 Computer Vision에서 고전적으로 outliar를 filtering하거나 score를 높지 않은 것을 제거할 때 많이 사용하는 term이다.
 * Results
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/13e33031-70bd-4708-96e4-249628480216)
 Table 1: Real-Time System on PASCAL VOC 2007. Comparing the performance and speed of fast detectors. Fast YOLO is the fastest detector on record for PASCAL VOC detection and is still twice as accurate as any other real-time detector. YOLO is 10 mAP more accurate than the fast version while still well above real-time in speed.
 YOLO는 real time detector 중에서도 Fast YOLO를 사용했을 때 155FPS라는 괴물같은 성능을 낸다.
 155FPS는 1초에 155장의 frame을 detection할 수 있다는 것이고 기존의 100Hz DPM이나 30Hz DPM보다 더 빠르고 좋은 성능을 낸다.
 Fast YOLO는 52.7 mAP를 내고, YOLO는 속도의 손해를 조금 보지만 mAP가 많이 늘어나는 것을 볼 수 있다.
 기존의 real-time detector 중에 30FPS를 넘어가면 실시간으로 사용할 수 있는 object detector라고 말하는데, real-time이 아닌 것과도 비교한 것을 나타내었다.
 Fastest DPM이나 VGG 16을 사용한 Faster R-CNN 결과들과 비교했을 때 Faster R-CNN VGG-16보다 YOLO의 성능이 떨어지지만 unified real time object detection, 빠르다는 것을 장점으로 내세워 object detection을 수행한다.
 결과적으로 66.4라는 mAP성능을 내고 R-CNN보다는 더 높은 mAP와 FPS를 나타내고 Faster R-CNN보다는 약간은 떨어지지만 더 빠른 성능을 보인다.
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/68059596-fb10-465e-b85f-c62d67a45437)
 Figure 4: Error Analysis: Fast R-CNN vs. YOLO. These charts show the percentage of localization and background errors in the top N detections for various categories (N = # objects in that category)
    * Connect: correct class and IOU > .5
    * Localization: correct class, .1 < IOU < .5
    * Similar: class is similar, IOU > .1
    * Other: class is wrong, IOU > .1
    * Background: IOU < .1 for any object
 YOLO는 맞춘 것에 비해 localization error가 크다.
 correct는 IOU가 0.5보다 큰 경우이고 Loc는 class는 맞추었으나 IOU score가 낮은, bounding box가 조금 빗겨나있는 경우를 뜻하고 Sim은 class는 similar하지만 IOU는 localization보다 낮은 것이다.
 Other은 class는 틀렸지만 IOU는 어느정도 맞춘 경우고 Background는 don't care하는 영역이다.
 결과적으로 YOLO는 class는 잘 맞추었지만 YOLO 자체가 7 by 7 grid로 나누고 precision의 정도, image는 448 by 448인데 feature map은 7 by 7이므로 너무 작아져 있기 때문에 정밀하게 bounding box를 맞출 가능성이 낮아진다.
 따라서 IOU가 0.1에서 0.5 사이인 경우가 많이 있었지만 class는 잘 맞추었다고 한다.
 정밀성 정도만 높인다면 YOLO의 성능을 개선할 수 있다는 message를 담고 있다.
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/e766e0cc-663d-4ee2-9529-5098062cb49d)
 Picasso dataset은 그림에서 object들을 찾는 task로 p-r curve에서 YOLO가 기존 기법들보다 성능이 좋다는 것을 나타낸다.
 VOC, Picasso, People-Art dataset에서도 AP score가 기존의 R-CNN보다 높은 것을 볼 수 있다.
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/28e6fc7a-c948-45d4-af19-b15b0de38e5b)
 natural image에 적용했을 때 합성 image에서도 정확하게 detection하는 것을 볼 수 있다.
 하지만 사람을 airplane으로 잘못 detection하는 것 또한 볼 수 있다.

# YOLO v2
 * YOLOv2: Improving YOLO
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/790526a1-36e6-44c7-9c09-8fae2fb1a338)
 2018년까지 object detection의 대표적인 algorithm을 도식화한 것이다.
 R-CNN, Region Proposal Network(RPN), YOLO, Single Shot box Detector(SSD), Feature Pyramid Network(FPN)등이 있다.
 
 * Better
    - Batch normalization
    - High-resolution classifier
    - Convolutional with anchor boxes
    - Dimension clusters
    - Direct location prediction
    - Fine-grained features
    - Multi-scale fraining
 * Faster
    - DarkNet
    - Training for classification
    - Training for detection
 * Stronger
    - Hierarchical classification
    - Dataset conbination with WordTree
    - Joint classification and detection
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/a45f9d59-c894-4a23-b3fd-16bd0d68497d)
 제안 방식은 3가지로 나누어진다.
 이 제안 방식이 얼마나 incremental하게 성능을 높이는지가 표에 나타나있다.
 YOLO에 batch nomalization만 추가했을 때 성능이 2%만 증가하고, High resolution classifier를 추가했을 때 성능이 4%정도 증가한다.
 convolutional layer들로 구성하고 anchor box를 추가하고 new network인 DarkNet이라는 network를 추가했을 때 성능이 조금 더 증가하는 것을 볼 수 있다.
 dimension prioir를 사용했을 때와 localization prediction을 사용하게되면 직접적으로 direct하게 location을 prediction하는 것이기 때문에 성능이 74.4까지 증가하게 된다.
 passthrough나 multi-scale inference, high resolution detector까지 포함하게 되고 최종적으로 제안하는 YOLOv2는 anchor box를 제외한 model로 78.6까지 무려 15% 성능 향상을 일으켰다.

 * Better: Accuracy, mAP 개선
    - Batch normalization: 모든 Conv layer에 BN 추가
    - High-resolution classifier: ImageNet으로 pre-training하고 448 x 448의 해상도로 10 epoch 동안 fine tuning
    - Convolutional with anchor boxes: V1에서 FC부분을 Conv로 대체하고 Anchor box 적용(recall 81% → 88%, mAP: 0.3%p 감소, computation: 33%p 감소)
 accuracy와 mAP를 개선하기 위한 방안들이다.
 anchor box를 적용하게 되면 box 후보들이 많이 생기기 때문에 recall이 증가하게 되지만 mAP는 감소하였고 conputation은 빨라졌다고 한다.
    - Dimension cluster
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/6c220853-9b32-4454-a068-93b4bdf9621d)
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/d96650c7-8060-49c3-8c14-811c80572310)
 Figure 2: Clustering box dimensions on VOC and COCO. we run k-means clustering on the dimensions of bounding box to get good priois for our model. We find that k = 5 gives a good tradeoff for recall vs. complexity of the model.

 cluster에 box generation방법을 SSE나 IOU, Anchor box등을 사용해서 clustring을 한다.
 비슷한 cluster끼리 뭉치는 clustring을 수행하는 것이다.
 cluster IOU의 9개를 썼을 때 가장 높아지지만 k = 5를 사용했을 때 recall과 complexity model이 너무 오래걸리면 안되기 때문에 cluster box dimension과 같은 요소를 사용한다.
 dimension cluster로 proposal box에 clustering으로 성능을 더 높인다.
    - Direct location prediction
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/ed06626c-0701-4332-aea1-5a8c2b98cb2a)
 x라는 것은 width의 t를 곱하고 anchor box를 빼주는 displacement를 구할 수 있다.
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/f655b932-c36d-4e4e-8c25-82b3c7dcf33a)
 network가 bounding box값을 구할 때 displacement가 아니라 값의 location을 direct하게 구해주면 더 좋아졌다고 한다.
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/889083ce-b66b-4f2a-87ab-9ed3cf54f8fa)
 Figure 3: Bounding boxes with dimension priors and location prediction. We predict the width and height of the box as offsets from cluster centroids. We predict the center coordinates of the box relative to the location of filter application using a sigmoid function.
 bounding box x, bounding box y, bounding box width, bounding box height는 어떤 anchor box가 있어서 움직이는 형태로 prediction하는 것이 아닌 anchor box 자체를 한 번에 바로 prediction한다는 것이다.
 R-CNN같은 경우 region proposal들이 있기 때문에 bounding regressior에서 그 움직임, 어떻게 커지고 작아져야할지 prediction하는 technique을 사용하는데 YOLO v2와 같은 1-stage detector는 따로 region proposal을 뽑는 algorithm이나 region proposal network가 없기 때문에 수행할 필요가 없어지고, 차이, 움직임을 transformation 계산하는 것이 아니라 바로 bounding box location을 prediction하는 것이다.
 offset은 cluster centroid로부터 사용하게 된다.
    - Fine-grained features: high-resolution feature map을 유지하고 bbox prediction, 최종 prediction 때 grid를 2 x 2로 나누어 prediction
 따라서 좀 더 어떤 feature의 지역성을 유지하는 feature map을 만드는 방식을 수행한다.
    - Multi-scale training: image size를 32 등차수열로 input으로 사용, {320, 352, ..., 608} 총 10개의 image pyramid
 위와 같은 7가지 technique을 통해 조금 더 object detection accuracy와 mAP를 개선한다.
 * Faster: 속도 개선
    - DarkNet: VGG는 30B의 계산량, 자체 개발한 DarkNet으로 8B 계산량 만듦
    - Training for classification: classification task로 pre-training
    - Training for detection: detection task로 학습(hyperparameter detail papaer 참고)
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/e9e043ce-0ee7-4de1-b284-0b96e1127f8f)
 ResNet과 paralle한 work라서 ResNet을 사용할 수도 있었지만 DarkNet을 사용했다.
 * Stronger: 더 많은 class를 cover하기 위한 전략
    - Hierarchical classification
       * ImageNet을 사용하여 같은 계층의 class끼리는 비슷하게 묶어서 학습
       * ImageNet의 class가 너무 fine-grained하게 되어 있기 때문에, 이를 보완하기 위한 전략
    - Dataset combination with WordTree
    - Joint classification and detection
    ![image](https://github.com/leexera/leexera.github.io/assets/122149118/0b1e4e53-cb94-41e7-a46d-92755ca54805)
    기존의 COCO dataset은 70개의 class가 있는데 ImageNet은 fine-grained하게 22k, 22000개의 class가 있다.
    따라서 hierarchical calssification한다.
    ![image](https://github.com/leexera/leexera.github.io/assets/122149118/9ab25f85-6519-4a08-abe8-9b33f43f5a10)
    22000개의 vector를 class로 가질 수 없기 때문에 이것을 1000개의 class만 남기고 hierarchical을 사용하는 것이다.
    wordtree가 있을 때 animal이나 artifact, netural object, phenomenon 4가지로 나누어질 수 있을 때 또 다른 tree로 구성이될 수 있다.
    즉, 어떤 hierarchy에서 비슷한 것끼리는 묶고 다른 것끼리는 분류하여 WordNet hierarchy를 사용한 dataset combining을 했다.
    이렇게 했을 때의 장점은 WordNet graph에 대해서 어떤 hierarchy tree가 구성이 되고 조금 더 분리된 결과에 대해서 값을 prediction을 하여 효과적인 학습이 가능하다는 것이다.
    COCO는 70개밖에 안되지만 더 많은 class에 대해서가 가능해진다.
    wordtree 내에서 비슷한 class에 대해서만 묶어서 softmax를 수행을 해줌으로써 조금 더 나은 classification을 수행할 수 있다.
 word tree와 같은 natural langauage를 활용하고자 하는 움직임은 2022년 multi-modal learning에서 visual grounding이라던지 refering object segmentation이나 image captioning, word 정보까지 같이 사용해서 joint embedding을 하거나 segmentation을 하거나 또는 image의 caption을 달아주거나 word, 자연어 정보까지 같이 활용하고자 하는 것이 vision에서도 활용이 되고 있다.

 * Results
    - 빠르고 좋음
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/cabd4089-0391-40f0-9ed6-4848a90f2191)
 COCO test-dev 2015 dataset에 대해서 했을 때 SSD512보다는 조금 안좋지만 real-time 성능을 낸다는 점에서 YOLO가 Fast R-CNN보다는 좋은 성능을 낸다.
 아쉽게도 Faster R-CNN과 SSD보다는 낮은 성능을 보인다.
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/15f9a986-0042-4aca-b9fb-c7980de7a4ba)
 30FPS를 기준으로 했을 때 YOLO는 real-time 성능까지도 낸다는 점을 장점으로 내세운다.
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/9743183d-d6b7-4e08-9923-b4212a64cbe1)
 PASCAL VOC 2007 detection framework에서 계산해보았을 때 fine tuning이 잘 되었는지 SSD500, YOLO, Faster R-CNN보다도 더 좋은 FPS와 mAP를 달성하여 SOTA 성능을 나타낸다.
 image resolution을 키우면서 성능을 많이 증가시킨 것을 확인할 수 있는데 resolution을 키우게 되면 정보가 많아지기 때문에 성능이 올라가는 것이므로 어찌보면 당연한 결과이기도 하다.

    - Real-time object detection system
    - WordTree를 사용하여 좀 더 general한 class의 bounding box prediction을 할 수 있다.
 YOLO v2의 결론은 real-time object detection system을 조금 더 정확하고 정밀하게 완성했다는 것이다.
 * Future work
    - Weakly-supervised image segmentation/detection으로의 적용?
    - Box data 말고도, 많은 labeled data를 좀 더 효과적으로 사용해보자
 ImageNet같은 경우도 bounding box image들이 많은데, label이 없는 box data 말고도 class label만 사용하거나 supervision이 낮은 것으로도 효과적으로 사용해보자는 것을 이야기 한다.
 추후에 CAM, Class Activation Map을 활용한 weakly-supervised object detection이나 segmentation에서 다루어질 것이다.

# YOLO v3
 * motivation
    - better
    - Detection accuracy
    - Fast inference
    - Low resource requirement
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/bac26a55-5f9f-4bef-9347-f71f2b8c77cd)
 YOLO v3가 RetinaNet보다 성능은 조금 낮지만 어느정도 성능이 조금 올라가는 것을 보여준다.

 * contribution
    - Bounding box prediction
       * 하나의 object에 하나의 anchor box할당
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/9adc559f-1883-4b62-88b9-c0daa5695a8a)
 YOLO는 bounding box를 하나씩 할당해주는 반면 다른 detector들은 여러 개의 box를 할당하여 사용한다.
 하나의 box만을 할당했을 때 더 빠른 inference가 가능해진다.

    - Class prediction
       * 각 box는 class를 예측, logistic classifier사용
    - Predictions across scales
       * 3가지 다른 scale 사용한 feature pyramid 사용, 3개 scale에 대해 각각 3개 box를 생성하여 9개의 anchor box가 있음
 Faster R-CNN의 region proposal Network와 비슷한 approach를 한다.
    - Feature extractor
       * DarkNet-53
    ![image](https://github.com/leexera/leexera.github.io/assets/122149118/8d296d85-2e29-4605-b37c-01f53e36bf6a)
    ![image](https://github.com/leexera/leexera.github.io/assets/122149118/e07f7f43-0b18-44d1-8205-b551124e98c1)
 DarkNet-19에서 DarkNet-53으로 개선한다.
 이와 같은 backbone network의 개선으로 성능을 증가시킨다.
    - Training 
 
 * Results
  ![image](https://github.com/leexera/leexera.github.io/assets/122149118/bac26a55-5f9f-4bef-9347-f71f2b8c77cd)

# YOLO v4
 * motivation
    - Practically, one-stage detector의 개선
    - 다양한 technique을 통해 성능, 속도 향상
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/68027bdc-802e-465b-b0e3-6aa64f7fe946)
 real-time 영역에서 성능을 많이 개선하였고 mAP도 많이 증가되었다.

![image](https://github.com/leexera/leexera.github.io/assets/122149118/a5163832-e949-441a-aea5-40caf5e987aa)

 1-stage detector는 2-stage detector의 일부로, 위와 같은 backbone을 통과시켜 feature pyramid network와 같은 구성을 하고 dense prediction을 하여 object box와 classification을 수행한다.
 변경할 수 있는 부분은 Input을 image로 넣을지, patch로 넣을 지, pyramid로 넣을지 등이 있고 Backbone은 VGG16이나 ResNet, ResNeXt, DarkNet과 같은 것을 사용할 수 있다.
 Neck은 Feature Pyramid Network(FPN), Pass Agregation Network, Bi-FPN과 같은 구조가 있어 upscale, downscale, feedfowarding이 있다.
 Head에서 Dense prediction으로는 Region Proposal Network(RPN)이나 YOLO, SSD, RetinaNet이 있고 Sparse Prediction은 box를 input으로 넣어서 2-stage로 수행하는 faster R-CNN이나 Region FCN(R-FCN)같은 경우로 분류한다.
 1-stage detector부분을 어떻게 더 개선할 것인지 box를 따로 prediction하는게 아니라 한 번에 network에서 prediction하는 구조를 개선할지에 대해 이야기한다.

 * Results
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/ef5435bd-b407-441a-8c03-32058e3c9ddb)
 최종적으로 CIoU loss를 사용하고 OA, GA, IT, M, S를 넣었을 때 가장 좋은 성능을 내는 것을 볼 수 있다.
    - M : Mosaic data augmentation - using the 4-image mosaic during training instead of single image
 cut mix 같은 느낌으로 data augmentation 효과를 누릴 수 있다.
    - IT : IoU threshold - using multiple anchors for a single ground truth IoU (truth, anchor) > IoU threshold
 single ground된 anchor를 사용하는 것이 아니라 여러 개의 anchor를 사용하여 IoU thresholding이 넘는 anchor box를 사용하는 것이다.
    - GA : Genetic algorithms - using genetic algorithms for selecting the optimal hyperparameters during network training on the first 10% of time periods
 rainforcement learning이나 bayesian optimization과 같이 hyperparameter를 tuning하는 유전 algorithm이라는 이름을 가진 기법이다.
 gradient descent로 network 자체에서 parameter를 학습하는 것이 아니라 hyper parameter를 어떻게 선택할지, 어떻게 더 빠르게 optimal한 parameter를 선택할지에 대한 algorithm이다.
 인간이 하나하나 learning rate나 batch-size를 조정할 수 없기 때문에 genetic algorithm을 먹여서 job을 넣어놓으면 적절한 hyperparameter와 좋은 결과를 내놓을 수 있다.
    - LS : Class label smoothing - using class label smoothing for sigmoid activation
 sigmoid activation때 class label을 one out vector로 넣는 것이 아니라 근처 값에 smoothin을 하여 넣는 것이다.
    - CBN : CmBN - using Cross mini-Batch Normalization for collecting statistics inside the entire batch, instead of collecting statistics inside a single mini-batch
 전체 batch안에서 statistic을 합쳐 어떤 single image가 feed fowarding을 할 때 사용하는 batchnorm parameter, unfine parameter를 accumurlation시켜 모든 batch에 batchnorm parameter를 사용하는 것이다.
    - CA : Cosine annealing scheduler - altering the learning rate during sinusoid training
 learning rate를 sine 곡선으로 training하는 것이 아닌 cosine함수를 이용하여 annealing을 해 줄여나가는 technique이다.
    - DM : Dynamic mini-batch size - automatic increase of mini-bach size during small resolution training by using Random training shapes
 small resolution에서는 mini-batch size를 작게 사용하다가 증가시키는 technique이다.
 LS, CBN, CA, DM은 실제 propose한 model에는 사용되지 않았다.
    - OA : Optimized Anchors - using the optimized anchors for training with the 512x512 network resolution
    - GIoU, CIoU, DIoU, MSE - using different loss algorithms for bounded box regression
 boudning box를 regression할 때 어떤 score를 minimize할 것인지에 대해 다양한 variation을 주어 학습을 시킨 것이다.

 CenterNet이나 ConerNet으로 1-stage detector는 YOLO이후에도 계속 발전되어왔다.

# Evaluation Metrics of Object Detection
 * IoU(Intersection over Union)
 ![image](https://github.com/leexera/leexera.github.io/assets/122149118/b76908ce-1eb3-4e0c-86bc-02c4672a3ab9)
 * Precision
    예측한 box 중에 정답을 맞춘 수 / 예측한 box 수
 적은 box로 정답을 많이 맞추는 것이 좋은 algorithm이다.
 * Recall
    정답 box를 맞춘 수 / 전체 정답 box 수
 proposal을 많이 만든다면 recall에 유리해질 것이다.   
 * AP(Average Precision)
 * mAP