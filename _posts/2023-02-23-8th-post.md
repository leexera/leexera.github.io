---
layout: post
title: "Gradient Descent"
description: "ì´í™”ì—¬ìëŒ€í•™êµ ê°•ì œì› êµìˆ˜ë‹˜(Electronic & Electrical Engineering)"
date: 2023-02-23
tags: study
comments: true
---

Gradient descent algorithmê³¼ ê·¸ ì™¸ì— Machine learningì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª‡ ê°€ì§€ ìµœì í™” ê¸°ìˆ ì— ëŒ€í•´ì„œ ì•Œì•„ë³¸ë‹¤.

---

# ì§€ë„ í•™ìŠµ

## Overview

 * We have some function (loss function)

 ![image](https://user-images.githubusercontent.com/122149118/220755807-0e50f77f-5b1a-4edc-9d90-5f50295007bc.png)
 and want 
 ![image](https://user-images.githubusercontent.com/122149118/220756080-ee36f26d-df03-437d-8c87-536c9d648e32.png)
 Algorithm outline:
  * Start with some initial parameters <i>Î¸<sub>0</sub>, Î¸<sub>1</sub></i>
  * Keep changing the parameter to reduce the loss function until we hopefully end up at a minimum.

 ìœ„ì™€ ê°™ì€ objective í•¨ìˆ˜ê°€ ì¡´ì¬í•œë‹¤.
 ëª©ì ì€ ì´ objective í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ë„ë¡ í•˜ëŠ” <i>Î¸<sub>0</sub>ì™€ Î¸<sub>1</sub></i>ì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤.
 ê·¸ì— ëŒ€í•œ ë°©ë²•ë¡ ìœ¼ë¡œ gradient descnet algorithmì—ì„œëŠ” ì²« ë²ˆì§¸, initialí•œ parameter <i>Î¸<sub>0</sub>, Î¸<sub>1</sub></i>ìœ¼ë¡œë¶€í„° ì‹œì‘í•˜ì—¬ ì´ ê°’ë“¤ì„ ì§€ì†ì ìœ¼ë¡œ ìµœì†Œë¡œ í•˜ëŠ” ì§€ì ê¹Œì§€ ë³€í™”ì‹œí‚¤ëŠ” ê²ƒì´ ëª©ì ì´ë‹¤.

## Gradient Descent Algorithm

 **Key components**

 * Gradient : the derivative of vector functions (partial derivative along each dimension)
    - Direction of greatest increase (or decrease) of a function
 * <u>The step size Î±</u> affects the rate at which the weight vector moves down the error surface and must be a positive number. (<u>hyper parameter</u>)
 * Î¸ is the <u>learnable parameters</u>
 * The function <i>J</i> is <u> the objective function</u> that we want to minimize.

 ![image](https://user-images.githubusercontent.com/122149118/220884034-6adc0fbe-2be1-40f7-87ae-5b8436b4c294.png)

 ì´ ë•Œ gradientëŠ” vectorí•¨ìˆ˜ì˜ partial derivativetermì„ ê°€ì§„ë‹¤.
 í•¨ìˆ˜ì˜ ë³€í™”ëŸ‰ì´ ê°€ì¥ í° ë°©í–¥ìœ¼ë¡œ ê·¸ë¦¼ì—ì„œì™€ ê°™ì´ updateí•œë‹¤.
 step size Î±ëŠ” parameter updateì˜ ë³€í™” ì •ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” ê°’ìœ¼ë¡œ í•™ìŠµ ì´ì „ì— ì„¤ì •í•˜ëŠ” hyper parameterì´ë‹¤. 
 Î¸ëŠ” learnable parameterì´ë‹¤.
 Î±ëŠ” ì‚¬ì „ì— ì •ì˜í•´ì•¼ í•˜ëŠ” ê°’ì´ê³ , Î¸ëŠ” êµ¬í•˜ê³ ì í•˜ëŠ” modelì˜ í•™ìŠµ parameterì´ë‹¤. 
 ë§Œì•½ ì´ Î±ê°’ì´ ëŒ€ë‹¨íˆ í¬ë‹¤ ë˜ëŠ” ì‘ë‹¤ì— ë”°ë¼ì„œ lossê°€ ë³€í™”í•˜ëŠ” ì •ë„ë¥¼ ë³´ì´ê³  ìˆë‹¤.

 * If Î± is too small, gradient descent can be slow.
 * If Î± is too large, gradient descent can overshoot the minimum. It may fail to converge, or even diverge.

 ![image](https://user-images.githubusercontent.com/122149118/220885119-eb4c93da-2062-4dfe-8722-fd0526094b88.png)
 
 ê°€ìš´ë° graphëŠ” Î±ê°’ì´ êµ‰ì¥íˆ ì‘ì€ ê²½ìš°ì— ìˆ˜ë ´í•˜ëŠ” í˜•íƒœì˜ graphê°€ ëœë‹¤.
 ìˆ˜ë ´ì†ë„ê°€ êµ‰ì¥íˆ ì²œì²œíˆ ë–¨ì–´ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.
 ëŒ€ì‹  ì´ Î± ê°’ì´ ì‘ê²Œë˜ë©´ ìˆ˜ë ´í•˜ëŠ” í˜•íƒœê°€ ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´í•˜ê²Œ ë˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
 ë°˜ëŒ€ë¡œ Î± ê°’ì´ êµ‰ì¥íˆ í° ê²½ìš°ì˜ graphê°€ 3ë²ˆì§¸ì— ë‚˜íƒ€ë‚˜ ìˆë‹¤.
 ì´ ê²½ìš°ì—ëŠ” Î± ê°’ì´ í¬ê¸°ë•Œë¬¸ì— error surface ìƒì—ì„œ ìµœì†Œì˜ ì§€ì ì„ ì°¾ê¸° ì–´ë µê³ , ë°œì‚°í•˜ëŠ” í˜•íƒœë¡œ í•™ìŠµì´ ì§„í–‰ëœë‹¤. 
 ê·¸ì— ë”°ë¼ì„œ errorì˜ lossê°’ì´ ì¤„ì–´ë“œëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ëŠ˜ì–´ë‚˜ê³  ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.
 ì¦‰, í•™ìŠµì´ ì§„í–‰ë˜ê³  ìˆì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤.
 ì¤‘ê°„ ì •ë„ì˜ Î±ê°’ì„ í†µí•´ ì–´ëŠì •ë„ ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ë©´ì„œ ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´í•˜ë„ë¡ í•˜ëŠ” ê²ƒì´ ëª©ì ì´ë‹¤.

## Batch gradient descent

 ![image](https://user-images.githubusercontent.com/122149118/220886034-6cc56f6b-7eb3-4e2c-9388-424ae58c4f6b.png)

 ![image](https://user-images.githubusercontent.com/122149118/220886101-9db3af6b-2ce5-411d-995e-66eaeaa54bba.png)
 â†’ What if the number of sample size <i>m</i> is increasing?
 ì•ì„œ ì„¤ëª…í•œ Gradient descent algorithmì„ Batch gradient descent algorithmì´ë¼ê³  í•œë‹¤.
 ìˆ˜ì‹ì€ Batch gradient descent algorithmì„ ë³´ì´ëŠ” ê²ƒìœ¼ë¡œ linear regression modelì—ì„œ ëª©ì  í•¨ìˆ˜ Jì˜ partial derivative termì„ ë„£ì–´ì„œ ê°ê° <i>Î¸<sub>0</sub>ì™€ Î¸<sub>1</sub></i>ì„ ë°”ê¾¸ê³  ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.
 ìœ„ ê·¸ë˜í”„ëŠ” parameterê°€ updateë˜ë©´ì„œ ì ì°¨ modelì´ dataì— fittingì´ ë˜ì–´ê°€ëŠ” ê³¼ì •ì„ ë³´ì´ê³  ìˆë‹¤.
 ì´ëŸ¬í•œ algorithmì€ ë¹„ë¡ local optimumì— ì·¨ì•½í•˜ì§€ë§Œ, ì–´ëŠ ì •ë„ ìˆ˜ë ´ì´ ë˜ì–´ê°€ëŠ” ì¥ë©´ì„ ë³¼ ìˆ˜ ìˆë‹¤.
 ì´ ë°©ì‹ì€ í° ë‹¨ì ì´ ìˆë‹¤. 
 ê·¸ ë‹¨ì ì€ ìˆ˜ì‹ì—ì„œ ë³´ì´ëŠ” ë°”ì™€ ê°™ì´ ì§€ê¸ˆ í˜„ì¬ <i>Î¸<sub>0</sub>ê·¸ë¦¬ê³  Î¸<sub>1</sub></i>ì„ updateí•˜ëŠ” ê³¼ì •ì—ì„œ ë°”ë¡œ ì „ì²´ sample mê°œê°€ ëª¨ë‘ ë‹¬ë¼ ê³ ë ¤ë¥¼ í•´ì•¼ëœë‹¤ê³  í•˜ëŠ” ê²ƒì´ë‹¤.
 ì „ì²´ sample errorë¥¼ ê³„ì‚°í•˜ê²Œ ë˜ê³ (h<sub>Î¸</sub>(x<sup>(i)</sup>) - y<sup>(i)</sup>) ëª¨ë“  sampleì— ëŒ€í•´ì„œ ì „ë¶€ ë‹¤ accumilationí•´ì•¼ì§€ Î¸<sub>0</sub>ë¥¼ ë°”ë¡œ í•œ ë²ˆ updateí•  ìˆ˜ ìˆë‹¤. Î¸<sub>1</sub> ì—­ì‹œ ë¹„ìŠ·í•˜ê²Œ ì´ì™€ ê°™ì€ computation loadê°€ ì¡´ì¬í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.
 ì´ ì‹ì—ì„œì™€ ê°™ì´ m, data sampleì˜ ìˆ«ìê°€ ì¦ê°€í•˜ë©´ ì¦ê°€í• ìˆ˜ë¡ ë³µì¡ë„ê°€ êµ‰ì¥íˆ ì»¤ì§€ê²Œ ëœë‹¤.

## Stochastic gradient descent (SGD)

 ![image](https://user-images.githubusercontent.com/122149118/220888845-68e2ef9c-e79e-406a-b3e8-da73502dc140.png)
 1 < <i>m</i> < <i>N</i> : mini-batch SGD
    Vs SGD : aileviate randomness
    Vs GD : less time in converging

 ![image](https://user-images.githubusercontent.com/122149118/220889146-5660944f-b412-4d77-9263-7733bffed3f8.png)

 ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ mì„ ê·¹ë‹¨ì ìœ¼ë¡œ ì¤„ì—¬ 1ë¡œ ë°”ê¾¼ algorithmì„ Stocahstic gradient descent algorithmì´ë¼ê³  í•œë‹¤.
 ì´ algorithmì€ batch gradient descentì— ë¹„í•´ ë¹ ë¥´ê²Œ iterationì„ ëŒ ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆì§€ë§Œ, ë°˜ëŒ€ë¡œ ê° sample í•˜ë‚˜í•˜ë‚˜ ë§ˆë‹¤ ê³„ì‚°ì„ í†µí•´ì„œ parameterë¥¼ ì—°ì‚°í•˜ê¸° ë•Œë¬¸ì— noiseì˜ ì˜í–¥ì„ ë°›ê¸° ì‰½ê²Œ ëœë‹¤ëŠ” ë‹¨ì ì„ ê°€ì§€ê³  ìˆë‹¤.
 ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ ìˆ˜ë ´í•˜ëŠ” ê³¼ì •ì—ì„œ ë§ì€ oscillationì´ ë°œìƒí•œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

## Limitation : Local Optimum

 ![image](https://user-images.githubusercontent.com/122149118/220900508-a8c95780-89dd-4450-bf62-7a6b9af64274.png)
 Cannot guarantee global minimum but attempt to find a good local minimum

 gradient descent algorithmì€ parameterì˜ ì´ˆê¸° pointì— ë”°ë¼ì„œ local minimumì— ë¹ ì§€ê¸° ì‰½ë‹¤.
 ì˜ˆë¥¼ ë“¤ì–´ ìœ„ ê·¸ë¦¼ì—ì„œ ë³´ëŠ” ë°”ì™€ ê°™ì´ gradient descent algorithmì´ ì‹œì‘ë˜ëŠ” pointì— ë”°ë¼ì„œ local optimumì„ ë‹¬ì„±í•˜ëŠëƒ ê·¸ë ‡ì§€ ì•Šê³  global optimumì„ ë‹¬ì„±í•˜ëŠëƒê°€ ê²°ì •ì´ ëœë‹¤.

 **Critical points** with zero slope: <i>âˆ‡<sub>x</sub>f(x)</i> = 0 gives no information about which direction to move

 ![image](https://user-images.githubusercontent.com/122149118/220901086-361902ff-33e7-4ef5-ad19-f28228e228cb.png)
 ì‚¬ì‹¤ deep learningê³¼ ê°™ì€ ë³µì¡í•œ modelì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì— error surfaceê°€ êµ‰ì¥íˆ ë³µì¡í•´ì„œ ì´ëŸ¬í•œ local optimumì— ë¹ ì§€ê²Œ ë  ìœ„í—˜ì„±ì´ ë§ì€ ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆë‹¤.
 ê·¸ ì¤‘ì—ì„œë„ settle pointì™€ ê°™ì´ ì–´ëŠ í•œ ë°©í–¥ìœ¼ë¡œ ìˆ˜ë ´ì„ í•˜ê²Œ ë  ë•Œ gradient ê°’ì´ 0ì´ ë˜ì–´ local optimumì— ë¹ ì§€ê²Œ ë˜ëŠ” ì§€ì ë“¤ì´ ë‹¤ìˆ˜ ì¡´ì¬í•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆë‹¤.
 ì´ëŸ¬í•œ suboptimalí•œ ë¬¸ì œì ë“¤ì„ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ê¸°ì¡´ì˜ gradient descent algorithmì—ì„œ ë‹¤ì–‘í•œ ë³€í˜• algorithmë“¤ì´ ê°œë°œë˜ì—ˆë‹¤.
 ê·¸ ì¤‘ì— ê°€ì¥ ëŒ€í‘œì ì¸ ë°©ì‹ì´ ë°”ë¡œ momentumì„ ì´ìš©í•˜ëŠ” ê²ƒì´ë‹¤.
 > Momentum
 >   ê³¼ê±°ì— Gradientê°€ update ë˜ì–´ì˜¤ë˜ ë°©í–¥ ë° ì†ë„ë¥¼ ì–´ëŠ ì •ë„ ë°˜ì˜í•´ì„œ í˜„ì¬ pointì—ì„œ Gradientê°€ 0ì´ ë˜ë”ë¼ë„ ê³„ì†í•´ì„œ í•™ìŠµì„ ì§„í–‰í•  ìˆ˜ ìˆëŠ” ë™ë ¥ì„ ì œê³µí•˜ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.

## Some ideas to avoid local minimum

 **Method of momentum**

 * SGD : very popular but tends to be slow and difficult to reach the minimum
 * **Method of momentum**
    - Designed to speed up learning in high curvature and small/noise gradients
    - Exponentially weighted moving average of past gradients(low pass filtering)

 ![image](https://user-images.githubusercontent.com/122149118/220902981-4fba3168-6816-42a1-af6b-a8ffab00fb06.png)
 ###### <i>v<sub>t</sub></i> : Exponentially weighted moving average at time <i>t</i>
 ###### <i>g<sub>t</sub></i> : observation gradient at time <i>t</i>
 ###### <i>Ï</i> (0~1): degree of weighting decrease (smoothing factor)
 c.f. <i>v<sub>t</sub> = Ï<sup>k</sup>v<sub>t - k</sub></i> + (1 - <i>Ï</i>)[<i>g<sub>t</sub> + Ïg<sub>t - 1</sub> + ... + Ï<sup>k - 1</sup>g<sub>t - k + 1</sub></i>]

 ìœ„ ìˆ˜ì‹ì—ì„œ momentum vëŠ” ê³¼ê±°ì˜ momentum v<sub>t - 1</sub>ì— Ïë¥¼ ê³±í•˜ê²Œ ë˜ê³  ë‹¤ì‹œ 1 - Ïë¥¼ í˜„ì¬ ê³„ì‚°í•œ gradient g<sub>t</sub>ì— ê³±í•˜ì—¬ êµ¬ì„±í•˜ëŠ” ë°©ì‹ì„ ì˜ë¯¸í•œë‹¤.
 ì´ ìˆ˜ì‹ì„ í’€ì–´ ì˜ë¯¸ë¥¼ ì‚´í´ë³´ê²Œ ë˜ë©´ í˜„ì¬ì˜ momentumì€ ê³¼ê±°ì˜ momentumì—ë‹¤ê°€ Ïë§Œí¼ì˜ ê°’ì„ ê³±í•˜ê²Œ ë˜ê³  ê·¸ ì´í›„ì— ê³¼ê±°ì— ë‚˜ì˜¤ê²Œë˜ëŠ” gradient termì„ ëˆ„ì í•´ì„œ ê³„ì‚°í•˜ê²Œ ëœë‹¤.
 ì´ ë•Œ gradient termë“¤ì´ ëˆ„ì ì„ í•˜ê²Œ ë  ë•Œ í˜„ì¬ ì‹œì ì—ì„œ ë©€ë©´ ë©€ìˆ˜ë¡ Ïê°’ì´ ì—°ì†ì ìœ¼ë¡œ ê³±í•´ì§€ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
 ì´ ë•Œ ÏëŠ” 1ë³´ë‹¤ ì‘ê¸° ë•Œë¬¸ì— Ïë¥¼ ì—°ì†ì ìœ¼ë¡œ ê³±í•˜ê²Œ ë˜ë©´ 1ë³´ë‹¤ ë”ìš± ì‘ì•„ì§€ê²Œ ëœë‹¤.
 ë”°ë¼ì„œ ë¨¼ ê³¼ê±°ì˜ ê°’ì€ ë”ìš± ì‘ì•„ì§€ê²Œ ë˜ê³  ë¹„êµì  ê°€ê¹Œìš´ ê±°ë¦¬ì˜ ê³¼ê±° gradientëŠ” ì ê²Œ ì‘ì•„ì§€ê¸° ë•Œë¬¸ì— ì´ëŸ¬í•œ ê³¼ì •ì„ exponentially moving averageë¼ê³  ë¶€ë¥¸ë‹¤.
 ì´ ë°©ì‹ì€ low pass filtering ì—°ì‚°ì´ê¸° ë•Œë¬¸ì—, í˜„ì¬ pointì—ì„œì˜ saddle pointë‚˜ ì‘ì€ noise gradient ê°’ ë³€í™”ì— ë³´ë‹¤ ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´í•  ìˆ˜ ìˆê²Œ ë°”ë€Œê²Œ ëœë‹¤.

 **SGD**

 <i>Î¸<sub>t + 1</sub> = Î¸<sub>t</sub> - Î±âˆ‡<sub>0</sub>J(Î¸<sub>t</sub>)
 let g = âˆ‡<sub>0</sub>J(Î¸)</i>

 ![image](https://user-images.githubusercontent.com/122149118/220906140-9e1aaced-9435-4f84-90d1-8cf6c1ac86bd.png)

 **SGD + momentum : Use a velocity as a weighted moving average of previous gradients**
 <i>v â† Ïv - Î±g
 Î¸ â† Î¸ + v</i>

 ![image](https://user-images.githubusercontent.com/122149118/220906758-2cbfa623-9c9c-45b6-95a1-9d2a00b52f90.png)

 **A parameter is updated by linear combination of gradient and velocity**

 ê¸°ì¡´ì˜ stochastic gradient descent algorithmì€ momentumì„ ë”í•´ì„œ ê³¼ê±°ì— ê°’ì„ ë°˜ì˜í•œ gradient ê°’ì´ updateê°€ ë˜ê²Œ ëœë‹¤.
 ì„¤ë ¹ local minumumì´ë‚˜ saddle pointì—ì„œ gradietê°€ 0ì´ ë˜ëŠ” ì§€ì ì´ ë°œìƒ í•˜ë”ë¼ë„ ê³¼ê±°ì— ì´ì–´ì˜¤ë˜ momentum ê°’ì„ ë°˜ì˜ í•˜ì—¬ ê³„ì†í•´ì„œ í•™ìŠµì„ ì§„í–‰ í•  ìˆ˜ ìˆê²Œ ëœë‹¤.
 ì¦‰ ì„¤ë ¹ gradient ê°’ì´ 0ì´ ë˜ë”ë¼ë„ í•™ìŠµì„ ì´ì–´ì„œ ì§„í–‰í•  ìˆ˜ ìˆê²Œ ëœë‹¤.
 ì´ëŸ¬í•œ momentumì„ ì´ìš©í•˜ëŠ” gradient descentì—ì„œ ì¡°ê¸ˆ ë” ë°œì „í•œ ê²ƒì´ nestrov momentum ë°©ì‹ì´ë‹¤.
 nestrov momentumì€ ê¸°ì¡´ì˜ ë°©ì‹ê³¼ ë‹¤ë¥´ê²Œ ìš°ì„  gradientë¥¼ ë¨¼ì € í‰ê°€í•˜ê³  updateë¥¼ í•´ì£¼ê²Œ ëœë‹¤.
 ì´ ë°©ì‹ì„ look ahead gradient stepì„ ì´ìš©í•œë‹¤ê³  ì´ì•¼ê¸°í•œë‹¤.

 * Nestrov Momentum
    - Difference from standard momentum: where gradient <i>g</i> is evaluated (i.e. "lookahead" gradient step)
  <i>v â† Ïv - Î±âˆ‡<sub>Î¸</sub>J(Î¸ + Ïv)
  Î¸ â† Î¸ + v</i>

 ![image](https://user-images.githubusercontent.com/122149118/220908312-b401bf69-2913-4b7d-9539-5fece8431fef.png)

 ê¸°ì¡´ì˜ ë°©ì‹ê³¼ ê°™ì€ ê²½ìš°ëŠ” í˜„ì¬ gradient stepê³¼ ê¸°ì¡´ì˜ momentum stepì„ ê³ ë ¤í•˜ì—¬ ì‹¤ì œ ë‹¤ìŒë²ˆ actual stepì„ vectorì˜ í•©ìœ¼ë¡œì„œ ê³„ì‚°ì„ í•œë‹¤ë©´ Nestrov momentum update ë°©ì‹ì—ì„œëŠ” actual stepì´ì™¸ì—ë„ momentum ê³„ì‚°ì„ í•˜ê²Œ ë  ë•Œ momentum stepë§Œí¼ ì´ë™ ì§€ì ì—ì„œ lookahead gradient stepì„ ê³„ì‚°í•˜ê¸° ë•Œë¬¸ì— ê·¸ ë‘ ê°œì˜ vector í•©ìœ¼ë¡œì„œ actual stepì„ ê³„ì‚°í•˜ê²Œ ëœë‹¤.
 ìˆ˜ì‹ì—ì„œ Jì— partial derivative term(âˆ‡<sub>Î¸</sub>)ì„ Î¸ + Ïvê°€ ë“¤ì–´ê°€ëŠ” ê²ƒì´ ì´ëŸ¬í•œ ê³¼ì •ì„ ë³´ì´ëŠ” ê²ƒì´ë‹¤.
 ë‹¤ìŒ ë°©ì‹ìœ¼ë¡œëŠ” AdaGradë¼ëŠ” ë°©ì‹ì´ ìˆë‹¤.
 ì´ ë°©ì‹ì€ ê° ë°©í–¥ìœ¼ë¡œì˜ learning rateë¥¼ ì ì‘ì ìœ¼ë¡œ ì¡°ì ˆí•˜ì—¬ í•™ìŠµ íš¨ìœ¨ì„ ì˜¬ë¦¬ê²Œ ëœë‹¤.
 ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ learning rateê°€ ì¡°ì ˆì´ ë˜ëŠ”ì§€ ì‚´í´ë³¸ë‹¤.

 **per-parameter adaptive learning rates**

 * **AdaGrad** : Adapts an indinidual learning rate of each direction
    - Slow down the learning rate when an accumulated gradient is large
    - Speed up the learning rate when an accumulated gradient is small
 * Allows an automatic tuning of the learning rate per parameter

 ![image](https://user-images.githubusercontent.com/122149118/220910872-d77f8475-a6b0-4e41-9ccf-ac30b7616cf5.png)

 ![image](https://user-images.githubusercontent.com/122149118/220910981-d69115ec-de16-4b16-94ab-4ea3d3f29add.png)

 ìœ„ ìˆ˜ì‹ì—ì„œ rì€ ê¸°ì¡´ì˜ ê°’ì— gradientì˜ ì œê³±ì„ ë”í•´ê°€ë©° updateí•˜ê²Œ ëœë‹¤.
 ë”°ë¼ì„œ rì€ gradientì˜ ì œê³± ì¦‰, í•©ì´ ëˆ„ì ì´ ë˜ê²Œ ë˜ë©´ì„œ ì ì°¨ ê°’ë“¤ì´ ì»¤ì§€ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.
 ë°˜ë©´ì— ì´ëŸ¬í•œ rì€ Î”Î¸ë¥¼ updateí•˜ëŠ” ê³¼ì •ì—ì„œ ë¶„ëª¨ì˜ termìœ¼ë¡œ ë“¤ì–´ê°€ê¸° ë•Œë¬¸ì— ì´ Î”Î¸ëŠ” ê°’ì´ ì ì  ì‘ì•„ì§€ê²Œ ë  ê²ƒì´ë‹¤.
 ì–´ëŠ í•œ ë°©í–¥ìœ¼ë¡œ gradient ê°’ì´ í¬ë‹¤ë¼ê³  í•˜ëŠ” ê²ƒì€ ì´ë¯¸ ê·¸ ë°©í–¥ìœ¼ë¡œ í•™ìŠµì´ ë§ì´ ì§„í–‰ë˜ì—ˆë‹¤ë¼ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.
 ê·¸ ë°©í–¥ìœ¼ë¡œ ì´ë¯¸ í•™ìŠµì´ ë§ì´ ì§„í–‰ë˜ì–´ì„œ gradientì˜ ëˆ„ì í•©ì´ í¬ë‹¤ë¼ê³  í•œë‹¤ë©´ ë°˜ëŒ€ë¡œ Î”Î¸ê°’ì´ ì‘ì•„ì ¸ì„œ ê·¸ë§Œí¼ì˜ ìˆ˜ë ´ ì†ë„ë¥¼ ì¤„ì´ê²Œ ë˜ëŠ”ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.
 ë°˜ëŒ€ë¡œ ì•„ì§ í•™ìŠµ ê³¼ì •ì´ ì§„í–‰ë˜ì§€ ì•Šì•„ rê°’ì´ ì—¬ì „íˆ ì‘ë‹¤ë©´ ê·¸ ë°©í–¥ìœ¼ë¡œëŠ” Î”Î¸ê°’ì„ í¬ê²Œí•˜ì—¬ ìˆ˜ë ´ì˜ ì†ë„ë¥¼ ë” ë¹ ë¥´ê²Œ í•  ê²ƒì´ë‹¤.
 ì´ì™€ ê°™ì´ AdaGrad algorithmê°™ì€ ê²½ìš° accumulated gradient ê°’ì„ í†µí•´ learning rateë¥¼ ì¡°ì ˆí•˜ê²Œ ëœë‹¤.
 í•˜ì§€ë§Œ AdaGradì™€ ê°™ì€ ë°©ì‹ì€ ë‹¨ì ì´ í•˜ë‚˜ ìˆë‹¤.
 ë°”ë¡œ gradientì˜ ê°’ì´ ê³„ì†í•´ì„œ ëˆ„ì ì´ ë¨ì— ë”°ë¼ì„œ learning rate ê°’ì´ êµ‰ì¥íˆ ì‘ì•„ì§€ê²Œ ëœë‹¤ëŠ” ê²ƒì´ë‹¤.
 ğŸ” Learning rateì´ ì‘ì•„ì§€ê²Œ ë˜ë©´ ì–´ë–»ê²Œ ë˜ëŠ”ê°€?
 â¡ í•™ìŠµì´ ê·¸ ì§€ì ì—ì„œ ì¼ì–´ë‚˜ê²Œ ë˜ì§€ ì•Šì„ ê²ƒì´ë‹¤.
 ì´ëŸ¬í•œ ë°©ì‹ì„ ìˆ˜ì •í•œ ê²ƒì´ RMSProp algorithmì´ë‹¤.

 * **RMSProp** : attempts to fix the drawbacks of AdaGrad, in which the learning rate becomes infinitesimally small and the algorithm is no longer able learning when the accumulated gradient is large.
 * **Remedy** Gradient accumulation by weighted decaying

 ![image](https://user-images.githubusercontent.com/122149118/220917328-7a10ee8b-1917-4d43-be2f-f9329e018123.png)

 ì˜¤ë¥¸ìª½ ìˆ˜ì‹ì—ì„œ ë³´ëŠ” ë°”ì™€ ê°™ì´ rì„ updateí•˜ê²Œ ë  ë•Œì—ëŠ” ì´ì œëŠ” ë°”ë¡œ gradientì˜ ì œê³±ì„ ê·¸ëŒ€ë¡œ ê³±í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ê¸°ì¡´ì— ìˆë˜ rì— Ïê°’ì„ ê³±í•˜ê²Œ ëê³  (1 - Ï)ë¥¼ gradientì˜ ì œê³±ì—ë‹¤ê°€ ê³±í•¨ìœ¼ë¡œì„œ ì´ rì˜ ê°’ì„ ê³¼ê±°ì— r ë§Œí¼ì˜ Ï factorë¥¼ ê³±í•´ì„œ ì–´ëŠ ì •ë„ ì¡°ì ˆí•˜ê²Œ ëœë‹¤.
##### Center
{: .center}

##### Right
{: .right}

## 2. Body Text

Lorem ipsum dolor sit amet, [test link](https://www.google.com) adipiscing elit. **This is strong.** Nullam dignissim convallis est. Quisque aliquam. *This is emphasized.* Donec faucibus. Nunc iaculis suscipit dui. 5<sup>3</sup> = 125. Water is H<sub>2</sub>O. Nam sit amet sem. Aliquam libero nisi, imperdiet at, tincidunt nec, gravida vehicula, nisl. <u>Underline</u>. Maecenas ornare tortor. Donec sed tellus eget `COPY filename` sapien fringilla nonummy. Mauris a ante. Suspendisse quam sem, consequat at, <del>Dinnerâ€™s at 5:00.</del> commodo vitae, feugiat in, nunc. Morbi imperdiet augue <mark>mark element</mark> quis tellus.

## 3. Images

![Large example image](http://placehold.it/800x400 "Large example image")

![Medium example image](http://placehold.it/400x200 "Medium example image")
![Small example image](http://placehold.it/200x200 "Small example image")

### 3-1. Image Alignment

![Center example image](http://placehold.it/200x200 "Center")
{: .center}

## 4. Blockquotes

> Lorem ipsum dolor sit amet, test link adipiscing elit. Nullam dignissim convallis est. Quisque aliquam.

## 5. List Types

### Unordered List

* Lorem ipsum dolor sit amet, consectetur adipiscing elit.
* Nam ultrices nunc in nisi pellentesque ultricies. Cras scelerisque ipsum in ante laoreet viverra. Pellentesque eget quam et augue molestie tincidunt ac ut ex. Sed quis velit vulputate, rutrum nisl sit amet, molestie neque. Vivamus sed augue at turpis suscipit fringilla.
* Integer pretium nisl vitae justo aliquam, at varius nisi blandit.
  1. Nunc vehicula nulla ac odio gravida vestibulum sed nec mauris.
  2. Duis at diam eget arcu dapibus consequat.
* Etiam vel elit in purus iaculis pretium.

### Ordered List

1. Quisque ullamcorper leo non ex pretium, in fermentum libero imperdiet.
2. Donec eu nulla euismod, rhoncus ipsum nec, faucibus elit.
3. Nam blandit purus gravida, accumsan sem in, lacinia orci.
  * Duis congue dui nec nisi posuere, at luctus velit semper.
  * Suspendisse in lorem id lacus elementum pretium nec vel nibh.
4. Aliquam eget ipsum laoreet, maximus risus vitae, iaculis leo.

### Definition Lists

kramdown
: A Markdown-superset converter

Maruku
: Another Markdown-superset converter

## 6. Tables

| Header1 | Header2 | Header3 |
|:--------|:-------:|--------:|
| cell1   | cell2   | cell3   |
| cell4   | cell5   | cell6   |
|----
| cell1   | cell2   | cell3   |
| cell4   | cell5   | cell6   |
|=====
| Foot1   | Foot2   | Foot3


## 7. Code Snippets

### Highlighted Code Blocks

```css
#container {
  float: left;
  margin: 0 -240px 0 0;
  width: 100%;
}
```

### Standard code block

    <div id="awesome">
      <p>This is great isn't it?</p>
    </div>
