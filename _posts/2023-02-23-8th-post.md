---
layout: post
title: "Gradient Descent"
description: "이화여자대학교 강제원 교수님(Electronic & Electrical Engineering)"
date: 2023-02-23
tags: study
comments: true
---

Gradient descent algorithm과 그 외에 Machine learning에서 사용하는 몇 가지 최적화 기술에 대해서 알아본다.

---

# 지도 학습

## Overview

 * We have some function (loss function)

 ![image](https://user-images.githubusercontent.com/122149118/220755807-0e50f77f-5b1a-4edc-9d90-5f50295007bc.png)
 and want 
 ![image](https://user-images.githubusercontent.com/122149118/220756080-ee36f26d-df03-437d-8c87-536c9d648e32.png)
 Algorithm outline:
  * Start with some initial parameters <i>θ<sub>0</sub>, θ<sub>1</sub></i>
  * Keep changing the parameter to reduce the loss function until we hopefully end up at a minimum.

 위와 같은 objective 함수가 존재한다.
 목적은 이 objective 함수를 최소화하도록 하는 <i>θ<sub>0</sub>와 θ<sub>1</sub></i>을 구하는 것이다.
 그에 대한 방법론으로 gradient descnet algorithm에서는 첫 번째, initial한 parameter <i>θ<sub>0</sub>, θ<sub>1</sub></i>으로부터 시작하여 이 값들을 지속적으로 최소로 하는 지점까지 변화시키는 것이 목적이다.

## Gradient Descent Algorithm

 **Key components**

 * Gradient : the derivative of vector functions (partial derivative along each dimension)
    - Direction of greatest increase (or decrease) of a function
 * <u>The step size α</u> affects the rate at which the weight vector moves down the error surface and must be a positive number. (<u>hyper parameter</u>)
 * θ is the <u>learnable parameters</u>
 * The function <i>J</i> is <u> the objective function</u> that we want to minimize.

 ![image](https://user-images.githubusercontent.com/122149118/220884034-6adc0fbe-2be1-40f7-87ae-5b8436b4c294.png)

 이 때 gradient는 vector함수의 partial derivativeterm을 가진다.
 함수의 변화량이 가장 큰 방향으로 그림에서와 같이 update한다.
 step size α는 parameter update의 변화 정도를 조절하는 값으로 학습 이전에 설정하는 hyper parameter이다. 
 θ는 learnable parameter이다.
 α는 사전에 정의해야 하는 값이고, θ는 구하고자 하는 model의 학습 parameter이다. 
 만약 이 α값이 대단히 크다 또는 작다에 따라서 loss가 변화하는 정도를 보이고 있다.

 * If α is too small, gradient descent can be slow.
 * If α is too large, gradient descent can overshoot the minimum. It may fail to converge, or even diverge.

 ![image](https://user-images.githubusercontent.com/122149118/220885119-eb4c93da-2062-4dfe-8722-fd0526094b88.png)
 
 가운데 graph는 α값이 굉장히 작은 경우에 수렴하는 형태의 graph가 된다.
 수렴속도가 굉장히 천천히 떨어지는 것을 볼 수 있다.
 대신 이 α 값이 작게되면 수렴하는 형태가 안정적으로 수렴하게 되는 것을 알 수 있다.
 반대로 α 값이 굉장히 큰 경우의 graph가 3번째에 나타나 있다.
 이 경우에는 α 값이 크기때문에 error surface 상에서 최소의 지점을 찾기 어렵고, 발산하는 형태로 학습이 진행된다. 
 그에 따라서 error의 loss값이 줄어드는 것이 아니라 늘어나고 있는 것을 볼 수 있다.
 즉, 학습이 진행되고 있지 않는다는 것이다.
 중간 정도의 α값을 통해 어느정도 빠르게 수렴하면서 안정적으로 수렴하도록 하는 것이 목적이다.

## Batch gradient descent

 ![image](https://user-images.githubusercontent.com/122149118/220886034-6cc56f6b-7eb3-4e2c-9388-424ae58c4f6b.png)

 ![image](https://user-images.githubusercontent.com/122149118/220886101-9db3af6b-2ce5-411d-995e-66eaeaa54bba.png)
 → What if the number of sample size <i>m</i> is increasing?
 앞서 설명한 Gradient descent algorithm을 Batch gradient descent algorithm이라고 한다.
 수식은 Batch gradient descent algorithm을 보이는 것으로 linear regression model에서 목적 함수 J의 partial derivative term을 넣어서 각각 <i>θ<sub>0</sub>와 θ<sub>1</sub></i>을 바꾸고 있는 것을 볼 수 있다.
 위 그래프는 parameter가 update되면서 점차 model이 data에 fitting이 되어가는 과정을 보이고 있다.
 이러한 algorithm은 비록 local optimum에 취약하지만, 어느 정도 수렴이 되어가는 장면을 볼 수 있다.
 이 방식은 큰 단점이 있다. 
 그 단점은 수식에서 보이는 바와 같이 지금 현재 <i>θ<sub>0</sub>그리고 θ<sub>1</sub></i>을 update하는 과정에서 바로 전체 sample m개가 모두 달라 고려를 해야된다고 하는 것이다.
 전체 sample error를 계산하게 되고(h<sub>θ</sub>(x<sup>(i)</sup>) - y<sup>(i)</sup>) 모든 sample에 대해서 전부 다 accumilation해야지 θ<sub>0</sub>를 바로 한 번 update할 수 있다. θ<sub>1</sub> 역시 비슷하게 이와 같은 computation load가 존재한다고 볼 수 있다.
 이 식에서와 같이 m, data sample의 숫자가 증가하면 증가할수록 복잡도가 굉장히 커지게 된다.

## Stochastic gradient descent (SGD)

 ![image](https://user-images.githubusercontent.com/122149118/220888845-68e2ef9c-e79e-406a-b3e8-da73502dc140.png)
 1 < <i>m</i> < <i>N</i> : mini-batch SGD
    Vs SGD : aileviate randomness
    Vs GD : less time in converging

 ![image](https://user-images.githubusercontent.com/122149118/220889146-5660944f-b412-4d77-9263-7733bffed3f8.png)

 이러한 문제를 해결하기 위해 m을 극단적으로 줄여 1로 바꾼 algorithm을 Stocahstic gradient descent algorithm이라고 한다.
 이 algorithm은 batch gradient descent에 비해 빠르게 iteration을 돌 수 있다는 장점이 있지만, 반대로 각 sample 하나하나 마다 계산을 통해서 parameter를 연산하기 때문에 noise의 영향을 받기 쉽게 된다는 단점을 가지고 있다.
 위의 그림과 같이 수렴하는 과정에서 많은 oscillation이 발생한다는 것을 알 수 있다.

## Limitation : Local Optimum

 ![image](https://user-images.githubusercontent.com/122149118/220900508-a8c95780-89dd-4450-bf62-7a6b9af64274.png)
 Cannot guarantee global minimum but attempt to find a good local minimum

 gradient descent algorithm은 parameter의 초기 point에 따라서 local minimum에 빠지기 쉽다.
 예를 들어 위 그림에서 보는 바와 같이 gradient descent algorithm이 시작되는 point에 따라서 local optimum을 달성하느냐 그렇지 않고 global optimum을 달성하느냐가 결정이 된다.

 **Critical points** with zero slope: <i>∇<sub>x</sub>f(x)</i> = 0 gives no information about which direction to move

 ![image](https://user-images.githubusercontent.com/122149118/220901086-361902ff-33e7-4ef5-ad19-f28228e228cb.png)
 사실 deep learning과 같은 복잡한 model을 사용하는 경우에 error surface가 굉장히 복잡해서 이러한 local optimum에 빠지게 될 위험성이 많은 것으로 알려져 있다.
 그 중에서도 settle point와 같이 어느 한 방향으로 수렴을 하게 될 때 gradient 값이 0이 되어 local optimum에 빠지게 되는 지점들이 다수 존재하는 것으로 알려져 있다.
 이러한 suboptimal한 문제점들을 해결하기 위해서 기존의 gradient descent algorithm에서 다양한 변형 algorithm들이 개발되었다.
 그 중에 가장 대표적인 방식이 바로 momentum을 이용하는 것이다.
 > Momentum
 >   과거에 Gradient가 update 되어오던 방향 및 속도를 어느 정도 반영해서 현재 point에서 Gradient가 0이 되더라도 계속해서 학습을 진행할 수 있는 동력을 제공하게 되는 것이다.

## Some ideas to avoid local minimum

 **Method of momentum**

 * SGD : very popular but tends to be slow and difficult to reach the minimum
 * **Method of momentum**
    - Designed to speed up learning in high curvature and small/noise gradients
    - Exponentially weighted moving average of past gradients(low pass filtering)

 ![image](https://user-images.githubusercontent.com/122149118/220902981-4fba3168-6816-42a1-af6b-a8ffab00fb06.png)
 ###### <i>v<sub>t</sub></i> : Exponentially weighted moving average at time <i>t</i>
 ###### <i>g<sub>t</sub></i> : observation gradient at time <i>t</i>
 ###### <i>ρ</i> (0~1): degree of weighting decrease (smoothing factor)
 c.f. <i>v<sub>t</sub> = ρ<sup>k</sup>v<sub>t - k</sub></i> + (1 - <i>ρ</i>)[<i>g<sub>t</sub> + ρg<sub>t - 1</sub> + ... + ρ<sup>k - 1</sup>g<sub>t - k + 1</sub></i>]

 위 수식에서 momentum v는 과거의 momentum v<sub>t - 1</sub>에 ρ를 곱하게 되고 다시 1 - ρ를 현재 계산한 gradient g<sub>t</sub>에 곱하여 구성하는 방식을 의미한다.
 이 수식을 풀어 의미를 살펴보게 되면 현재의 momentum은 과거의 momentum에다가 ρ만큼의 값을 곱하게 되고 그 이후에 과거에 나오게되는 gradient term을 누적해서 계산하게 된다.
 이 때 gradient term들이 누적을 하게 될 때 현재 시점에서 멀면 멀수록 ρ값이 연속적으로 곱해지는 것을 알 수 있다.
 이 때 ρ는 1보다 작기 때문에 ρ를 연속적으로 곱하게 되면 1보다 더욱 작아지게 된다.
 따라서 먼 과거의 값은 더욱 작아지게 되고 비교적 가까운 거리의 과거 gradient는 적게 작아지기 때문에 이러한 과정을 exponentially moving average라고 부른다.
 이 방식은 low pass filtering 연산이기 때문에, 현재 point에서의 saddle point나 작은 noise gradient 값 변화에 보다 안정적으로 수렴할 수 있게 바뀌게 된다.

 **SGD**

 <i>θ<sub>t + 1</sub> = θ<sub>t</sub> - α∇<sub>0</sub>J(θ<sub>t</sub>)
 let g = ∇<sub>0</sub>J(θ)</i>

 ![image](https://user-images.githubusercontent.com/122149118/220906140-9e1aaced-9435-4f84-90d1-8cf6c1ac86bd.png)

 **SGD + momentum : Use a velocity as a weighted moving average of previous gradients**
 <i>v ← ρv - αg
 θ ← θ + v</i>

 ![image](https://user-images.githubusercontent.com/122149118/220906758-2cbfa623-9c9c-45b6-95a1-9d2a00b52f90.png)

 **A parameter is updated by linear combination of gradient and velocity**

 기존의 stochastic gradient descent algorithm은 momentum을 더해서 과거에 값을 반영한 gradient 값이 update가 되게 된다.
 설령 local minumum이나 saddle point에서 gradiet가 0이 되는 지점이 발생 하더라도 과거에 이어오던 momentum 값을 반영 하여 계속해서 학습을 진행 할 수 있게 된다.
 즉 설령 gradient 값이 0이 되더라도 학습을 이어서 진행할 수 있게 된다.
 이러한 momentum을 이용하는 gradient descent에서 조금 더 발전한 것이 nestrov momentum 방식이다.
 nestrov momentum은 기존의 방식과 다르게 우선 gradient를 먼저 평가하고 update를 해주게 된다.
 이 방식을 look ahead gradient step을 이용한다고 이야기한다.

 * Nestrov Momentum
    - Difference from standard momentum: where gradient <i>g</i> is evaluated (i.e. "lookahead" gradient step)
  <i>v ← ρv - α∇<sub>θ</sub>J(θ + ρv)
  θ ← θ + v</i>

 ![image](https://user-images.githubusercontent.com/122149118/220908312-b401bf69-2913-4b7d-9539-5fece8431fef.png)

 기존의 방식과 같은 경우는 현재 gradient step과 기존의 momentum step을 고려하여 실제 다음번 actual step을 vector의 합으로서 계산을 한다면 Nestrov momentum update 방식에서는 actual step이외에도 momentum 계산을 하게 될 때 momentum step만큼 이동 지점에서 lookahead gradient step을 계산하기 때문에 그 두 개의 vector 합으로서 actual step을 계산하게 된다.
 수식에서 J에 partial derivative term(∇<sub>θ</sub>)을 θ + ρv가 들어가는 것이 이러한 과정을 보이는 것이다.
 다음 방식으로는 AdaGrad라는 방식이 있다.
 이 방식은 각 방향으로의 learning rate를 적응적으로 조절하여 학습 효율을 올리게 된다.
 어떤 방식으로 learning rate가 조절이 되는지 살펴본다.

 **per-parameter adaptive learning rates**

 * **AdaGrad** : Adapts an indinidual learning rate of each direction
    - Slow down the learning rate when an accumulated gradient is large
    - Speed up the learning rate when an accumulated gradient is small
 * Allows an automatic tuning of the learning rate per parameter

 ![image](https://user-images.githubusercontent.com/122149118/220910872-d77f8475-a6b0-4e41-9ccf-ac30b7616cf5.png)

 ![image](https://user-images.githubusercontent.com/122149118/220910981-d69115ec-de16-4b16-94ab-4ea3d3f29add.png)

 위 수식에서 r은 기존의 값에 gradient의 제곱을 더해가며 update하게 된다.
 따라서 r은 gradient의 제곱 즉, 합이 누적이 되게 되면서 점차 값들이 커지게 되는 것이다.
 반면에 이러한 r은 Δθ를 update하는 과정에서 분모의 term으로 들어가기 때문에 이 Δθ는 값이 점점 작아지게 될 것이다.
 어느 한 방향으로 gradient 값이 크다라고 하는 것은 이미 그 방향으로 학습이 많이 진행되었다라는 것을 의미한다.
 그 방향으로 이미 학습이 많이 진행되어서 gradient의 누적합이 크다라고 한다면 반대로 Δθ값이 작아져서 그만큼의 수렴 속도를 줄이게 되는게 되는 것이다.
 반대로 아직 학습 과정이 진행되지 않아 r값이 여전히 작다면 그 방향으로는 Δθ값을 크게하여 수렴의 속도를 더 빠르게 할 것이다.
 이와 같이 AdaGrad algorithm같은 경우 accumulated gradient 값을 통해 learning rate를 조절하게 된다.
 하지만 AdaGrad와 같은 방식은 단점이 하나 있다.
 바로 gradient의 값이 계속해서 누적이 됨에 따라서 learning rate 값이 굉장히 작아지게 된다는 것이다.
 🔍 Learning rate이 작아지게 되면 어떻게 되는가?
 ➡ 학습이 그 지점에서 일어나게 되지 않을 것이다.
 이러한 방식을 수정한 것이 RMSProp algorithm이다.

 * **RMSProp** : attempts to fix the drawbacks of AdaGrad, in which the learning rate becomes infinitesimally small and the algorithm is no longer able learning when the accumulated gradient is large.
 * **Remedy** Gradient accumulation by weighted decaying

 ![image](https://user-images.githubusercontent.com/122149118/220917328-7a10ee8b-1917-4d43-be2f-f9329e018123.png)

 오른쪽 수식에서 보는 바와 같이 r을 update하게 될 때에는 이제는 바로 gradient의 제곱을 그대로 곱하는 것이 아니라 기존에 있던 r에 ρ값을 곱하게 됐고 (1 - ρ)를 gradient의 제곱에다가 곱함으로서 이 r의 값을 과거에 r 만큼의 ρ factor를 곱해서 어느 정도 조절하게 된다.
##### Center
{: .center}

##### Right
{: .right}

## 2. Body Text

Lorem ipsum dolor sit amet, [test link](https://www.google.com) adipiscing elit. **This is strong.** Nullam dignissim convallis est. Quisque aliquam. *This is emphasized.* Donec faucibus. Nunc iaculis suscipit dui. 5<sup>3</sup> = 125. Water is H<sub>2</sub>O. Nam sit amet sem. Aliquam libero nisi, imperdiet at, tincidunt nec, gravida vehicula, nisl. <u>Underline</u>. Maecenas ornare tortor. Donec sed tellus eget `COPY filename` sapien fringilla nonummy. Mauris a ante. Suspendisse quam sem, consequat at, <del>Dinner’s at 5:00.</del> commodo vitae, feugiat in, nunc. Morbi imperdiet augue <mark>mark element</mark> quis tellus.

## 3. Images

![Large example image](http://placehold.it/800x400 "Large example image")

![Medium example image](http://placehold.it/400x200 "Medium example image")
![Small example image](http://placehold.it/200x200 "Small example image")

### 3-1. Image Alignment

![Center example image](http://placehold.it/200x200 "Center")
{: .center}

## 4. Blockquotes

> Lorem ipsum dolor sit amet, test link adipiscing elit. Nullam dignissim convallis est. Quisque aliquam.

## 5. List Types

### Unordered List

* Lorem ipsum dolor sit amet, consectetur adipiscing elit.
* Nam ultrices nunc in nisi pellentesque ultricies. Cras scelerisque ipsum in ante laoreet viverra. Pellentesque eget quam et augue molestie tincidunt ac ut ex. Sed quis velit vulputate, rutrum nisl sit amet, molestie neque. Vivamus sed augue at turpis suscipit fringilla.
* Integer pretium nisl vitae justo aliquam, at varius nisi blandit.
  1. Nunc vehicula nulla ac odio gravida vestibulum sed nec mauris.
  2. Duis at diam eget arcu dapibus consequat.
* Etiam vel elit in purus iaculis pretium.

### Ordered List

1. Quisque ullamcorper leo non ex pretium, in fermentum libero imperdiet.
2. Donec eu nulla euismod, rhoncus ipsum nec, faucibus elit.
3. Nam blandit purus gravida, accumsan sem in, lacinia orci.
  * Duis congue dui nec nisi posuere, at luctus velit semper.
  * Suspendisse in lorem id lacus elementum pretium nec vel nibh.
4. Aliquam eget ipsum laoreet, maximus risus vitae, iaculis leo.

### Definition Lists

kramdown
: A Markdown-superset converter

Maruku
: Another Markdown-superset converter

## 6. Tables

| Header1 | Header2 | Header3 |
|:--------|:-------:|--------:|
| cell1   | cell2   | cell3   |
| cell4   | cell5   | cell6   |
|----
| cell1   | cell2   | cell3   |
| cell4   | cell5   | cell6   |
|=====
| Foot1   | Foot2   | Foot3


## 7. Code Snippets

### Highlighted Code Blocks

```css
#container {
  float: left;
  margin: 0 -240px 0 0;
  width: 100%;
}
```

### Standard code block

    <div id="awesome">
      <p>This is great isn't it?</p>
    </div>
